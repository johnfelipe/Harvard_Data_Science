
Central Limit Theorem (CLT)

When the number of independent draws (aka sample size) is large, the probability distribution of the sum of these 
draws is approximimately normal.

We will now go over the mathematical theory that lets us approximate the probability distribution for the sum 
of draws. The same approach we use for sum of the draws will be useful for describing the distribution of averages
and proportions, which we will need to understand, for example, how polls work.


Expected Value

E[X] = u

E is the expected value
X is the random variable
Mu is a greek letter for M, which is the first letter in the word mean, which is a synonym with average.


A random variable will vary around an expected value in a way that if you take the average of many, many draws, 
the average of the draws will approximate the expected value. Getting closer and closer the more draws you take.

A useful formula is that the expected value of the random variables defined by one draw is the average of the 
numbers in the urn.

In our urn, we have twenty 1's and eighteen -1's.
E[X] = (20 + -18)/38 = 0.05

An intuitive way to think about the expected value is that if we play the game over and over,
the Casino wins, on average, $0.05 per game.

Monte Carlo Simulation confirms this:
> B <- 10^6
> X <- sample(c(-1,1), B, replace=TRUE, prob=c(9/19,10/19))
> mean(X)
[1] 0.05183

If there are just two possible outcomes, a and b, with proportions p and 1-p respective, the average is 
a times p plus b times 1-p.


(n * a * p + n * b * 91-0)) / n    =    ap + b(1-p)


The expected value of the sum of draws is
(number of draws) * (average of the numbers in the urn)

1000 people play roulette - casino wins $50 on average

But casino wants to know
How different can one observation be from the expected value ?


Standard error (SE)
gives us an idea of the size of the variation around the expected value.
SE[X]


If the draws are independent (very important assumption):
sqrt(number of draws) * (standard deviation of the numbers in the urn

| b - a | sqrt(p(1-p))

In our roulette example
| 1- (-1) | sqrt(10/19 * 9/19)
> 2* sqrt(90)/19
[1] 0.998614

so practically 1.

So because 1 draw is obviously the sum of 1 draw, we can use a formula to calculate that the random variable 
defined by 1 draw has an expected value of $0.05 and a standard error of about 1.
This makes sense since we either get a 1 or a minus 1 with 1 slightly favored over the minus 1.

> n <- 1000
> sqrt(n) * 2 * sqrt(90)/19
[1] 31.57895

so when 1000 bet on red, the casino is expected to win $50 with a standard error of $32.

So how likely is the Casino to lose money?
Here The Central Limit Theorem will help. The central limit theorem tells us that the distribution of the sum of S
is approximated by a normal distribution.

# expected value
> mu <- n * (20-18)/38
[1] 52.63158
# standard error
> se <- sqrt(n) * n * sqrt(90)/19
[1] 31.57895

matches the monte carlo simulation:
> mean(S)
[1] 52.5386
> sd(S)
[1] 31.74741

Using the Central Limit Theory, we can skip the Monte Carlo simulation
and instead, compute the probability of the Casino losing
money using the approximation.
> pnorm(0, mu, se)
[1] 0.04779035
about 5%

matches the monte carlo simulation:
> mean(S <0)
[1] 0.046


------------------------------------------------------------------------------------------------------

Averages and Proportions

Property 1: The expected value of the sum of random variables is the sum of the expected values of the individual
	random variables.
	
	E[X1 + X2 + ..... + Xn] = E[X1] + E[X2] + ... + E[Xn]
	
	If the x are drawn from the same urn, then they all have the same expected value.
	We call it mu here. And therefore, the expected value of the sum is n times mu, which is another way of 
	writing the result of the sum of draws.
	
	E[X1 + X2 + ..... + Xn] = nu
	
	
Property 2: The expected value of a random variable times a non-random constant is the expected value times
	that non-random constant.
	
	E[aX] = a * E[X]
	
	To see why this is intuitive, think of a change of units.
	If we change the units of the random variable, say from dollars to cents, the expectation should change 
	in the same way.
	
A consequence of these two facts that we described is that the expected value of the average of draws from the 
same urn is the expected value of the urn, call it mu again.

E[(X1 + X2 + ..... + Xn)/n] = E[X1] + E[X2] + ... + E[Xn])/n
				= nu/n
				= u
				
Property 3: The square of the standard error of the sum of INDEPENDENT random variables is the sum of the 
	square of the standard error of EACH random variable
	
	SE[X1 + X2 + ..... + Xn] = sqrt(SE[X1]^2 + SE[X2]^2 + ... + SE[Xn]^2)
	
	Note that the square of the standard error is referred to as a variance:
	
	Variance[X1] = SE[X1]^2
	
Property 4: The standard error of a random variable times a non-random constant is the standard error times the
	non-random constant.
	
	SE[aX] = a * SE[X]
	


A consequence of these previous two properties is that the standard error of the average of independent draws
from the same urn is the standard deviation of the urn-- let's call it sigma, this is the Greek letter for s.
--divided by the square root of n.	
	SE[(X1 + X2 + ..... + Xn)/x] 	= SE[X1 + X2 + ... + Xn]/n
					= sqrt(SE[X1]^2 + SE[X2]^2 + ... + SE[Xn]^2)/n
					= sqrt(sigma^2 + sigma^2 + ... + sigma^2) /n
					= sqrt(n*sigma^2) / n
					= sigma/sqrt(n)

Property 5: If X is a normally distributed random variable, then if a and b are non-random constants, 
	aX + B is also a normally distributed random variable.
	Note that all we are doing is changing the units of the random variable by multiplying by a and 
	then shifting the center by adding b.
	
	
-----------------------------------------------------------------------------------------------------------


Law of Large Numbers

An important implication of the properties we described in the previous video is that the standard error
of the average of draws becomes smaller and smaller as the number of draws n grows larger.
When n is very large, then the standard error is practically 0, and the average of the draws converges to the 
average of the urn.

This is known of law of large numbers or law of average.

The chance of tossing heads is 50% regardless of the previous 5 tosses.

After a million tosses, you will definitely see about 50% heads, regardless of what the first five were.


-----------------------------------------------------------------------------------------------------------

How large is large in CLT?

In many circumstances, as few as 30 draws is enough to make the CLT useful.
However, these should not be considered general rules.

Note for example, that when the probability of success is very small, we need larger sample sizes.
Consider for example, the lottery. In the lottery, the chance of winning are less than 1 in a million.

Yet, the number of winners, the sum of the draws, range between 0, and in very extreme cases, four.
This sum is certainly not well approximated by the normal distribution.
So the central limit theorem doesn't apply, even with a very large sample size.

This is generally true when the probability of success is very low.
In these cases, the Poisson solution is more appropriate.

-----------------------------------------------------------------------------------------------------------

Assessment

1.
The exercises in the previous chapter explored winnings in American roulette. In this chapter of exercises, we will continue with the roulette example and add in the Central Limit Theorem.
In the previous chapter of exercises, you created a random variable S that is the sum of your winnings after betting on green a number of times in American Roulette.
What is the probability that you end up winning money if you bet on green 100 times?

Execute the sample code to determine the expected value avg and standard error se as you have done in previous exercises.
Use the pnorm function to determine the probability of winning money.

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Define the number of bets using the variable 'n'
n <- 100

# Calculate 'avg', the expected outcome of 100 spins if you win $17 when the ball lands on green and you lose $1 when the ball doesn't land on green
avg <- n * (17*p_green + -1*p_not_green)

# Compute 'se', the standard error of the sum of 100 outcomes
se <- sqrt(n) * (17 - -1)*sqrt(p_green*p_not_green)

# Using the expected value 'avg' and standard error 'se', compute the probability that you win money betting on green 100 times.
1-pnorm(0, avg, se)


2.
Create a Monte Carlo simulation that generates 10,000 outcomes of S, the sum of 100 bets.
Compute the average and standard deviation of the resulting list and compare them to the expected value (-5.263158) and standard error (40.19344) for S that you calculated previously.
Use the replicate function to replicate the sample code for B <- 10000 simulations.
Use the sample function to simulate outcomes of either a loss ($-1) or a win ($17) for the bet.
Use the sum function to add up the winnings over all iterations of the model.
Use the mean function to compute the average winnings.
Use the sd function to compute the standard deviation of the winnings.
# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Define the number of bets using the variable 'n'
n <- 100

# The variable `B` specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1)

# Create an object called `S` that replicates the sample code for `B` iterations and sums the outcomes.
S <- replicate(B, {
  X <- sample(c(-1,17), n, replace=TRUE, prob=c(p_not_green, p_green))
  sum(X)
  })

# Compute the average value for 'S'
mean(S)


# Calculate the standard deviation of 'S'
sd(S)


3.
In this chapter, you calculated the probability of winning money in American roulette using the CLT.
Now, calculate the probability of winning money from the Monte Carlo simulation. The Monte Carlo simulation from the previous exercise has already been pre-run for you, resulting in the variable S that contains a list of 10,000 simulated outcomes.
Use the mean function to calculate the probability of winning money from the Monte Carlo simulation, S.
mean(S > 0)


4.
The Monte Carlo result and the CLT approximation for the probability of losing money after 100 bets are close, but not that close. What could account for this?
The CLT does not work as well when the probability of success is small.

5.
Now create a random variable Y that contains your average winnings per bet after betting on green 10,000 times.
Use set.seed to make sure the result of your random operation matches the expected answer for this problem.
Specify the number of times you want to sample from the possible outcomes.
Use the sample function to return a value from a vector of possible values: winning $17 or losing $1.
Be sure to assign a probability to each outcome and to indicate that you are sampling with replacement.
Calculate the average result per bet placed using the mean function.
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1)

# Define the number of bets using the variable 'n'
n <- 10000

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Create a vector called `X` that contains the outcomes of `n` bets
X <- sample(c(-1,17), n, replace=TRUE, prob=c(p_not_green, p_green))

# Define a variable `Y` that contains the mean outcome per bet. Print this mean to the console.
Y <- mean(X)
Y

6.
What is the expected value of Y, the average outcome per bet after betting on green 10,000 times?
Using the chances of winning $17 (p_green) and the chances of losing $1 (p_not_green), calculate the expected outcome of a bet that the ball will land in a green pocket.
# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Calculate the expected outcome of `Y`, the mean outcome per bet in 10,000 bets
p_green * 17 + p_not_green*-1



7.
What is the standard error of Y, the average result of 10,000 spins?
Compute the standard error of Y, the average result of 10,000 independent spins.
# Define the number of bets using the variable 'n'
n <- 10000

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Compute the standard error of 'Y', the mean outcome per bet from 10,000 bets.
sqrt(n) * abs(-1 - 17) * sqrt(p_not_green*p_green)/n


8.
What is the probability that your winnings are positive after betting on green 10,000 times?
Execute the code that we wrote in previous exercises to determine the average and standard error.
Use the pnorm function to determine the probability of winning more than $0.
# We defined the average using the following code
avg <- 17*p_green + -1*p_not_green

# We defined standard error using this equation
se <- 1/sqrt(n) * (17 - -1)*sqrt(p_green*p_not_green)

# Given this average and standard error, determine the probability of winning more than $0. Print the result to the console.
1-pnorm(0, avg, se)


9.
Create a Monte Carlo simulation that generates 10,000 outcomes of S, the average outcome from 10,000 bets on green.
Compute the average and standard deviation of the resulting list to confirm the results from previous exercises using the Central Limit Theorem.
Use the replicate function to model 10,000 iterations of a series of 10,000 bets.
After each iteration, take the average of the 10,000 outcomes.
Find the average of the 10,000 average outcomes.
Compute the standard deviation of the 10,000 simulations.
# The variable `n` specifies the number of independent bets on green
n <- 10000

# The variable `B` specifies the number of times we want the simulation to run
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random number generation
set.seed(1)

# Generate a vector `S` that contains the the average outcomes of 10,000 bets modeled 10,000 times
S <- replicate(B, {
  X <- sample(c(-1,17), n, replace=TRUE, prob=c(p_not_green, p_green))
  mean(X)
  })
  
# Compute the average of `S`
mean(S)

# Compute the standard deviation of `S`
sd(S)


10.
In a previous exercise, you found the probability of winning more than $0 after betting on green 10,000 times using the Central Limit Theorem. Then, you used a Monte Carlo simulation to model the average result of betting on green 10,000 times over 10,000 simulated series of bets.
What is the probability of winning more than $0 as estimated by your Monte Carlo simulation? The code to generate the vector S that contains the the average outcomes of 10,000 bets modeled 10,000 times has already been run for you.
mean(S > 0)


11.
The Monte Carlo result and the CLT approximation are now much closer than when we calculated the probability of winning for 100 bets on green. What could account for this difference?
The CLT does works better when the sample size is larger.



