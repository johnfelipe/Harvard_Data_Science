Introduction to Smoothing

Before continuing with machine learning algorithms, we introduce the important concept of smoothing.
Smoothing is a very powerful technique used all across data analysis.
Other names given to this technique are "curve fitting" and "low band pass filtering".
It's designed to detect trends in the presence of noisy data in cases in which the shape of the trend is unknown.
The smoothing name comes from the fact that to accomplish this feat we assume that the trend is smooth,
as in a smooth surface, and the noise is unpredictably wobbly.
Something like this.
Part of what we explain here are the assumptions that permit us to extract a trend from the noise.
To understand why we cover this topic, note that the concepts behind smoothing techniques
are extremely useful in machine learning because conditional expectations
and probabilities can be thought of as trends of unknown shapes
that we need to estimate in the presence of uncertainty.
To explain these concepts, we will focus first on a problem with just one predictor.
Specifically we try to estimate the time trend in the popular vote
from the 2008 election, the difference between Obama and McCain.
You can load the data like this and we can see a plot here.


data("polls_2008")
qplot(day, margin, data = polls_2008)

For the purposes of this example, do not think of it as a forecasting problem.
We're simply interested in learning the shape of the trend after collecting all the data.
We assume that for any given day x, there's a true preference among the electorate, f
of x, but due to the uncertainty introduced by polling, each data point comes with an error, epsilon.
A mathematical model for the observed poll margin, y, is y equals f of x plus epsilon.

Yi = f(xi) + ei

To think of this as a machine learning problem, consider that we want to predict y given the day x.
And that if we knew it, we would use the conditional expectation, f of x equals expectation of y given x.

f(x) E(Y | X = x)

But we don't know it, so we have to estimate it.
We're going to start by using regression, the only method we know, to see how it does.

resid <- ifelse(lm(margin~day, data = polls_2008)$resid > 0, "+", "-")
polls_2008 %>% 
  mutate(resid = resid) %>% 
  ggplot(aes(day, margin)) + 
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  geom_point(aes(color = resid), size = 3)


The line we see does not appear to describe the trend very well.
Note, for example, that on September 4, this is day negative 62,
62 days until Election Day, the Republican Convention was held.
This appeared to give McCain a boost in the polls, which can be clearly seen in the data.
The regression line does not capture this.
To further see the lack of fit, we note that points
above the fitted line, blue, and those below, red, are not evenly distributed.
We therefore need an alternative, a more flexible approach.

-------------------------------------------------------------------------------------------

Bin Smoothing and Kernels

The general idea of bin smoothing is to group data points 
into strata in which the value of f of x can be assumed to be constant.
We can make this assumption because we think f of x changes slowly.
And as a result, f of x is almost constant in small windows of time.
An example of this idea is to assume for the poll data
that public opinion remain approximately the same within a week's time.
With this assumption in place, we have several data points with the same expected value.
So if we fix a day to be in the center of our week-- call it x0--
then for any day x, such that the absolute value of x minus x0 is less than 3.5, we assume that f of x is a constant.

for any other day x that |x-x0| <= 3.5
we assume f(x) is a constant

Let's call it mu.

f(x) = mu

This assumption implies that the expected value of y given xd
is approximately mu when the distance between x i and x0 is less than 3.5.

E[Yi|Xi = xi] ~ mu
if |xi - x0| <= 3.5


In smoothing, we call the size of the interval satisfying the condition the distance between x i and x0
is less than 3.5 the window size, the bandwidth, or the span.
These are all synonyms.
Now, this assumption implies that a good estimate for f of x is the average of the y values in the window.
If we define A0 to be the set of indexes i such that x i minus x0
is less than 3.5 in absolute value and N0 as the number of indexes in A0,
then our estimate is given by this formula.

A0 set of indexes i
such that
|xi - x0| <= 3.5

N0 number of indexes in A0

f^(x0) = 1/N0 EieA0Yi

It's just simply the average in the window.
The idea behind bin smoothing is to make this calculation for each value of x.
So we make each value of x the center and recompute that average.
So in the poll example, for each day, we would compute the average for the values within a week of the day
that we're considering.
Let's look at two examples.
Let's set the center at negative 125 and then also set it at negative 55.
Here's what the data looks like.
The black points are the points that are used to compute
the average at those two points.
The blue line represents the average that was computed.

By computing this average for every point, we form an estimate of the underlying curve f of x.
In this animation, we see the procedure happening, starting at negative 155 all the way up to election day-- day 0.
At each value of x0, we keep the estimate f hat of x0 and move on to the next point.

The final result, which you can compute using this code, looks like this.

span <- 7 
fit <- with(polls_2008, 
            ksmooth(day, margin, x.points = day, kernel="box", bandwidth = span))

polls_2008 %>% mutate(smooth = fit$y) %>%
  ggplot(aes(day, margin)) +
    geom_point(size = 3, alpha = .5, color = "grey") + 
  geom_line(aes(day, smooth), color="red")


Note that the final result for the bin smoother is quite wiggly.
One reason for this is that each time the window moves, two points change.
So if we start with seven points and change two, that's a substantial percentage of points that are changing.
We can attenuate this somewhat by taking weighted averages that give the center of a point more weight than those
that are far away from the center, with the two points at the edges receiving very little weight.
We call the functions from which we compute these weights the kernel.
Note that you can think of the bin smoother as an approach that uses a kernel.
The formula looks like this.

f^(x0) = En i=1 w0(xi)Yi

Each point receives a weight, in the case of bin smoothers,
between 0 for points that are outside the window and 1 divided by N0 for points inside the window,
with N0 the number of points in that week.
In the code we showed, we told the function k-smooth to use the kernel "box."
That is because the kernel that gives us bin smoother using this formulation looks like a box.
Here's a picture.
Now, the k-smooth function provides a way to obtain a smoother estimate.
This is by using the normal or Gaussian density to assign weights.
So the kernel will be the normal density, which we can see here.
In this animation, the size of the points represent the weights they get in the weighted average.
You can see that the points near the edges receive little weight.

The final result, which you can get using this code, looks like this.

span <- 7
fit <- with(polls_2008, 
            ksmooth(day, margin,  x.points = day, kernel="normal", bandwidth = span))

polls_2008 %>% mutate(smooth = fit$y) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") + 
  geom_line(aes(day, smooth), color="red")

Note that the final estimate looks smoother now.
Now, there are several functions in R that implement bin smoothers or kernel approaches.
One example, the one we showed, is k-smooth.
However, in practice, we typically prefer methods that use slightly more complex models than fitting a constant.
The final result that we saw for the smooth bin smoother is still somewhat wiggly.
You can see in some parts-- for example, from negative 125 to negative 75,
we see that the function is more wiggly than we really expect.
We're now going to learn about approaches that improve on this.


---------------------------------------------------------------------------------------------------------------

Local Weighted Regression (loess)

A limitation of the bin smoother approach we just described is that we need small windows for the approximately
constant assumption to hold.
As a result, we end up with a small number of data points to average.
And as a result of this, we obtain imprecise estimates of our trend.
Here, we describe how local weighted regression or loess permits us to consider larger windows.
To do this, we will use a mathematical result referred to as Taylor's theorem, which tells us
that if you look close enough at any smooth function f, it looks like a line.
To see why this makes sense, consider the curved edges gardeners make.
They make these using spades which are straight lines so they can generate curves that are locally straight lines.
So instead of assuming the function is approximately constant in a window, we assume the function is locally linear.
With the linear assumption, we can consider larger window sizes than with a constant.
So instead of the one-week window, we will instead consider a larger window in which the trend is approximately linear.
We start with a three-week window and later consider enabling other options.
So the model for points that are in a three-week window looks like this.
We assume that Y given X in that window is a line.

E[Yi|Xi = xi] = B0 + B1(xi-x0)
if |xi-x0| <= 10.5

Now, for every point x0 loess defines a window and then fits a line within that window.
So here's an example of those fits for x0 equals negative 125 and x0 equals negative 55.

The fitted values at x0 become our estimate of the trend.
In this animation, we demonstrate the idea.
The final result is a smoother fit than the bin
smoother since we used larger sample sizes to estimate our local parameters.
You can get the final estimate using this code, and it looks like this.

total_days <- diff(range(polls_2008$day))
span <- 21/total_days

fit <- loess(margin ~ day, degree=1, span = span, data=polls_2008)

polls_2008 %>% mutate(smooth = fit$fitted) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(day, smooth), color="red")


Now, note that different spans give us different estimates.
We can see how different window sizes lead to different estimates with this animation.
Here are the final estimates.
We can see that with 0.1 the line is quite wiggly.
With 0.15, it's slightly less.
Now, with 0.25, we get a rather smooth estimate.
And with 0.66, it almost looks like a straight line.
There are three other differences between loess and the typical bin smoother which we describe here.
The first is that rather than keeping the bin size the same,
loess keeps the number of points used in the local fit the same.
This number is controlled via the span argument which expects a proportion.
So for example, if N is a number of data points, and the span is 0.5, then for any given X,
loess will use 0.5 times N closest points to X for the fit.

loess will use 0.5 * N closest points

Another difference is that when fitting a line locally, loess uses a weighted approach.
Basically, instead of least squares, we minimize a weighted version.
So we would minimize this equation.



However, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight which you can see here.
And to define weights, we use this formula.

The kernel for the tri-weight looks like this.

The third difference is that loess has the option of fitting the local model robustly.
An iterative algorithm is implemented in which,
after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration.
To use this option, use the argument family equals symmetric.

family = "symmetric"


One more important point about loess.
Taylor's theorem also tells us that if you look at a function close enough, it looks like a parabola and that you don't
have to look as close as you do for the linear approximation.
This means we can make our windows even larger 
and fit parabolas instead of lines, so the local model would look like this.
This is actually the default procedure for the function loess.
You may have noticed that when we show the code for loess, we set a parameter degree equals 1.
This tells loess to fit polynomials of degree 1, a fancy name for lines.
If you read the help page for loess, you'll see that the argument degree defaults to 2.
So by default, loess fits parabolas not lines.
Here is a comparison of fitting lines, the red dashed, and fitting parabolas, the orange solid.

Notice that degree equals 2 gives us a more wiggly result.
I personally prefer degree equals 1 as it is less prone to this kind of noise.
Now, one final note.
This relates to ggplot.
Note that ggplot uses loess and the geom smooth function.
So if you type this, you get your points and fitted loess line.
But be careful with the default table,
as they are rarely optimal.
However, you can change these quite easily as is demonstrated in this code, and now we get a better fit.



------------------------------------------------------------------------------------------------


Q1:


In the Wrangling course of this series, PH125.6x, we used the following code to obtain mortality counts for Puerto Rico for 2015-2018:

library(tidyverse)
library(purrr)
library(pdftools)
    
fn <- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf", package="dslabs")
dat <- map_df(str_split(pdf_text(fn), "\n"), function(s){
	s <- str_trim(s)
	header_index <- str_which(s, "2015")[1]
	tmp <- str_split(s[header_index], "\\s+", simplify = TRUE)
	month <- tmp[1]
	header <- tmp[-1]
	tail_index  <- str_which(s, "Total")
	n <- str_count(s, "\\d+")
	out <- c(1:header_index, which(n==1), which(n>=28), tail_index:length(s))
	s[-out] %>%
		str_remove_all("[^\\d\\s]") %>%
		str_trim() %>%
		str_split_fixed("\\s+", n = 6) %>%
		.[,1:5] %>%
		as_data_frame() %>% 
		setNames(c("day", header)) %>%
		mutate(month = month,
			day = as.numeric(day)) %>%
		gather(year, deaths, -c(day, month)) %>%
		mutate(deaths = as.numeric(deaths))
}) %>%
	mutate(month = recode(month, "JAN" = 1, "FEB" = 2, "MAR" = 3, "APR" = 4, "MAY" = 5, "JUN" = 6, 
                          "JUL" = 7, "AGO" = 8, "SEP" = 9, "OCT" = 10, "NOV" = 11, "DEC" = 12)) %>%
	mutate(date = make_date(year, month, day)) %>%
	filter(date <= "2018-05-01")
Use the loess function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long.

Which of the following plots is correct?


total_days <- diff(range(new_dat$date))

total_days
span <- 61/total_days


fit <- loess(deaths ~ as.numeric(date), degree=1, span = 0.05, data=new_dat)

new_dat %>% mutate(smooth = fit$fitted) %>%
  ggplot(aes(as.numeric(date), deaths)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(as.numeric(date), smooth), color="red")

#1


Q2:

Work with the same data as in Q1 to plot smooth estimates against day of the year, all on the same plot, but with different colors for each year.

Which code produces the desired plot?

dat %>% 
	mutate(smooth = predict(fit, as.numeric(date)), day = yday(date), year = as.character(year(date))) %>%
	ggplot(aes(day, smooth, col = year)) +
	geom_line(lwd = 2)
	
	
	
Q3:

Suppose we want to predict 2s and 7s in the mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power.

In fact, if we fit a regular logistic regression the coefficient for x_2 is not significant!

This can be seen using this code:

library(broom)
mnist_27$train %>% glm(y ~ x_2, family = "binomial", data = .) %>% tidy()
Plotting a scatterplot here is not useful since y is binary:

qplot(x_2, y, data = mnist_27$train)
Fit a loess line to the data above and plot the results. What do you observe?

library(broom)
library(dslabs)
data("mnist_27")
mnist_27$train %>% glm(y ~ x_2, family = "binomial", data = .) %>% tidy()

head(mnist_27$train,6)

lapply(mnist_27$train$y, class)

qplot(x_2, y, data = mnist_27$train)


fit <- loess(as.numeric(y) ~ x_2, degree=1, span = 1, data=mnist_27$train)

mnist_27$train %>% mutate(smooth = fit$fitted) %>%
  ggplot(aes(x_2, as.numeric(y))) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(x_2, smooth), color="red")
  
There is predictive power and the conditional probability is non-linear. 


  