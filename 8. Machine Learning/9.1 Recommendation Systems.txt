Recommendation Systems

Recommendation systems use ratingss
that users have given items to make specific recommendations to users.
Companies like Amazon that sell many products to many customers
and permit these customers to rate their products are able to collect massive data sets that
can be used to predict what rating a given user will give a specific item.
Items for which a high rating is predicted for specific users are then recommended to that user.
Netflix uses recommendation systems to predict how many stars a user will give a specific movie.
Here we provide the basics of how these recommendations are predicted,
motivated by some of the approaches taken by the winners of the Netflix challenge.
What's the Netflix challenge?
On October 2006, Netflix offered a challenge to the data science community.
Improve our recommendation algorithm by 10% and win a $1 million.
In September 2009, the winners were announced.
You can follow this link to see the news article.

http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/

You can read a good summary of how the winning algorithm was put together following this link.

http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/

We'll include it in the class material.
And a more detailed explanation following this link, also included in the class material.

http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf

Here we show you some of the data analysis strategies used by the winning team.
Unfortunately, the Netflix data is not publicly available.
But the GroupLens research lab generated their own database with over 20 million ratings for over 27,000 movies
by more than 138,000 users.
We make a small subset of this data available via the DS labs package.
You can upload it like this.

library(dslabs)
data("movielens")

We can see that the movie lens table is tidy formant and contains thousands of rows.

head(movielens)

Each row represents a rating given by one user to one movie.
We can see the number of unique users that provide ratings 
and for how many unique movies they provided them using this code.

movielens %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))

If we multiply those two numbers, we get a number much larger than 5 million.
Yet our data table has about 100,000 rows.
This implies that not every user rated every movie.
So we can think of this data as a very large matrix with users
on the rows and movies on the columns with many empty cells.
The gather function permits us to convert to this format, but if we try to do it for the entire matrix
it will crash R. So lets look at a smaller subset.
This table shows a very small subset of seven users and five movies.
You can see the ratings that each user gave each movie
and you also see NA's for movies that they didn't watch or they didn't rate.
You can think of the tasking recommendation systems as filling in the NA's in the table we just showed.
To see how sparse the entire matrix is, here the matrix for a random sample of 100 movies and 100 users
is shown with yellow indicating a user movie combination for which we have a rating.
All right so let's move on to try to make predictions.
The machine learning challenge here is more complicated than we have studied up to now because each outcome
y has a different set of predictors.
To see this, note that if we are predicting the rating for movie i 
by user u, in principle, all other ratings related to movie i and by user u may be used as predictors.
But different users rate a different number of movies and different movies.
Furthermore, we may be able to use information from other movies
that we have determined are similar to movie i or from users determined to be similar to user u.
So in essence, the entire matrix can be used as predictors for each cell.
OK.
So let's get started.
Let's look at some of the general properties of the data to better understand the challenge.
The first thing we notice is that some movies get rated more than others.
Here's the distribution.

movielens %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Movies")

This should not surprise us given that there are blockbusters watched by millions and artsy independent movies watched
by just a few.
A second observation is that some users are more active than others at rating movies.

movielens %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Users")

Notice that some users have read it over 1,000 movies while others have only rated a handful.
To see how this is a machine learning challenge, note that we need to build an algorithm with data we have collected.
And this algorithm will later be used by others as users look for movie recommendations.
So let's create a test set to assess the accuracy of the models we implement,
just like in other machine learning algorithms.
We use the caret package using this code.

library(caret)
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]

To make sure we don't include users and movies in the test set that do not
appear in the training set, we removed these using the semi_join function, using this simple code.

test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

All right, now.
To compare different models or to see how well we're
doing compared to some baseline, we need to quantify what it means to do well.
We need a loss function.
The Netflix challenge used the typical error and thus decided on a winner
based on the residual mean squared error on a test set.
So if we define yui as the rating for movie i by user u
and y hat ui as our prediction, then the residual mean squared error is defined as follows.

Yu,i rating for movie i by user u
^Yi,u prediction

RMSE= sqrt( 1/N  Eu,i (^Yu,i-Yu,i)^2)

Here N is a number of user movie combinations and the sum is occurring over all these combinations.
Remember that we can interpret the residual mean squared error similar to standard deviation.
It is the typical error we make when predicting a movie rating.
If this number is much larger than one, we're typically missing by one or more stars rating which is not very good.

RMSE > 1, not a good prediction

So let's quickly write a function that computes this residual means squared error for a vector of ratings
and their corresponding predictors.
It's a simple function that looks like this.

RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
  
  
And now we're ready to build models and compare them to each other.


---------------------------------------------------------------------------------

Building the Recommendation System

The Netflix challenge winners implemented two general classes of models.
One was similar to k-nearest neighbors, where you found movies that were similar to each other and users that
were similar to each other.
The other one was based on an approach called "matrix factorization".
That's the one we we're going to focus on here.
So let's start building these models.
Let's start by building the simplest possible recommendation system.
We're going to predict the same rating for all movies, regardless of the user and movie.
So what number should we predict?
We can use a model-based approach.
A model that assumes the same rating for all movies and all users, 
with all the differences explained by random variation would look something like this.

Yui = mu + eu,i

Here epsilon represents independent errors sampled from the same distribution centered at zero,
and mu represents the true rating for all movies and users.
We know that the estimate that minimizes the residual mean squared error is the least squared estimate of mu. 
And in this case, that's just the average of all the ratings.
We can compute it like this.

mu_hat <- mean(train_set$rating)
mu_hat
#> [1] 3.54

That average is 3.54.
So that is the average rating of all movies across all users.
So let's see how well this movie does.
We compute this average on the training data.
And then we compute the residual mean squared error on the test set data.

naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse
#> [1] 1.05

So we're predicting all unknown ratings with this average.
We get a residual mean squared error of about 1.05.
That's pretty big.
Now note, if you plug in any other number, you get a higher RMSE.

predictions <- rep(2.5, nrow(test_set))
RMSE(test_set$rating, predictions)
#> [1] 1.49

That's what's supposed to happen, because we know that the average minimizes the residual mean squared error
when using this model.
And you can see it with this code.
So we get a residual mean squared error of about 1.
To win the grand prize of $1 million, a participating team had to get to a residual mean squared error of about 0.857.
So we can definitely do better.
Now because as we go along we will be comparing different approaches,
we're going to create a table that's going to store the results that we obtain as we go along.
We're going to call it RMSE results.
It's going to be created using this code.

rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

All right.
So let's see how we can do better.
We know from experience that some movies are just generally rated higher than others.
We can see this by simply making a plot of the average rating that each movie got.
So our intuition that different movies are rated differently is confirmed by data.
So we can augment our previous model by adding a term, b i, to represent the average rating for movie i.

Yu,i = mu + bi + eu,i

In statistics, we usually call these b's, effects.
But in the Netflix challenge papers, they refer to them as "bias," thus the b in the notation.
All right.
We can again use least squares to estimate the b's in the following way, like this.

fit <- lm(rating ~ as.factor(userId), data = movielens)

However, note that because there are thousands of b's, each movie gets one parameter, one estimate.
So the lm function will be very slow here.
So we don't recommend running the code we just showed.
However, in this particular situation, we
know that the least squared estimate, b hat i, is just the average of yui minus the overall mean for each movie, i.
So we can compute them using this code.

mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

Note that we're going to drop the hat notation in the code
to represent the estimates going forward, just to make the code cleaner.
So this code completes the estimates for the b's.
We can see that these estimates vary substantially, not surprisingly.

movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))

Some movies are good.
Other movies are bad.
Remember, the overall average is about 3.5.
So a b i of 1.5 implies a perfect five-star rating.

^mu = 3.5
bi = 1.5
^Yu,i = ^mu + ^bi

Now let's see how much our prediction improves once we predict using the model that we just fit.


predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i

model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()


We can use this code and see that our residual mean squared error did drop a little bit.
We already see an improvement.
Now can we make it better?
All right.
How about users?
Are different users different in terms of how they rate movies?
To explore the data, let's compute the average rating for user, u, for those that have rated over 100 movies.
We can make a histogram of those values.
And it looks like this.

train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")

Note that there is substantial variability across users, as well.
Some users are very cranky.
And others love every movie they watch, while others are somewhere in the middle.
This implies that a further improver to our model may be something like this.

Yu,i = mu + bi + bu + eu,i

We include a term, bu, which is the user-specific effect.
So now if a cranky user-- this is a negative bu-- rates a great movie, which will have a positive b i, the effects
counter each other, and we may be able to correctly predict 
that this user gave a great movie a three rather than a five, which will happen.
And that should improve our predictions.
So how do we fit this model?
Again, we could use lm.
The code would look like this.

lm(rating ~ as.factor(movieId) + as.factor(userId))

But again, we won't do it, because this is a big model.
It will probably crash our computer.
Instead, we will compute our approximation by computing the overall mean, u-hat, the movie effects, b-hat i,
and then estimating the user effects, b u-hat, by taking the average of the residuals obtained
after removing the overall mean and the movie effect from the ratings yui.
The code looks like this.

user_avgs <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

And now we can see how well we do with this new model
by predicting values and computing the residual mean squared error.


We can now construct predictors and see how much the RMSE improves:

predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred


model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()



We see that now we obtain a further improvement.
Our residual mean squared error dropped down to about 0.89.

----------------------------------------------------------------------------------------------------------
The following exercises all work with the movielens data, which can be loaded using the following code:

library(dslabs)
data("movielens")

Q1:
Compute the number of ratings for each movie and then plot it against the year the movie came out. 
Use the square root transformation on the counts.

What year has the highest median number of ratings?
movielens %>% group_by(movieId) %>%
	summarize(n = n(), year = as.character(first(year))) %>%
	qplot(year, n, data = ., geom = "boxplot") +
	coord_trans(y = "sqrt") +
	theme(axis.text.x = element_text(angle = 90, hjust = 1))
	
Or

movielens %>% na.omit() %>% group_by(year) %>%
  summarize(number = n()) %>%
ggplot(aes(year, number)) + geom_point()

df <- movielens %>% na.omit() %>% group_by(year) %>%
  summarize(number = n()) 

df[order(-df$number),]
1995


Q2:
We see that, on average, movies that came out after 1993 get more ratings. We also see that with newer movies, 
starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users 
have had to rate it.

Among movies that came out in 1993 or later, what are the 25 movies with the most ratings per year, and what is the 
average rating of each of the top 25 movies?

What is the average rating for the movie The Shawshank Redemption?

df <- movielens %>% na.omit() %>% filter(year >= 1993) %>% group_by(year, title) %>%
  summarize(number = mean(rating))

df %>% filter(title == "Shawshank Redemption, The")
4.49


What is the average number of ratings per year for the movie Forrest Gump?

df <- movielens %>% na.omit() %>% filter(year >= 1993) %>% group_by(year, title) %>%
  summarize(number = n()/(2018-1994))
df %>% filter(title == "Forrest Gump")
14.2


Q3:
From the table constructed in Q2, we can see that the most frequently rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post-1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.

What type of trend do you observe?

movielens %>% na.omit() %>% filter(year >= 1993) %>% group_by(year, title) %>%
  summarize(average = mean(rating), total = n() %>%
  ggplot(aes(average, total)) + geom_point() + geom_smooth()

Or

movielens %>% 
  filter(year >= 1993) %>%
  group_by(movieId) %>%
  summarize(n = n(), years = 2017 - first(year),
            title = title[1],
            rating = mean(rating)) %>%
  mutate(rate = n/years) %>%
  ggplot(aes(rate, rating)) +
  geom_point() +
  geom_smooth()


The more often a movie is rated, the higher its average rating. correct

Q4:
Suppose you are doing a predictive analysis in which you need to fill in the missing ratings with some value.

Given your observations in the exercise in Q3, which of the following strategies would be most appropriate?
Fill in the missing values with a lower value than the average rating across all movies.
Because a lack of ratings is associated with lower ratings, it would be most appropriate to fill in the missing value with a lower value than the average. You should try out different values to fill in the missing value and evaluate prediction in a test set.


Q5:
The movielens dataset also includes a time stamp. This variable represents the time and data in which the rating was 
provided. The units are seconds since January 1, 1970. Create a new column date with the date.

Which code correctly creates this new column?
movielens <- mutate(movielens, date = as_datetime(timestamp))

Q6:
Compute the average rating for each week and plot this average against day. Hint: use the round_date function before you group_by.

What type of trend do you observe?

movielens <- mutate(movielens, rounddate = round_date(date, unit = c("week")))
movielens

movielens %>% na.omit() %>% filter(year >= 1993) %>% group_by(rounddate) %>%
  summarize(average = mean(rating)) %>%
              ggplot(aes(average, rounddate)) + geom_point() + geom_smooth() 

or

movielens %>% mutate(date = round_date(date, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth()

There is some evidence of a time effect on average rating. correct


Q7:
Consider again the plot you generated in Q6.

If we define  as the day for user's  rating of movie , which of the following models is most appropriate?

Yu,i = mu + bi + bu + f(du,i) + eu,i with f aa smoth function of du,i


Q8:
The movielens data also has a genres column. This column includes every genre that applies to the movie. 
Some movies fall under several genres. Define a category as whatever combination appears in this column. 
Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. 
Plot these as error bar plots.

Which genre has the lowest average rating?
movielens %>% na.omit() %>% filter(year >= 1993) %>% group_by(genres) %>%
  summarize(sum_rating = n(), average = mean(rating), se = sd(rating)/sqrt(n()))%>% filter(sum_rating >1000) %>%
  ggplot(aes(x = genres, y = average, ymin = average - 2*se, ymax = average + 2*se)) +
geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

or
movielens %>% group_by(genres) %>%
	summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%
	filter(n >= 1000) %>% 
	mutate(genres = reorder(genres, avg)) %>%
	ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
	geom_point() +
	geom_errorbar() + 
	theme(axis.text.x = element_text(angle = 90, hjust = 1))
	
Comedy

Q9:
The plot you generated in Q8 shows strong evidence of a genre effect. Consider this plot as you answer the following question.

If we define  as the day for user's  rating of movie , which of the following models is most appropriate?

Yu,i = mu + bi + bu + Ek=1^K xu,i Bk + eu,i  with xu,i ^k = 1 if gu,i is genre k



            