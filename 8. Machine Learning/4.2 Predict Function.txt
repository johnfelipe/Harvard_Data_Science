
Predict Function

Before we continue with the concepts that connect linear regression to machine learning,
let's describe the function predict.
The predict function is very useful for machine learning applications.
This function takes a fitted object from functions such a lm or glm and a data frame with the new predictors
for which you want to predict and returns a prediction.
So in our current example, instead of writing out
the formula for the regression line, we can use the function predict like this.

y_hat <- predict(fit, test_set)

We can see that we get the same result for the loss by computing it just like we did before.
And we see that we once again get 4.78.

mean((y_hat - test_set$son)^2)

It's because using predict is equivalent to using the regression line.
Note that predict does not always return objects of the same type.
To learn about the specifics, you need to look
at the help file for the type of fitted object that is being used.
So if we use lm, we might get a different object type than if we use glm.

?predict.lm
?predict.glm


To learn about the specifics, you can look at the help file for predict.lm and predict.glm.
There'll be other examples like this, where the function is called, say, knn.
Then we have to look at the help file for predict.knn.


----------------------------------------------------------------------------------------------------------------


Q1

Create a data set using the following code:

set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))
Use the caret package to partition the dataset into test and training sets of equal size. Train a linear model and calculate the RMSE. Repeat this exercise 100 times and report the mean and standard deviation of the RMSEs. (Hint: You can use the code shown in a previous course inside a call to replicate using a seed of 1.


set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
  data.frame() %>% setNames(c("x", "y"))

library(caret)

set.seed(1)
rmse_dat <- replicate(100, {
  test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
  
  train_set <- dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)
  
  fit <- lm(y~x, data=train_set)
  y_hat <- predict(fit, test_set)
  
  RMSE<-sqrt(mean((y_hat - test_set$y)^2))
  
})

mean(rmse_dat)
sd(rmse_dat)

Mean: 2.497754

SD: 0.1243952


Q2:
Now we will repeat the above but using larger datasets. Repeat the previous exercise but for datasets 
with n <- c(100, 500, 1000, 5000, 10000). Save the average and standard deviation of RMSE from the 100 repetitions 
using a seed of 1. Hint: use the sapply or map functions.


set.seed(1)
RSME_fun <- function(a){
  Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
  dat <- MASS::mvrnorm(n = a, c(69, 69), Sigma) %>%
    data.frame() %>% setNames(c("x", "y"))
  
  rmse_dat <- replicate(100, {
    test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
    
    train_set <- dat %>% slice(-test_index)
    test_set <- dat %>% slice(test_index)
    
    fit <- lm(y~x, data=train_set)
    y_hat <- predict(fit, test_set)
    
    RMSE<-sqrt(mean((y_hat - test_set$y)^2))
    
  })
  
  list(mean(rmse_dat), sd(rmse_dat))
}

a <- c(100, 500, 1000, 5000, 10000)
set.seed(1)
sapply(a, RSME_fun)


     [,1]      [,2]       [,3]       [,4]       [,5]      
[1,] 2.497754  2.720951   2.555545   2.624828   2.618442  
[2,] 0.1180821 0.08002108 0.04560258 0.02309673 0.01689205


Q3:
What happens to the RMSE as the size of the dataset becomes larger?
On average, the RMSE does not change much as n gets larger, but the variability of the RMSE decreases. 


Q4:
Now repeat the exercise from Q1, this time making the correlation between x and y larger, as in the following code:

set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))
Note what happens to RMSE - set the seed to 1 as before.



set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
  data.frame() %>% setNames(c("x", "y"))


set.seed(1)
rmse_dat <- replicate(100, {
  test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
  
  train_set <- dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)
  
  fit <- lm(y~x, data=train_set)
  y_hat <- predict(fit, test_set)
  
  RMSE<-sqrt(mean((y_hat - test_set$y)^2))
  
})

mean(rmse_dat)
sd(rmse_dat)

Mean: 0.9099808
SD: 0.06244347


Q5:
Which of the following best explains why the RMSE in question 4 is so much lower than the RMSE in question 1?
When we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y. 


Q6:
Create a data set using the following code.

set.seed(1)
n <- 1000
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
Note that y is correlated with both x_1 and x_2 but the two predictors are independent of each other, as seen by cor(dat).

Use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2 and both x_1 and x_2. Train a linear model for each.

Which of the three models performs the best (has the lowest RMSE)?

set.seed(1)
n <- 1000
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
  data.frame() %>% setNames(c("y", "x_1", "x_2"))
cor(dat)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)

train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y~x_1, data=train_set)
y_hat <- predict(fit, test_set)

RMSE<-sqrt(mean((y_hat - test_set$y)^2))
mean(RMSE)
#0.6708231
sd(RMSE)
#Na



fit <- lm(y~x_2, data=train_set)
y_hat <- predict(fit, test_set)

RMSE<-sqrt(mean((y_hat - test_set$y)^2))
mean(RMSE)
#0.6775359
sd(RMSE)


fit <- lm(y~x_1 + x_2, data=train_set)
y_hat <- predict(fit, test_set)

RMSE<-sqrt(mean((y_hat - test_set$y)^2))
mean(RMSE)
#0.3552544
sd(RMSE)

x_1 and x_2


Q7:

Report the lowest RMSE of the three models tested in Q6.


set.seed(1)
n <- 1000
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
  data.frame() %>% setNames(c("y", "x_1", "x_2"))

set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)

train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y~x_1 + x_2, data=train_set)
y_hat <- predict(fit, test_set)

RMSE<-sqrt(mean((y_hat - test_set$y)^2))
mean(RMSE)
0.3070962


Q8:

Repeat the exercise from q6 but now create an example in which x_1 and x_2 are highly correlated.

set.seed(1)
n <- 1000
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
Use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2.

Compare the results from q6 and q8. What can you conclude?

set.seed(1)
n <- 1000
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
  data.frame() %>% setNames(c("y", "x_1", "x_2"))

set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)

train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y~x_1, data=train_set)
y_hat <- predict(fit, test_set)
RMSE<-sqrt(mean((y_hat - test_set$y)^2))
mean(RMSE)

fit_x2 <- lm(y~x_2, data=train_set)
y_hat <- predict(fit, test_set)
RMSE<-sqrt(mean((y_hat - test_set$y)^2))
mean(RMSE)

fit_x1_x2 <- lm(y~x_1 + x_2, data=train_set)
y_hat <- predict(fit, test_set)
RMSE<-sqrt(mean((y_hat - test_set$y)^2))
mean(RMSE)

Adding extra predictors can improve RMSE substantially, but not when the added predictors are highly correlated with other predictors. 


