Generative Models

We have described how when using square loss the conditional expectation or probabilities
provide the best approach to developing a decision rule.
In a binary case, the best we can do is called Bayes' rule
which is a decision rule based on the true conditional probability, probably y equals one given the predictors x.

p(x)=Pr(Y=1|X=x)

We have described several approaches to estimating this conditional probability.
Note that in all these approaches, we estimate the conditional probability
directly and do not consider the distribution of the predictors.
In machine learning, these are referred to as "discriminative" approaches.
However, Bayes' theorem tells us that knowing the distribution of the predictors x may be useful.
Methods that model the joint distribution of y and the predictors x are referred to as "generative models".
We start by describing the most general generative model "naive Bayes"
and then proceed to describe some more specific cases, quadratic discriminant
analysis QDA and linear discriminant analysis LDA.
Recall that Bayes' theorem tells us that we can rewrite the conditional probability like this
with the f's representing the distribution functions of the predictors x for the two classes y equals 1
and when y equals 0.


p(x)=Pr(Y=1|X=x) = (fX|Y=1(x)Pr(Y=1)) / (fX|Y=0(x)Pr(Y=0)+fX|Y=1(x)Pr(Y=1))


The formula implies that if we can estimate these conditional distributions, the predictors,
we can develop a powerful decision realm. 
However, this is a big if.
As we go forward, we will encounter examples in which the predictors x have many dimensions
and we do not have much information about their distribution.
So it will be very hard to estimate those conditional distributions.
In these cases, naive Bayes would be practically impossible to implement.
However, there are instances in which we have a small number of predictors,
not much more than two, and many categories in which generated models can be quite powerful.
We describe two specific examples and use our previously described case study to illustrate them.


--------------------------------------------------------------------------------------------------------

Naive Bayes

Let's start with a very simple and uninteresting, yet illustrative, example.
Predict sex from the height example.
We can get the data and generate training and test set using this code.

library(caret)
data("heights")
y <- heights$height
set.seed(2)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)

In this example, the naive Bayes approach is particularly appropriate.
Because we know that the normal distribution is a very good approximation of the conditional distributions
of height given sex for both classes, females and males.
This implies that we can approximate the conditional distributions by simply estimating averages and standard
deviations from the data with this very simple piece of code, like this.

params <- train_set %>% 
  group_by(sex) %>% 
  summarize(avg = mean(height), sd = sd(height))
params
#> # A tibble: 2 x 3
#>   sex      avg    sd
#>   <fct>  <dbl> <dbl>
#> 1 Female  64.5  4.02
#> 2 Male    69.3  3.52


The prevalence, which we will denote with pi, which is equal to the probability of y equals 1,
can be estimated from the data as well like this.

Prevalence
pi = Pr(Y=1)

pi <- train_set %>% 
  summarize(pi=mean(sex=="Female")) %>% 
  .$pi
pi
#> [1] 0.229

We basically compute the proportion of females.
Now we can use our estimates of average and standard deviations to get the actual rule.

x <- test_set$height

f0 <- dnorm(x, params$avg[2], params$sd[2])
f1 <- dnorm(x, params$avg[1], params$sd[1])

p_hat_bayes <- f1*pi / (f1*pi + f0*(1 - pi))

We get the conditional distributions, f0 and f1, and then we use Bayes theorem to compute the naive Bayes estimate
of the conditional probability.
This estimate of the conditional probably looks a lot like a logistic regression estimate, as we can see in this graph.
In fact, we can show mathematically that the naive Bayes approach is similar to the logistic regression
approach in this particular case.
But we're not going to show that derivation here.

------------------------------------------------------------------------------------------------------------------

Controlling Prevalence

One nice feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence.
Using our sample, we estimated the conditional probabilities and the prevalence pi.
If we use hats to denote the estimates, we can rewrite the estimate of the conditional probability
with this formula.

As we discussed, our sample has much lower prevalence than the general population.
We only have 23% women.
So if we use our rule that the conditional probability has to be bigger than 0.5 to predict females,
our accuracy will be affected due to the low sensitivity, which we can see by typing this code.

y_hat_bayes <- ifelse(p_hat_bayes > 0.5, "Female", "Male")
sensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))
#> [1] 0.263

Again, this is because the algorithm gives more weight to specificity to account for the low prevalence.
You can see that we have very high specificity by typing this code.

specificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))
#> [1] 0.953

This is due mainly to the fact that pi hat is substantially less than 0.5,
so we tend to predict male more often than female.
It makes sense for a machine learning algorithm to do this in our sample
because we do have a higher percentage of males.
But if we were to extrapolate this to the general population,
our overall accuracy would be affected by the low sensitivity.
The Naive Bayes approach gives us a direct way to correct this,
since we can simply force our estimate of pi to be different.
So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule,
we could simply change pi hat.
Here in this code, we changed it to 0.5.

p_hat_bayes_unbiased <- f1*0.5 / (f1*0.5 + f0*(1-0.5)) 
y_hat_bayes_unbiased <- ifelse(p_hat_bayes_unbiased> 0.5, "Female", "Male")

Now note the difference in sensitivity and the better balance.
We can see it using this code.

sensitivity(data = factor(y_hat_bayes_unbiased), reference = factor(test_set$sex))
#> [1] 0.712
specificity(data = factor(y_hat_bayes_unbiased), reference = factor(test_set$sex))
#> [1] 0.821

This plot shows us that the new rule also gives us a very intuitive cutoff between 66 and 67,
which is about the middle of the female and male average heights.


qplot(x, p_hat_bayes_unbiased, geom = "line") + 
  geom_hline(yintercept = 0.5, lty = 2) + 
  geom_vline(xintercept = 67, lty = 2)
  
  
----------------------------------------------------------------------------------------------------------------

qda and lda


Quadratic discriminant analysis, or QDA, is a version of Naive Bayes in which we assume
that the conditional probabilities for the predictors are multivariate normal.
So the simple example we described in our Naive Bayes video was actually QDA.
In this video, we're going to look at a slightly more complicated example where we have two predictors.
It's the 2 or 7 example that we've previously seen.
We can load it with this code.

data("mnist_27")

In this case, we have two predictors.
So we "assume that their conditional distribution is bivariate normal".
This implies that we need to estimate two averages, two standard deviations,
and a correlation for each case, the 7s and the 2s.
Once we have these, we can approximate the conditional distributions.

conditional distributions
fx1,x2|y=1
fx1,x2|y=0

We can easily estimate these parameters from the data using this simple code.

params <- mnist_27$train %>% 
  group_by(y) %>% 
  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), 
            sd_1= sd(x_1), sd_2 = sd(x_2), 
            r = cor(x_1,x_2))
params
#> # A tibble: 2 x 6
#>   y     avg_1 avg_2   sd_1   sd_2     r
#>   <fct> <dbl> <dbl>  <dbl>  <dbl> <dbl>
#> 1 2     0.129 0.283 0.0702 0.0578 0.401
#> 2 7     0.234 0.288 0.0719 0.105  0.455


We can also visually demonstrate the approach.
We plot the data and use contour plots to give an idea of what the two estimated normal densities look like.

mnist_27$train %>% mutate(y = factor(y)) %>% 
  ggplot(aes(x_1, x_2, fill = y, color=y)) + 
  geom_point(show.legend = FALSE) + 
  stat_ellipse(type="norm", lwd = 1.5)

We show a curve representing a region that includes 95% of the points.
Once you've estimated these two distributions, this defines an estimate for the conditional probability of y
equals 1 given x1 and x2.
We can the caret package to fit the model and obtain predictors.
The code is quite simple and it looks like this.

library(caret)
train_qda <- train(y ~ ., 
                   method = "qda",
                   data = mnist_27$train)

We see that we obtain a relatively good accuracy of 0.82.


y_hat <- predict(train_qda, mnist_27$test)
confusionMatrix(data = y_hat, reference = mnist_27$test$y)$overall["Accuracy"]
#> Accuracy 
#>     0.82

The estimated conditional probability looks relatively similar to the true distribution, as we can see here.
Although the fit is not as good as the one we obtain with kernel smoothers, which we saw in a previous video.
And there's a reason for this.
The reason is that we can show mathematically that the boundary must be a quadratic function of the form x2
equals ax1 squared plus bx plus c.

x2 = ax1^2 + bx1 + c

One reason QDA does not work as well as the kernel method is perhaps
because the assumption of normality do not quite hold.
Although for the 2s, the bivariate normal approximation seems reasonable, for the 7 it does seem to be off.
Notice the slight curvature.

mnist_27$train %>% mutate(y = factor(y)) %>% 
  ggplot(aes(x_1, x_2, fill = y, color=y)) + 
  geom_point(show.legend = FALSE) + 
  stat_ellipse(type="norm") +
  facet_wrap(~y)


Although QDA work well here, it becomes harder to use as a number of predictors increases.
Here we have two predictors and have to compute four means, four standard deviations, and two correlations.
How many parameters would we have to estimate if instead of two predictors we had 10?
The main problem comes from estimating correlations for 10 predictors.
With 10, we have 45 correlations for each class.
In general, this formula tells us how many parameters we have to estimate, and it gets big pretty fast.

K x (2p + p x (p-1) / 2)

Once the number of parameters approaches the size of our data, the method becomes unpractical due to overfitting.


A relatively simple solution to having too many parameters is to assume that the correlations structure is the same
for all classes. This reduces the number of parameters we need to estimate.  In the example we've been examining,
we can just compute just one standard of deviation and one correlation. So the parameters would look something like this.
We use this code to estimate.

params <- mnist_27$train %>% 
  group_by(y) %>% 
  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), sd_1= sd(x_1), sd_2 = sd(x_2), r = cor(x_1,x_2))

params <-params %>% mutate(sd_1 = mean(sd_1), sd_2=mean(sd_2), r=mean(r))
params 
#> # A tibble: 2 x 6
#>   y     avg_1 avg_2   sd_1   sd_2     r
#>   <fct> <dbl> <dbl>  <dbl>  <dbl> <dbl>
#> 1 2     0.129 0.283 0.0710 0.0813 0.428
#> 2 7     0.234 0.288 0.0710 0.0813 0.428


Now the conditional distribution would look like this.


Notice how the size of the elipses as well as angle are same, this is because they have the same standard of deviation
as the assumption.
When we force this assumption we can show mathematically that the boundrary is alined just as logistic regression.
For this reason, we call this method the linear discriminant analysis (LDA).

Here is the estimate of the conditional probability we obtained when using LDA. 
In this case the lack of flexibility does not permit us to obtain a good estimate. 

train_lda <- train(y ~ .,
                   method = "lda",
                   data = mnist_27$train)
y_hat <- predict(train_lda, mnist_27$test)
confusionMatrix(data = y_hat, reference = mnist_27$test$y)$overall["Accuracy"]
#> Accuracy 
#>     0.75

We can see that the accuracy is actually quite low.

--------------------------------------------------------------------------------------------------------------

Case Study: More than Three Classes


In this video, we will give a slightly more complex example, one with three classes instead of two.
We first create a dataset similar to the two or seven dataset.
Except now we have one, twos, and sevens.
We can generate that dataset using this rather complex code.


if(!exists("mnist")) mnist <- read_mnist()

set.seed(3456)
index_127 <- sample(which(mnist$train$labels %in% c(1,2,7)), 2000)
y <- mnist$train$labels[index_127] 
x <- mnist$train$images[index_127,]
index_train <- createDataPartition(y, p=0.8, list = FALSE)

## get the quandrants
#temporary object to help figure out the quandrants
row_column <- expand.grid(row=1:28, col=1:28) 
upper_left_ind <- which(row_column$col <= 14 & row_column$row <= 14)
lower_right_ind <- which(row_column$col > 14 & row_column$row > 14)

#binarize the values. Above 200 is ink, below is no ink
x <- x > 200 

#cbind proportion of pixels in upper right quandrant and
##proportion of pixes in lower rigth quandrant
x <- cbind(rowSums(x[ ,upper_left_ind])/rowSums(x), 
           rowSums(x[ ,lower_right_ind])/rowSums(x)) 

train_set <- data.frame(y = factor(y[index_train]),
                        x_1 = x[index_train,1],
                        x_2 = x[index_train,2])
test_set <- data.frame(y = factor(y[-index_train]),
                       x_1 = x[-index_train,1],
                       x_2 = x[-index_train,2])


Once we're done, we obtain training set and a test set.
Here we're showing the data for the training set.

train_set %>% 
  ggplot(aes(x_1, x_2, color=y)) + 
  geom_point()

You can see the x1 and x2 predictors.
And then in color, we're showing you the different labels, the different categories.
The ones are in red, the greens are the twos, and the blue points are the sevens.
As an example, we'll fit a qda model.
We'll use the caret package.
So all we do is type this piece of code.

train_qda <- train(y ~ .,
                   method = "qda",
                   data = train_set)


So how do things differ now?
First note that we estimate three conditional probabilities, although they all have to add up to 1.
So if you type predict with type probability, you now get a matrix with three columns, a probability for the ones,
a probability for the two, a probability for the sevens.

predict(train_qda, test_set, type = "prob") %>% head()
#>        1     2      7
#> 1 0.2223 0.660 0.1180
#> 2 0.1926 0.454 0.3539
#> 3 0.6275 0.322 0.0505
#> 4 0.0462 0.101 0.8529
#> 5 0.2167 0.623 0.1604
#> 6 0.1267 0.335 0.5383

We predict the one with the highest probability.
So for the first observation, we would predict a two.
And now our predictors are one of three classes.
If we use the predict function, with the default setting of just giving you the outcome, we get twos, ones, and sevens.

predict(train_qda, test_set)
#>   [1] 2 2 1 7 2 7 2 1 7 1 7 7 2 7 7 1 1 7 1 7 7 1 7 7 7 7 1 1 1 2 1 7 2 7 1
#>  [36] 7 2 7 2 7 7 7 7 1 7 7 1 2 2 7 1 7 2 2 1 7 7 2 2 1 7 1 1 1 2 2 1 1 1 2
#>  [71] 7 1 7 7 2 1 7 1 7 2 7 1 2 2 1 2 2 2 1 1 1 7 1 2 1 7 2 2 2 7 1 1 1 7 1
#> [106] 7 7 7 2 7 1 7 7 2 1 2 1 1 2 1 2 2 1 2 1 7 1 1 2 1 7 2 7 1 2 1 2 1 2 1
#> [141] 1 1 7 2 2 7 1 1 2 2 2 7 7 1 7 1 7 7 2 1 2 7 7 1 7 7 7 1 7 7 2 2 1 2 2
#> [176] 1 2 2 2 1 2 7 2 7 7 7 1 7 7 2 2 1 2 1 2 7 1 7 7 1 1 1 7 1 1 7 2 1 7 7
#> [211] 2 2 1 7 2 2 2 2 7 2 2 1 2 2 1 1 1 7 1 7 1 7 7 7 2 7 1 1 2 2 1 7 1 7 7
#> [246] 7 1 1 7 1 1 7 2 1 2 1 2 1 7 1 7 1 2 7 7 7 7 7 2 1 7 1 7 2 1 7 7 1 7 7
#> [281] 7 2 7 1 2 7 2 2 7 2 2 7 2 1 2 1 1 1 7 1 1 7 7 1 7 2 1 7 7 7 7 1 2 2 7
#> [316] 7 1 1 7 1 2 1 2 1 7 7 1 1 1 7 1 2 7 7 1 1 7 2 7 7 7 1 7 7 7 7 7 2 2 1
#> [351] 7 7 2 1 2 2 7 7 1 7 7 1 1 7 7 1 1 1 2 1 7 2 7 2 7 1 2 2 1 1 7 2 7 2 1
#> [386] 2 7 7 7 2 7 1 1 7 1 7 2 7 7
#> Levels: 1 2 7

The confusion matrix is a three-by-three table now because we can make two kinds of mistakes with the ones,
two kinds of mistakes with the two, and two kinds of mistakes with the sevens.
You can see it here.


confusionMatrix(predict(train_qda, test_set), test_set$y)
#> Confusion Matrix and Statistics
#> 
#>           Reference
#> Prediction   1   2   7
#>          1 111  17   7
#>          2  14  80  17
#>          7  19  25 109
#> 
#> Overall Statistics
#>                                         
#>                Accuracy : 0.752         
#>                  95% CI : (0.706, 0.794)
#>     No Information Rate : 0.361         
#>     P-Value [Acc > NIR] : <2e-16        
#>                                         
#>                   Kappa : 0.627         
#>  Mcnemar's Test P-Value : 0.0615        
#> 
#> Statistics by Class:
#> 
#>                      Class: 1 Class: 2 Class: 7
#> Sensitivity             0.771    0.656    0.820
#> Specificity             0.906    0.888    0.835
#> Pos Pred Value          0.822    0.721    0.712
#> Neg Pred Value          0.875    0.854    0.902
#> Prevalence              0.361    0.306    0.333
#> Detection Rate          0.278    0.201    0.273
#> Detection Prevalence    0.338    0.278    0.383
#> Balanced Accuracy       0.838    0.772    0.827

The accuracy's still at one number because it just basically computes how often we make the correct prediction.

confusionMatrix(predict(train_qda, test_set), test_set$y)$overal["Accuracy"]
#> Accuracy 
#>    0.752

Note that for sensitivity and specificity, we have a pair of values for each class.
This is because to define these terms, we need a binary outcome.
We therefore have three columns, one for each class as a positive and the other two as the negatives.
Finally, we can visualize what parts of the regions are called ones, twos, and seven by simply plotting
the estimated conditional probability.

GS <- 150
new_x <- expand.grid(x_1 = seq(min(train_set$x_1), max(train_set$x_1), len=GS),
                     x_2 = seq(min(train_set$x_2), max(train_set$x_2), len=GS))
new_x %>% mutate(y_hat = predict(train_qda, new_x)) %>%
  ggplot(aes(x_1, x_2, color = y_hat, z = as.numeric(y_hat))) +
  geom_point(size = 0.5, pch = 16) + 
  stat_contour(breaks=c(1.5, 2.5),color="black") + 
  guides(colour = guide_legend(override.aes = list(size=2)))
  

Let's see how it looks like for lda.

train_lda <- train(y ~ .,
                   method = "lda",
                   data = train_set)

confusionMatrix(predict(train_lda, test_set), test_set$y)$overal["Accuracy"]
#> Accuracy 
#>    0.664

We can train the model like this.
The accuracy is much worse, and it is because our boundary regions have three lines.
This is something we can show mathematically.

train_knn <- train(y ~ .,
                   method = "knn",
                   tuneGrid = data.frame(k = seq(15, 51, 2)),
                   data = train_set)

confusionMatrix(predict(train_knn, test_set), test_set$y)$overal["Accuracy"]
#> Accuracy 
#>    0.769


The results for knn are actually much better.
Look how higher the accuracy is.
And we can also see that the estimated conditional probability is much more flexible, as we can see in this plot.


new_x %>% mutate(y_hat = predict(train_knn, new_x)) %>%
  ggplot(aes(x_1, x_2, color = y_hat, z = as.numeric(y_hat))) +
  geom_point(size = 0.5, pch = 16) + 
  stat_contour(breaks=c(1.5, 2.5),color="black") + 
  guides(colour = guide_legend(override.aes = list(size=2)))

Note that the reason that qda and, in particularly, lda are not working well is due to lack of fit.
We can see that by plotting the data and noting
that at least the ones are definitely not bivariate normally distributed.

train_set %>% mutate(y = factor(y)) %>% 
  ggplot(aes(x_1, x_2, fill = y, color=y)) + 
  geom_point(show.legend = FALSE) + 
  stat_ellipse(type="norm") 
  

So in summary, generating models can be very powerful but only 
when we're able to successfully approximate the joint distribution of predictor's condition on each class.


--------------------------------------------------------------------------------------------------------

Q1:
Create a dataset of samples from just cerebellum and hippocampus, two parts of the brain, and a predictor matrix with 10 randomly selected columns using the following code:

set.seed(1993)
data("tissue_gene_expression")
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]
Use the train function to estimate the accuracy of LDA.

What is the accuracy?

set.seed(1993)
data("tissue_gene_expression")
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]

fit <- train(x, y, method = "lda")
fit$results
0.8707879


Q2:
In this case, LDA fits two 10-dimensional normal distributions. Look at the fitted model by looking at the finalModel component of the result of train. Notice there is a component called means that includes the estimated means of both distributions. Plot the mean vectors against each other and determine which predictors (genes) appear to be driving the algorithm.

Which TWO genes appear to be driving the algorithm?


fit$finalModel

t(fit$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(cerebellum, hippocampus, label = predictor_name)) +
	geom_point() +
	geom_text() +
	geom_abline()
	
	
RAB1B
OAZ2


Q3:

Repeat the exercise in Q1 with QDA.

Create a dataset of samples from just cerebellum and hippocampus, two parts of the brain, and a predictor matrix with 10 randomly selected columns using the following code:

set.seed(1993)
data("tissue_gene_expression")
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]
Use the train function to estimate the accuracy of QDA.

What is the accuracy?

fit <- train(x, y, method = "qda")
fit$results
0.8147954


Q4:

Which TWO genes drive the algorithm when using QDA instead of LDA?

t(fit$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(cerebellum, hippocampus, label = predictor_name)) +
	geom_point() +
	geom_text() +
	geom_abline()
	
RAB1B
OAZ2


Q5:

One thing we saw in the previous plots is that the values of the predictors correlate in both groups: some predictors are low in both groups and others high in both groups. The mean value of each predictor found in colMeans(x) is not informative or useful for prediction and often for purposes of interpretation, it is useful to center or scale each column. This can be achieved with the preProcessing argument in train. Re-run LDA with preProcessing = "scale". Note that accuracy does not change, but it is now easier to identify the predictors that differ more between groups than based on the plot made in Q2.

Which TWO genes drive the algorithm after performing the scaling?


fit <- train(x, y, method = "lda", preProcess="center")
fit$results


t(fit$finalModel$means) %>% data.frame() %>%
  mutate(predictor_name = rownames(.)) %>%
  ggplot(aes(cerebellum, hippocampus, label = predictor_name)) +
  geom_point() +
  geom_text() +
  geom_abline()

OAZ2
SPI1


Q6:

Now we are going to increase the complexity of the challenge slightly: we will consider all the tissue types. Use the following code to create your dataset:

set.seed(1993)
data("tissue_gene_expression")
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x
x <- x[, sample(ncol(x), 10)]
What is the accuracy using LDA?

fit <- train(x, y, method = "lda", preProcess="center")
fit$results

0.8194837

