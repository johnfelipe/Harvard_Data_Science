Regularization

In this video, we're going to introduce the concept of regularization
and show how it can improve our results even more.
This is one of the techniques that was used by the winners of the Netflix challenge.
All right.
So how does it work?
Note that despite the large movie to movie variation,
our improvement in residual mean square error when we just included the movie effect was only about 5%.
So let's see why this happened.
Let's see why it wasn't bigger.
Let's explore where we made mistakes in our first model when we only used movies.
Here are 10 of the largest mistakes that we made when only using the movie effects in our models.
Here they are.

test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(residual = rating - (mu + b_i)) %>%
  arrange(desc(abs(residual))) %>% 
  select(title,  residual) %>% slice(1:10) %>% knitr::kable()

Note that these all seem to be obscure movies and in our model many of them obtained large predictions.
So why did this happen?
To see what's going on, let's look at the top 10 best
movies in the top 10 worst movies based on the estimates of the movie effect b hat i.
So we can see the movie titles, we're going to create a database that includes movie ID and titles using
this very simple code.

movie_titles <- movielens %>% 
  select(movieId, title) %>%
  distinct()

movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i) %>% 
  slice(1:10) %>%  
  knitr::kable()

So here are the best 10 movies according to our estimates.
America is number one, Love and Human Remains also number one, Infer L number one.
Look at the rest of the movies in this table.
And here are the top 10 worst movies.

movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i) %>% 
  slice(1:10) %>%  
  knitr::kable()

The first one started with Santa with Muscles.
Now they all have something in common.
They're all quite obscure.
So let's look at how often they were rated.
Here's the same table, but now we include the number of ratings they received in our training set.

train_set %>% count(movieId) %>% 
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()


We can see the same for the bad movies.

train_set %>% count(movieId) %>% 
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()

So the supposed best and worst movies were rated by very few users, in most cases just one.
These movies were mostly obscure ones.
This is because with just a few users, we have more uncertainty, therefore larger estimates of bi, negative or positive,
are more likely when fewer users rate the movies.
These are basically noisy estimates that we should not trust, especially when it comes to prediction.
Large errors can increase our residual mean squared error, so we would rather be conservative when we're not sure.
Previously we've learned to compute standard errors and construct confidence intervals to account
for different levels of uncertainty.
However, when making predictions we need one number, one prediction, not an interval.
For this, we introduce the concept of regularization.
"Regularization permits us to penalize large estimates that come from small sample sizes."
It has commonalities with the Bayesian approaches that shrunk predictions.
The general idea is to add a penalty for large values of b to the sum of squares equations that we minimize.
So having many large b's makes it harder to minimize the equation that we're trying to minimize.
One way to think about this is that if we were to fit an effect to every rating, we
could of course make the sum of squares equation by simply making each b match its respective rating y.
This would yield an unstable estimate that changes drastically with new instances of y.
Remember y is a random variable.
But by penalizing the equation, we optimize to b bigger when the estimate b are far from zero.
We then shrink the estimates towards zero.
Again, this is similar to the Bayesian approach we've seen before.
So this is what we do.
To estimate the b's instead of minimizing the residual sum of squares
as is done by least squares, we now minimize this equation.

1/N Eu,i (yu,i - mu - bi) ^2 + lamda Ei bi^2

Note the penalty term. (lamda Ei bi^2)
The first term is just the residual sum of squares
and the second is a penalty that gets larger when many b's are large.
Using calculus, we can actually show that the values
of b that minimized equation are given by this formula, where ni is a number of ratings b for movie i.

^bi(lamda) = 1/(lamda + ni) Eu=1,ni (Yu,i - ^mu)

Note that this approach will have our desired effect.
When ni is very large which will give us a stable estimate, then lambda is effectively ignored because ni plus lambda
is about equal to ni.
However, when ni is small, then the estimate of bi is shrunken towards zero.
The larger lambda, the more we shrink.
So let's compute these regularized estimates of vi using lambda equals to 3.0.
Later we see why we picked this number.
So here is the code.

lambda <- 3
mu <- mean(train_set$rating)
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 

To see how the estimates shrink, let's make a plot of the regularized estimate versus the least square estimates
with the size of the circle telling us how large ni was.

data_frame(original = movie_avgs$b_i, 
           regularlized = movie_reg_avgs$b_i, 
           n = movie_reg_avgs$n_i) %>%
  ggplot(aes(original, regularlized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)

You can see that when n is small, the values are shrinking more towards zero.
All right, so now let's look at our top 10 best movies based on the estimates we got when using regularization.

train_set %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()


Note that the top five movies are now All About Eve, Shawshank Redemption, The Godfather, The Godfather
II, and the Maltese Falcons.
This makes much more sense.

train_set %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()

We can also look at the worst movies and the worst five are Battlefield Earth, Joe's Apartment, Speed 2, Cross
Control, Super Mario Bros, and Police Academy 6: City Under Siege.
Again, this makes sense.
So do we improve our results?

predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  .$pred

model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()



We certainly do.
We get the residual mean squared error all the way down to 0.885 from 0.986.
So this provides a very large improvement.
Now note that lambda is a tuning parameter.
We can use cross-fertilization to choose it.
We can use this code to do this.

lambdas <- seq(0, 10, 0.25)

mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
lambdas[which.min(rmses)]


And we see why we picked 3.0 as lambda.
One important point.
Note that we show this as an illustration and in practice, we should be using full cross-validation just
on a training set without using the test it until the final assessment.

We can also use regularization to estimate the user effect.
The equation we would minimize would be this one now.


1/N Eu,i (yi,u - mu - bi - bu)^2 + lamda (Ei bi^2 + Eu bu^2

It includes the parameters for the user effects as well.
The estimates that minimizes can be found similarly to what we do previously.
Here we again use cross-validation to pick lambda.
The code looks like this, and we see what lambda minimizes our equation.

lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(train_set$rating)
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
    return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses) 


lambda <- lambdas[which.min(rmses)]
lambda

For the full model including movie and user effects, the optimal lambda is 3.75.

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()

And we can see that we indeed improved our residual mean squared error.
Now it's 0.881.


----------------------------------------------------------------------------------------------------------

The exercises in Q1-Q8 work with a simulated dataset for 100 schools. This pre-exercise setup walks you through the code needed to simulate the dataset.

An education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let's simulate a dataset for 100 schools. First, let's simulate the number of students in each school, using the following code:

set.seed(1986)
n <- round(2^rnorm(1000, 8, 1))

Now let's assign a true quality for each school that is completely independent from size. This is the parameter we want to estimate in our analysis. The true quality can be assigned using the following code:

set.seed(1)
mu <- round(80 + 2*rt(1000, 5))
range(mu)
schools <- data.frame(id = paste("PS",1:100),
                      size = n,
                      quality = mu,
                      rank = rank(-mu))

We can see the top 10 schools using this code: 

schools %>% top_n(10, quality) %>% arrange(desc(quality))

Now let's have the students in the school take a test. There is random variability in test taking, so we will simulate the test scores as normally distributed with the average determined by the school quality with a standard deviation of 30 percentage points. This code will simulate the test scores:

set.seed(1)
scores <- sapply(1:nrow(schools), function(i){
       scores <- rnorm(schools$size[i], schools$quality[i], 30)
       scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))


Q1:
What are the top schools based on the average score? Show just the ID, size, and the average score.

Report the ID of the top school and average score of the 10th school.

What is the ID of the top school?
Note that the school IDs are given in the form "PS x" - where x is a number. Report the number only.

schools %>% arrange(desc(score)) %>% top_n(10, score)
67

What is the average score of the 10th school?
88.09490



Q2:
Compare the median school size to the median school size of the top 10 schools based on the score.

What is the median school size overall?
median(schools$size)
261

What is the median school size of the of the top 10 schools based on the score?
df <- schools %>% arrange(desc(score)) %>% top_n(10, score)
median(df$size)
136


Q3:
According to this analysis, it appears that small schools produce better test scores than large schools. 
Four out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so 
that quality and size were independent. Repeat the exercise for the worst 10 schools.

What is the median school size of the bottom 10 schools based on the score?

df <- schools %>% arrange(score) %>% top_n(-10, score)
median(df$size)
146


Q4:
Correct (1/1 point) Review
Q4
1 point possible (graded)
From this analysis, we see that the worst schools are also small. Plot the average score versus school size to see 
what's going on. Highlight the top 10 schools based on the true quality. Use a log scale to transform for the size.

What do you observe?
schools %>% ggplot(aes(score,size)) + geom_point()
schools %>% arrange(desc(quality)) %>% top_n(10, quality)

or 


schools %>% ggplot(aes(size, score)) +
  geom_point(alpha = 0.5) +
  geom_point(data = filter(schools, rank<=10), col = 2) 
schools %>% top_n(10, score) %>% arrange(desc(score)).
The standard error of the score has larger variability when the school is smaller, which is why both the best and the worst schools are more likely to be small. 


Q5:
Let's use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 
0. To apply regularization here, we first need to define the overall average for all schools, using the following code:

	overall <- mean(sapply(scores, mean))
Then, we need to define, for each school, how it deviates from that average.

Write code that estimates the score above the average for each school but dividing by n + alpha instead of n, 
with n the schools size and alpha a regularization parameters. Try alpha = 25.

What is the ID of the top school with regularization?
Note that the school IDs are given in the form "PS x" - where x is a number. Report the number only.
schools %>% 
  mutate(Q4 = overall + (score - overall) * size/(size + 25)) %>% arrange(desc(Q4)) %>% top_n(10, Q4)
91
  
  
What is the regularized score of the 10th school?
86.90070


Q6:
Notice that this improves things a bit. The number of small schools that are not highly ranked is now lower. 
Is there a better alpha? Find the alpha that minimizes the RMSE = 1/100 E100, i=1 (quality-estimate)^2.

What value of alpha gives the minimum RMSE?

alphas <- seq(10,250)
rmse <- sapply(alphas, function(alpha){
  score_reg <- sapply(scores, function(x) overall+sum(x-overall)/(length(x)+alpha))
  mean((score_reg - schools$quality)^2)
})
plot(alphas, rmse)
alphas[which.min(rmse)] 

128


Q7:
Rank the schools based on the average obtained with the best . Note that no small school is incorrectly included.

What is the ID of the top school now?
Note that the school IDs are given in the form "PS x" - where x is a number. Report the number only.
schools %>% 
  mutate(Q4 = overall + (score - overall) * size/(size + 128)) %>% arrange(desc(Q4)) %>% top_n(10, Q4)
91

What is the regularized average score of the 10th school now?
85.35335

Q8:
A common mistake made when using regularization is shrinking values towards 0 that are not centered around 0. 
For example, if we don't subtract the overall average before shrinking, we actually obtain a very similar result. 
Confirm this by re-running the code from the exercise in Q6 but without removing the overall mean.

What value of  gives the minimum RMSE here?

alphas <- seq(10,250)
rmse <- sapply(alphas, function(alpha){
	score_reg <- sapply(scores, function(x) sum(x)/(length(x)+alpha))
	mean((score_reg - schools$quality)^2)
})
plot(alphas, rmse)
alphas[which.min(rmse)]  

10


