
Knn

In this video, we will learn our first machine learning algorithm, the k-nearest neighbors algorithm.
To demonstrate it, we're going to use the digits data with two predictors that we created in a previous video.

data("mnist_27")
mnist_27$test%>% ggplot(aes(x_1, x_2, color = y)) +
  geom_point()


K-nearest neighbors is related to smoothing.
To see this, we will think about the conditional probably,
the probably of being a seven, y equals 1, given the two predictors.

p(x1,x2) = Pr(Y=1 | X1 = x1, X2 = x2)

This is because the zeros and ones we observe are noisy because some of the regions of the conditional probability
are not close to zero or one, which means that you can go either way sometimes.
So we have to estimate the conditional probability.
How do we do this?
We're going to try smoothing.
K-nearest neighbors is similar to bin smoothing.
But it is easier to adapt to multiple dimensions.
We first defined the distance between observations based on the features.
Basically, for any point for which you want to estimate the conditional probability, we look at the k-nearest points
and then take an average of these points.
We refer to the set of points used to compute the average as a neighborhood.
Due to the connection we described earlier between conditional expectations and conditional probabilities,
this gives us the estimated conditional probability,
just like bin smoothers gave us an estimated trend.

P^ (x1,x2)
estimated conditional probabily


We can control, flexibility of our estimate through k.
Larger Ks result in smaller estimates, while smaller Ks result in more flexible and more wiggly estimates.

larger K, smoother esimate
smaller K, more flexible but wigglier estimate

So let's implement k-nearest neighbors.
We're going to compare it to logistic regression, which will be the standard we need to beat.
We can write this code to compute the glm predictions.


library(caret)
fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family="binomial")
p_hat_logistic <- predict(fit_glm, mnist_27$test)
y_hat_logistic <- factor(ifelse(p_hat_logistic > 0.5, 7, 2))
confusionMatrix(data = y_hat_logistic, reference = mnist_27$test$y)$overall[1]

And notice that we have an accuracy of 0.76.
Now let's compare this to knn.
We will use the function knn3 which comes with the caret package.
If we look at the help file of this package, we see that we can call it in one of two ways.
In the first, we specify a formula and a data frame.
The data frame contains all the data to be used.
The formula has the form outcome tilde predictor 1 plus predictor 1 plus predictor 3 and so on.

outcome ~ predictor_1 + predictor_2 + predictor_3

So in this case where we only have two predictors, we would type y-- those are the outcomes-- tilde x1 plus x2.

y ~ x_1 + x_2

But if we're going to use all the predictors, we can use a shortcut, and it's the dot.
We would type y tilde dot.\

y ~ .

And that says use all the predictors.
So the call to knn3 looks simply like this.

knn_fit <- knn3(y ~ ., data = mnist_27$train)

The second way to call this function is that the first argument being the
matrix predictors and the second, a vector of outcomes.
So the code would look like this instead.

x <- as.matrix(mnist_27$train[,2:3])
y <- mnist_27$train$y
knn_fit <- knn3(x, y)

We would define our matrix with the predictors.
Then when we would define a vector with the outcomes.
And then we would call it simply like this.
The reason we have two ways of doing this is because the formula is a quicker, simpler way 
to write it when we're in a hurry.
But once we face large data sets, we will want to use the matrix approach, the second approach.

All right, now, for this function, we also need to pick a parameter, the number of neighbors to include.
Let's start with the default, which is k equals 5.
We can write it explicitly like this.

knn_fit <- knn3(y ~ ., data = mnist_27$train, k = 5)

Because this data set is balanced-- there's many twos as there are sevens-- and we care just as much about sensitivity
as we do about specificity-- both mistakes are equally bad-- we will use accuracy to quantify performance.
The predict function for this knn function produces either a probability for each class,
or it could actually produce the outcome that maximizes the probability, the outcome with the highest probability.
So we're going to use the code predict, the fitted object, the new data that we're predicting for.
That's the test data set.
And then we're going to type, type equals class.
This will give us the actual outcomes that are predicted.


knn_fit: the fitted objectd
mnist_27$test: the new data that we're predicting for. That's the test data set.

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(data = y_hat_knn, reference = mnist_27$test$y)$overall["Accuracy"]


So once we have this, we can compute our accuracy using the confusion matrix formula like this.
And we see that we already have an improvement over the logistic regression.
Our accuracy is 0.815.

----------------------------------------------------------------------------------------------------------

Overtraining and Oversmoothing

Now to see why we improved over logistic regression in this case that we only have two predictors,
we can actually make some visualizations.
Here is the true conditional probability on the left.
And on the right, you can see the estimate that we obtained with knn with five neighbors.
So you see that the estimate has the essence of the shape of the true conditional probability.
Therefore we do better than logistic regression.
However, we can probably do better.
Because if you look closely, this estimate, we see some isles of blue in the red areas.
Intuitively this does not make much sense.
Why are they on their own like that?
This is due to what we call overtraining.
To understand what overtraining is, notice that we have higher accuracy when we predict on a training set
than we compare on a test set.
We can do it using this code.


y_hat_knn <- predict(knn_fit, mnist_27$train, type = "class")
confusionMatrix(data = y_hat_knn, 
                reference = mnist_27$train$y)$overall["Accuracy"]
#> Accuracy 
#>    0.882

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(data = y_hat_knn, reference = mnist_27$test$y)$overall["Accuracy"]
#> Accuracy 
#>    0.815


You can see that the accuracy computed on the training side is quite higher.
It's 0.882 compared to what we get on the test set, which is only 0.815.
This is because we overtrained.
Overtraining is at its worst when we set k equals to 1.
k=1
With k equals to 1, the estimate for each point in the training set is obtained with just the y corresponding to that
point because you are your closest neighbor.
So in this case, we obtain practically perfect accuracy in the training set because each point is used to predict itself.

Perfect accuracy will occur when we have unique predictors, which we almost do have here.
So you can see that when we use a k equals to 1, our accuracy on a training set is 0.995, five almost perfect accuracy.
However, when we check on the test set, the accuracy is actually worse than with logistic regression.
It's only 0.735.
We can see that using this code.

knn_fit_1 <- knn3(y ~ ., data = mnist_27$train, k = 1)
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$train, type = "class")
confusionMatrix(data=y_hat_knn_1, 
                reference=mnist_27$train$y)$overall["Accuracy"]


To see the over-fitting in a figure, we can plot the data
and then use contours to show what divides the twos from the sevens.
And this is what you get when you use k equals 1.
Notice all the little islands that in the training set fit the data perfectly.
You'll have this little red point on its own, and a little island will be formed around it
so that you get the perfect prediction.
But once you look at the test set, that point is gone.
There's no red there anymore.
Now there's perhaps a blue, and you make a mistake.

The estimated conditional probability followed the training data too closely.
Although it's not as bad, we see this overtraining
with k equals 5, or the default. So we should consider a larger k.
Let's try an example.
Let's try a much larger example.
Let's try 401.
We can fit the model just by simply changing the k to 401 like this.

knn_fit_401 <- knn3(y ~ ., data = mnist_27$train, k = 401)
y_hat_knn_401 <- predict(knn_fit_401, mnist_27$test, type = "class")
confusionMatrix(data=y_hat_knn_401, reference=mnist_27$test$y)$overall["Accuracy"]
#> Accuracy 
#>     0.79

We see that the accuracy on a test set is only 0.79, not very good, a similar accuracy to logistic regression.
In fact, the estimates actually look quite similar.
On the left is logistic regression.
On the right is k-nearest neighbors with k equals 401.
The size of k is so large that it does not permit enough flexibility.
We're almost including half the data to compute each single estimated conditional probability.
We call this oversmoothing.
So how do we pick k?
Five seems to be too small.
401 seems to be too big.
Something in the middle might be better.
So what we can do is we can repeat what we just did for different values of k
So we can try all the odd numbers between 3 and 251.

ks <- seq(3, 251, 2)

And we'll do this using the map_df function to repeat what we just did for each k.
For comparative purposes, we will compute the accuracy by using both training set-- that's incorrect.
We shouldn't do that, but just for comparison we're going to do it--
and the test set, which is the correct way to do it.
The code looks simply like this.


library(purrr)
accuracy <- map_df(ks, function(k){
  fit <- knn3(y ~ ., data = mnist_27$train, k = k)
  
  y_hat <- predict(fit, mnist_27$train, type = "class")
  cm_train <- confusionMatrix(data = y_hat, reference = mnist_27$train$y)
  train_error <- cm_train$overall["Accuracy"]
  
  y_hat <- predict(fit, mnist_27$test, type = "class")
  cm_test <- confusionMatrix(data = y_hat, reference = mnist_27$test$y)
  test_error <- cm_test$overall["Accuracy"]
  
  list(train = train_error, test = test_error)
})


Once we run that code, we can now plot the accuracy against the value of k, and that looks like this.
First, note that the accuracy versus k plot is quite jagged.
We don't not expect this because small changes in k should not affect the algorithm's performance too much.
The jaggedness is explained by the fact that the accuracy
is computed on this sample and therefore is a random variable.
This demonstrates why we prefer to minimize the expectation loss, rather
than the loss we observe with one dataset.
We will soon learn a better way of estimating this expected loss.
Now, despite the noise present in the plot, we still see a general pattern.
Low values of k give low test set accuracy but high train set accuracy,
which is evidence of overtraining.
Large values of k result in low accuracy, which is evidence of oversmoothing.
The maximum is achieved somewhere between 25 and 41.
And the maximum accuracy is 0.85, substantially higher than logistic regression.
In fact, the resulting estimate with k equals
to 41 looks quite similar to the true conditional probability, as we see in this plot.


p1 <- plot_cond_prob() + ggtitle("True conditional probability")

knn_fit <- knn3(y ~ ., data = mnist_27$train, k = 41)
p2 <- plot_cond_prob(predict(knn_fit, newdata = mnist_27$true_p)[,2]) +
  ggtitle("kNN-41 estimate")
grid.arrange(p1, p2, nrow=1)

max(accuracy$test)


Now, is an accuracy of 0.85 what we should expect if we apply this algorithm in the real world?
The answer is actually no because we broke a golden rule of machine learning.
We selected the k using the test set.
So how do we select the k?
In the next videos, we introduce the important concept of cross-validation, which provides a way
to estimate the expected loss for a given method using only the training set.

----------------------------------------------------------------------------------------------

Q1:

Previously, we used logistic regression to predict sex based on height. Now we are going to use knn to do the same. Use the code described in these videos to select the F_1 measure and plot it against k. Compare to the F_1 of about 0.6 we obtained with regression. Set the seed to 1.

library(caret)
library(dslabs)
library(dplyr)
library(purrr)

data(heights)

ks <- seq(3, 251, 1)

y<-heights$sex

x<-heights$height

set.seed(1)

accuracy <- map_df(ks, function(k) {
  
  test_index<- createDataPartition(y, times=1, p=0.5, list=FALSE)
  
  train_set<- heights%>% slice(test_index)
  
  test_set<- heights%>% slice(-test_index)
  
  fit<- knn3(sex ~ height, data=train_set, k=k)
  
  y_hat<-predict(fit, test_set, type="class")
  
  F_val<-F_meas(data=y_hat, reference=factor(train_set$sex))
  
  list(k=k, F_val=F_val)
  
})


set.seed(1)
data("heights")
library(caret)
ks <- seq(1, 101, 3)
F_1 <- sapply(ks, function(k){
  test_index <- createDataPartition(heights$sex, times = 1, p = 0.5, list = FALSE)
  test_set <- heights[test_index, ]
  train_set <- heights[-test_index, ]
  fit <- knn3(sex ~ height, data = train_set, k = k)
  y_hat <- predict(fit, test_set, type = "class") %>% 
    factor(levels = levels(train_set$sex))
  F_meas(data = y_hat, reference = test_set$sex)
})
plot(ks, F_1)
max(F_1)


What is the max value of F_1? 0.6297872
At what value of k does the max occur? 38


------------------------------------------------------------------------------------------------------------

Q2:

Next we will use the same gene expression example used in the Comprehension Check: Distance exercises. You can load it like this:

library(dslabs)
data("tissue_gene_expression")
Split the data into training and test sets, and report the accuracy you obtain. Try it for k = 1, 3, 5, 7, 9, 11. Set the seed to 1.


library(dslabs)
data("tissue_gene_expression")

library(caret)
k <-c( 1, 3, 5, 7, 9, 11)

x <- tissue_gene_expression$x
y <- tissue_gene_expression$y

set.seed(1)
train_index<- createDataPartition(tissue_gene_expression$y, times=1, p=0.5, list=FALSE)

accuracy <- map_df(k, function(k) {
  test_set <- x[-train_index, ]
  train_set <- x[train_index, ]
  test_set_y = y[-train_index ] 
  train_set_y = y[train_index ] 
  fit<- knn3(train_set,train_set_y, k=k)
  y_hat<-predict(fit, test_set, type="class")
  test_error <- confusionMatrix(data=y_hat, reference=test_set_y)$overall["Accuracy"]
  print(k)
  print(test_error)
})


[1] 1
 Accuracy 
0.9784946 
[1] 3
 Accuracy 
0.9677419 
[1] 5
 Accuracy 
0.9892473 
[1] 7
 Accuracy 
0.9677419 
[1] 9
 Accuracy 
0.9569892 
[1] 11
 Accuracy 
0.9569892 
