
Caret Package

We have already learned about regression, logistic regression, and k-nearest neighbors as machine-learning algorithms.
In later sections, we learn several others.
And this is just a small subset of all the algorithms out there.
Many of these algorithms are implemented in R. However, they are distributed via different packages,
developed by different authors, and often use different syntax.
The caret package tries to consolidate these differences and provide consistency.
It currently includes 237 different methods, which are summarized in the following site.
We'll include the link in the courseware.

https://topepo.github.io/caret/available-models.html

Note that caret does not automatically install the packages needed to run these methods.
So to implement a package through caret, you still need to install the library.
The required package for each method is included in this page.
We'll include the link in the courseware.

https://topepo.github.io/caret/train-models-by-tag.html

The caret package also provides a function that performs cross-validation for us.
Here we provide some examples showing how we use this incredibly helpful package.
We will use the two-or-seven example to illustrate this.
You can load it like this.

data("mnist_27")

The train function lets us train different algorithms using similar syntax.
So for example, we can train a logistic regression model or a k-NN model using very similar syntax, like this.


library(caret)
train_glm <- train(y ~ ., method = "glm", data = mnist_27$train)
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)


Now, to make predictions, we can use the output of this function 
directly without needing to look at the specifics of predict.glm or predict.knn.
Instead, we can learn how to obtain predictions from the predict.train function.
Once we read this help page, we know how to use a predict function for these objects.
Here is the code to get the predictions for the logistic regression and the k-NN.

y_hat_glm <- predict(train_glm, mnist_27$test, type = "raw")
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")

Notice that the syntax is very similar.
We can also very quickly study the confusion matrix.d
For example, we can compare the accuracy of both these methods like this.


confusionMatrix(y_hat_glm, mnist_27$test$y)$overall["Accuracy"]
#> Accuracy 
#>     0.75
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]
#> Accuracy 
#>     0.84


----------------------------------------------------------------------------------------------------------

Tuning Parameters with Caret

When an algorithm includes a tuning parameter, train automatically uses cross-validation to decide
among a few default values.
To find out what parameter or parameters are optimized, you can read this page that explains it.
We'll include the link in the courseware.

http://topepo.github.io/caret/available-models.html

Or study the output of the following code.

getModelInfo("knn")

The get model info function can be used to get information of the method that you're interested in.
You can do a quick lookup using the model lookup function like this.

modelLookup("knn")
#>   model parameter      label forReg forClass probModel
#> 1   knn         k #Neighbors   TRUE     TRUE      TRUE

When we run this code, we see that for knn, the parameter that's optimized is k.
So if we run the function train with default values, you can quickly see the results of the cross-validation using
the ggplot function.

train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)

You can use the argument highlight to highlight the parameter that optimizes the algorithm.
So you can type this.

ggplot(train_knn, highlight = TRUE)

By default, the cross-validation is performed by testing on 25 bootstrap samples comprised of 25% of the observations.
Also, for the knn method, the default is to try out k=5, 7, and 9.
We already saw that 9 maximizes this.
But maybe there's another k that's even better.
So to change this, we need to use the tunegrid parameter in the train function.
The grid of values that are going to be compared must be supplied by a data frame with the column names
as specified by the parameters that you get in the model lookup output.
Here we present an example trying out 30 values between 9 and 67.
We need to use a column in k, so the data frame we use is going to be this one.

data.frame(k = seq(9, 67, 2))

Now, note that when running this code, we are fitting 30 versions of knn
to 25 bootstrap samples, so we're fitting 750 knn models.
And thus, running this code will take several seconds.
Here's the code.

train_knn <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))
ggplot(train_knn, highlight = TRUE)


In the plot, we can see the k that maximizes accuracy, but we can also access it using this code.

train_knn$bestTune
#>     k
#> 11 29


The bestTune component gives us the parameter that maximizes the accuracy.
We can also access the best-performing model using this code.

train_knn$finalModel
#> 29-nearest neighbor model
#> Training set outcome distribution:
#> 
#>   2   7 
#> 379 421

Now, if you apply the function predict to the output of the train function, 
it will use this best-performing model to make predictions.
Note that the best model was obtained using the training set.
We did not use the test set at all.
The cross-validation was performed on the training set.
So now, if we want to see the accuracy we obtain on the test set, which hasn't been used,
we can use the following code.

confusionMatrix(predict(train_knn, mnist_27$test, type = "raw"),
                mnist_27$test$y)$overall["Accuracy"]
#> Accuracy 
#>     0.84

Sometimes we like to change the way we perform cross-validation.
We might change the method, we might change how we do the partitions, et cetera.
If we want to do this, we need to use a trainControl function.
So for example, we can make the code that we just showed go a bit faster by using 10-fold cross-validation.
This means we're going to have 10 validation samples that use 10% of the observations each.

control <- trainControl(method = "cv", number = 10, p = .9)
train_knn_cv <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)),
                   trControl = control)
ggplot(train_knn_cv, highlight = TRUE)


Notice that if we plot the estimated accuracy versus k plot, we notice that the accuracy estimates are more
variable than in the previous example.
Now this is expected since we changed a number of samples we use to estimate accuracy.
In the first example, we used 25 bootstrap samples, and in this example, we use 10-fold cross-validation.
One more thing to point out.
Note that the train function also provides standard deviation values for each parameter that was tested.
This is obtained from the different validation sets.
So we can make a plot like this that shows the point estimates of the accuracy along with standard deviations.

train_knn$results %>% 
  ggplot(aes(x = k, y = Accuracy)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(x = k, 
                    ymin = Accuracy - AccuracySD, 
                    ymax = Accuracy + AccuracySD))
                    

To finish this example up, let's notice that the best-fitting knn 
model approximates the true condition of probability pretty well.
However, we do see that the boundary is somewhat wiggly.
This is because knn, like the basic bin smoother, does not use a smooth kernel.
To improve this, we could try loess.

By reading through the available models of the caret package, which
you can get to through this link, which we include in the course material, we see that we can use the gamLoess method.

https://topepo.github.io/caret/available-models.html

Also from the caret documentation link-- you can go to it here-- you can see that we need to install the gam
package if we have not done so already.

https://topepo.github.io/caret/train-models-by-tag.html


So we will type something like this.

install.packages("gam")

Then we will see that we have two parameters to optimize if we use this particular method.

modelLookup("gamLoess")
#>      model parameter  label forReg forClass probModel
#> 1 gamLoess      span   Span   TRUE     TRUE      TRUE
#> 2 gamLoess    degree Degree   TRUE     TRUE      TRUE


You can see this with the model lookup function, like this.
For this example, we'll keep the degree fixed at one.
We won't try out degree two.
But to try out different values for the span, we still have to include a column in the table with the named degree.
This is a requirement of the caret package.
So we would define a grid using the expand.grid function, like this.

grid <- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1)

Now, we use the default cross-validation control parameters, so we type code like this to train our model.

train_loess <- train(y ~ ., 
                   method = "gamLoess", 
                   tuneGrid=grid,
                   data = mnist_27$train)
ggplot(train_loess, highlight = TRUE)


Then, select the best-performing model, and now we can see the final result.
It performs similarly to knn.

confusionMatrix(data =predict(train_loess, mnist_27$test), 
                reference = mnist_27$test$y)$overall["Accuracy"]
#> Accuracy 
#>     0.85

However, we can see that the conditional probability estimate is indeed smoother than what we get with knn.

plot_cond_prob(predict(train_loess, mnist_27$true_p, type = "prob")[,2])

Note that not all parameters in machine-learning algorithms are tuned.
For example, in regression models or in LDA, we fit the best model using least squares estimates or maximum likelihood
estimates.
Those are not tuning parameters.
We obtained those using least squares, or MLE, or some other optimization technique.
Parameters that are tuned are parameters that we can change and then get an estimate of the model for each one.
So in k-nearest neighbors, the number of neighbors is a tuning parameter.
In regression, the number of predictors that we include could be considered a parameter that's optimized.
So in the caret package, in the train function, we only optimize parameters that are tunable.
So it won't be the case that, for example, in regression models, 
the caret package will optimize the regression coefficients that are estimated.
Instead, it will just estimate using least squares.
This is an important distinction to make when using the caret package-- 
knowing which parameters are optimized, and which ones are not.


---------------------------------------------------------------------------------------------------------

Q1:

In the exercise in Q6 from Comprehension Check: Trees and Random Forests, we saw that changing nodesize to 50 and 
setting maxnodes to 25 yielded smoother results. Let's use the train function to help us pick what the values of 
nodesize and maxnodes should be.

From the caret description of methods, we see that we can't tune the maxnodes parameter or the nodesize argument 
with randomForests. So we will use the __Rborist__ package and tune the minNode argument. Use the train function 
to try values minNode <- seq(25, 100, 25). Set the seed to 1.

Which value minimizes the estimated RMSE?

  set.seed(1)
  library(caret)
  fit <- train(y ~ ., method = "Rborist",   
               tuneGrid = data.frame(predFixed = 1, 
                                     minNode = seq(25, 100, 25)),
               data = dat)
  ggplot(fit)
  50
  

Q2:

Part of the code to make a scatterplot along with the prediction from the best fitted model is provided below.

library(caret)
dat %>% 
	mutate(y_hat = predict(fit)) %>% 
	ggplot() +
	geom_point(aes(x, y)) +
    #BLANK
Which code correctly can be used to replace #BLANK in the code above?
geom_step(aes(x, y_hat), col = 2)


Q3:

Use the rpart function to fit a classification tree to the tissue_gene_expression dataset. Use the train function 
to estimate the accuracy. Try out cp values of seq(0, 0.1, 0.01). Plot the accuracies to report the results of the 
best model. Set the seed to 1991.

Which value of cp gives the highest accuracy?



library(caret)
library(dslabs)
set.seed(1991)
data("tissue_gene_expression")

fit <- with(tissue_gene_expression, 
            train(x, y, method = "rpart",
                  tuneGrid = data.frame(cp = seq(0, 0.1, 0.01))))

ggplot(fit)  
0


Q4:
Study the confusion matrix for the best fitting classification tree from the exercise in Q3.

What do you observe happening for the placenta samples?

Placenta samples are being classified somewhat evenly across tissues. 
confusionMatrix(fit) will show the confusion matrix for the classification tree from the tissue gene expression dataset. Looking at the confusion matrix, you can see that placenta is classified somewhat evenly across different tissue types, and in fact, placentas are called endometriums more frequently than they are called placentas.


Q5:
Note that there are only 6 placentas in the dataset. By default, rpart requires 20 observations before splitting a node. That means that it is difficult to have a node in which placentas are the majority. Rerun the analysis you did in the exercise in Q3, but this time, allow rpart to split any node by using the argument control = rpart.control(minsplit = 0). Look at the confusion matrix again to determine whether the accuracy increases. Again, set the seed to 1991.

What is the accuracy now?
fit <- with(tissue_gene_expression, 
            train(x, y, method = "rpart",
                  tuneGrid = data.frame(cp = seq(0, 0.1, 0.01)),
                  control = rpart.control(minsplit=0)))

confusionMatrix(fit)
0.9141


Q6:
Plot the tree from the best fitting model of the analysis you ran in Q5.

Which gene is at the first split?
plot(fit$finalModel)
text(fit$finalModel)
GPA33


Q7:
We can see that with just seven genes, we are able to predict the tissue type. Now let's see if we can predict the 
tissue type with even fewer genes using a Random Forest. Use the train function and the rf method to train a Random 
Forest. Try out values of mtry ranging from seq(50, 200, 25) (you can also explore other values on your own). What 
mtry value maximizes accuracy? To permit small nodesize to grow as we did with the classification trees, use the 
following argument: nodesize = 1.

Note: This exercise will take some time to run. If you want to test out your code first, try using smaller values 
with ntree. Set the seed to 1991 again.

What value of mtry maximizes accuracy?
set.seed(1991)
fit_rf <-  with(tissue_gene_expression, 
                train(x,y, data=tissue_gene_expression, method="rf", 
                tuneGrid=data.frame(mtry=seq(50,200,25)), nodesize=1) )
fit_rf$bestTune
100


Q8:
Use the function varImp on the output of train and save it to an object called imp.

imp <- #BLANK
imp
What should replace #BLANK in the code above?
varImp(fit)


Q9:
The rpart model we ran above produced a tree that used just seven predictors. Extracting the predictor names is not 
straightforward, but can be done. If the output of the call to train was fit_rpart, we can extract the names like
this:

tree_terms <- as.character(unique(fit_rpart$finalModel$frame$var[!(fit_rpart$finalModel$frame$var == "<leaf>")]))
tree_terms
Calculate the variable importance in the Random Forest call for these seven predictors and examine where they rank.

What is the importance of the CFHR4 gene in the Random Forest call?
imp <- varImp(fit_rf)
imp
35.03

What is the rank of the CFHR4 gene in the Random Forest call?
data_frame(term = rownames(imp$importance), 
			importance = imp$importance$Overall) %>%
	mutate(rank = rank(-importance)) %>% arrange(desc(importance)) %>%
	filter(term %in% tree_terms)
7

