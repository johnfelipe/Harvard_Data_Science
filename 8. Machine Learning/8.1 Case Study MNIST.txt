Case Study: MNIST

We have learned several machine learning algorithms
and demonstrated how to use them with illustrative examples.
But we are now going to try them out on a real example.
This is a popular data set used in machine learning competitions called the MNIST digits.
We can load the data using the following dslabs package, like this.

mnist <- read_mnist()

The data set includes two components, a training set and a test set.
You can see that by typing this.

names(mnist)

Each of these components includes a matrix with features in the columns.
You can access them using code like this.

dim(mnist$train$images)

It also includes a vector with the classes as integers.
You can see that by using this code.

class(mnist$train$labels)

table(mnist$train$labels)

Because we want this example to run on a small laptop and in less than an hour,
we'll consider a subset of the data set.
We will sample 10,000 random rows from the training set and 1,000 random rows from the test set.

set.seed(123)
index <- sample(nrow(mnist$train$images), 10000)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])

index <- sample(nrow(mnist$train$images), 1000)
x_test <- mnist$train$images[index,]
y_test <- factor(mnist$train$labels[index])


-----------------------------------------------------------------------------------------------------

Preprocessing MNIST Data

In machine learning, we often transform predictors before running the machine learning algorithm.
This is actually an important step.
We also remove predictors that are clearly not useful-- also an important step.
We call all this pre-processing.
Examples of pre-processing include standardizing the predictors, taking the log transform of some predictors
or some other transformation, removing predictors that are highly correlated with others, and removing predictors
with very few non-unique values or close to zero variation.
We're going to show an example of one of these.
The example we're going to look at relates to the variability of the features.
We can see that there are a large number of features with zero variability,
or almost zero variability.
We can use this code to compute the standard deviation of each column and then plot them in a histogram.

library(matrixStats)
sds <- colSds(x)
qplot(sds, bins = 256, color = I("black"))

Here's what it looks like.
This is expected, because there are parts of the image that rarely contain
writing, very few dark pixels, so there's very little variation and almost all the values are 0.
The Caret package includes a function that recommends features to be removed due to near zero variance.
You can run it like this.

library(caret)
nzv <- nearZeroVar(x)

We can see the columns that are removed-- they're the yellow ones in this plot-- 
by simply making an image of the matrix.

image(matrix(1:784 %in% nzv, 28, 28))

Once we remove these columns, we end up keeping these many columns.

col_index <- setdiff(1:ncol(x), nzv)
length(col_index)
#> [1] 252

Now, we're ready to fit some models.

------------------------------------------------------------------------------------------------------

Model Fitting for MNIST Data

In this video, we're going to actually implement k-nearest neighbo0rs and random forest
on the MNIST data.
However, before we start, we need to add column names to the feature matrices,
as this is a requirement of the caret package.
We can do this using this code.

colnames(x) <- 1:ncol(mnist$train$images)
colnames(x_test) <- colnames(mnist$train$images)

We're going to add as a name the number of the column.
OK, so let's start with knn.
The first step is to optimize for the number of neighbors.
Now, keep in mind that when we run the algorithm,
we will have to compute a distance between each observation in the test set and each observation in the training set.
These are a lot of calculations.
We will therefore use k-fold cross-validation to improve speed.
So we can use the caret package to optimize our k-nearest neighbor algorithm.
This will find the model that maximizes the accuracy.
Note that running this code takes quite a bit of time on a standard laptop.
It could take several minutes.

control <- trainControl(method = "cv", number = 10, p = .9)
train_knn <- train(x[,col_index], y, 
                   method = "knn", 
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)
ggplot(train_knn)


In general, it is a good idea to test out a piece of code
with a small subset of the data first to get an idea of timing,
before we start running code that might take hours to run or even days.
So always performs checks or make calculations
by hand to make sure that your code isn't going to take too much time.
You want to know-- have an idea-- of how long your code will take.
So one thing you can do is test it out on smaller datasets.
Here we define n and b as the number of rows that we're going to use
and b, the number of cross-validation folds that we're going to use.
Then we can start increasing these numbers slowly to get an idea of how long the final code will take.

n <- 1000
b <- 2
index <- sample(nrow(x), n)
control <- trainControl(method = "cv", number = b, p = .9)
train_knn <- train(x[index ,col_index], y[index,] 
                   method = "knn", 
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)

Now, once we're done optimizing our algorithm, we can fit the entire dataset.
So we would code like this.

fit_knn<- knn3(x[ ,col_index], y,  k = 5)


We see that our accuracy is almost 0.95.

y_hat_knn <- predict(fit_knn, 
                        x_test[, col_index], 
                        type="class")
cm <- confusionMatrix(y_hat_knn, factor(y_test))
cm$overall["Accuracy"]
#> Accuracy 
#>    0.948

From the specificity and sensitivity output coming from the confusion matrix
function, we see that the eights are the hardest to detect, and the most commonly incorrect predicted digit seven.

cm$byClass[,1:2]
#>          Sensitivity Specificity
#> Class: 0       1.000       0.996
#> Class: 1       0.991       0.990
#> Class: 2       0.883       0.998
#> Class: 3       0.955       0.996
#> Class: 4       0.928       0.996
#> Class: 5       0.969       0.991
#> Class: 6       0.990       0.999
#> Class: 7       0.958       0.988
#> Class: 8       0.876       1.000
#> Class: 9       0.933       0.990

Now, let's see if we can do even better with random forest.
With random forest, computation time is even a bigger challenge than with k-nearest neighbors.
For each forest, we need to build hundreds of trees.
We also have several parameters that we can tune.
Here we use the random forest implementation
in the Rborist package, which is faster than the one in the random forest package.
It has less features, but it is faster.
Because with random forest, the fitting is the slowest part of the procedure
rather than the predicting, as with knn, we will only use five-fold cross-validation.
We'll also reduce the number of trees that are fit, since we are not yet building our final model.
Finally, we'll take a random subset of observations when constructing each tree.
We can change this number with the nSamp argument in the Rborist function.
So here's a code to optimize around a random forest.


library(Rborist)
#> Loading required package: Rcpp
#> Rborist 0.1-8
#> Type RboristNews() to see new features/changes/bug fixes.
control <- trainControl(method="cv", number = 5, p = 0.8)
grid <- expand.grid(minNode = c(1) , predFixed = c(10, 15, 35))

train_rf <-  train(x[ , col_index], 
                   y, 
                   method = "Rborist", 
                   nTree = 50,
                   trControl = control,
                   tuneGrid = grid,
                   nSamp = 5000)

ggplot(train_rf)
train_rf$bestTune
#>   predFixed minNode
#> 2        15       1

It takes a few minutes to run.
We can see the final results using ggplot.
And we can choose the parameters using the best tune component of the training object.
And now we're ready to optimize our final tree.
Now we're going to set the number of trees to a larger number.
We can write this piece of code.

fit_rf <- Rborist(x[, col_index], y, 
                  nTree = 1000,
                  minNode = train_rf$bestTune$minNode,
                  predFixed = train_rf$bestTune$predFixed)
y_hat_rf <- factor(levels(y)[predict(fit_rf, x_test[ ,col_index])$yPred])
cm <- confusionMatrix(y_hat_rf, y_test)
cm$overall["Accuracy"]
#> Accuracy 
#>    0.953

Once the code is done running, and it takes a few minutes, we can see, using the confusion matrix function,
that our accuracy is above 0.95.
We have indeed improved over k-nearest neighbors.
Now, let's look at some examples of the original image in the test set in our calls.
You can see that that first one we called an eight.
It's an eight.
The second is also called an eight.
Looks like an eight.
And all of them look like we made the right call.
Not surprising-- we have an accuracy above 0.95.
Now, note that we have done minimal tuning here.
And with some further tuning, examining more parameters, growing out more trees, we can get even higher accuracy.


------------------------------------------------------------------------------------------------------------------

Variable Importance

Earlier we described that one
of the limitations of random forest is that they're not very interpretable.
However, the concept of variable importance helps a little bit in this regard.
Unfortunately, the current implementation of the Rborist package does not yet
support variable importance calculations.
So to demonstrate the concept the variable importance,
we're going to use the random forest function in the random forest package.
Furthermore, we're not going to filter any columns out of the feature matrix out.
We're going to use them all.
So the code will look like this.

library(randomForest)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])
rf <- randomForest(x, y,  ntree = 50)

Once we run this, we can compute the importance of each feature using the important function.
So we would type something like this.

imp <- importance(rf)
imp

If you look at the importance, we immediately see that the first few features have zero importance.
They're never used in the prediction algorithm.
This makes sense because these are the features
on the edges, the features that have no writings in them, no dark pixels in them.
In this particular example, it makes sense to explore the importance of this feature using an image.
We'll make an image where each feature is plotted in the location of the image where it came from.
So we can use this code.

image(matrix(imp, 28, 28))

And we see where the important features are.
It makes a lot of sense.
They're in the middle, where the writing is.
And you can kind of see the different numbers there-- six, eight, seven.
These are the features that distinguish one digit from another.
An important part of data science is visualizing results to discern why we're failing.
How we do this depends on the application.
For the examples with the digits, we'll find
digits for which we were quite certain of a call, but it was incorrect.
We can compare what we got with k-nearest neighbors to what we got with random forest.
So we can write code like this and then make images of the cases where we made a mistake.

p_max <- predict(fit_knn, x_test[col_index])
p_max <- apply(pmax, 1, max)
ind <- which(y_hat_knn != y_test)
ind <- ind[order(p_max[ind], decreasing = TRUE)]

Here they are.
That first one was called a zero.
It's actually a two.
You can kind of see why.
The second one was called a four, but it's a six.
That one you definitely see why we made a mistake, et cetera.
But by looking at these images, you might get ideas of how you could improve your algorithm.
We can do the same for random forest.
Here are the top 12 cases where we were very sure it was one digit, when it was, in fact, another.


--------------------------------------------------------------------------------------------------

Ensembles

A very powerful approach in machine learning
is the idea of ensembling different machine learning algorithms into one.
Let's explain what we mean by that.
The idea of an ensemble is similar to the idea of combining data
from different pollsters to obtain a better estimate of the true support
for different candidates.
In machine learning, one can usually greatly improve the final result of our predictions
by combining the results of different algorithms.
Here we present a very simple example, where we compute new class
probabilities by taking the average of the class probabilities provided
by random forest and k-nearest neighbors.
We can use this code to simply average these probabilities.


p_rf <- predict(fit_rf, x_test[,col_index])$census  
p_rf<- p_rf / rowSums(p_rf)
p_knn  <- predict(fit_knn, x_test[,col_index])
p <- (p_rf + p_knn)/2
y_pred <- factor(apply(p, 1, which.max)-1)
confusionMatrix(y_pred, y_test)
#> Confusion Matrix and Statistics
#> 
#>           Reference
#> Prediction   0   1   2   3   4   5   6   7   8   9
#>          0 100   0   2   0   1   0   0   0   0   0
#>          1   0 106   2   0   2   0   0   1   2   0
#>          2   0   0  93   2   0   0   0   0   1   0
#>          3   0   0   0  85   0   1   0   0   3   0
#>          4   0   0   1   0 104   0   1   0   0   1
#>          5   0   0   0   1   0  95   0   0   1   2
#>          6   0   0   0   0   0   1  95   0   1   0
#>          7   0   0   5   0   1   0   0  94   0   3
#>          8   0   0   0   0   0   0   0   0  89   0
#>          9   0   1   0   0   3   1   0   1   0  98
#> 
#> Overall Statistics
#>                                        
#>                Accuracy : 0.959        
#>                  95% CI : (0.945, 0.97)
#>     No Information Rate : 0.111        
#>     P-Value [Acc > NIR] : <2e-16       
#>                                        
#>                   Kappa : 0.954        
#>  Mcnemar's Test P-Value : NA           
#> 
#> Statistics by Class:
#> 
#>                      Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5
#> Sensitivity             1.000    0.991    0.903    0.966    0.937    0.969
#> Specificity             0.997    0.992    0.997    0.996    0.997    0.996
#> Pos Pred Value          0.971    0.938    0.969    0.955    0.972    0.960
#> Neg Pred Value          1.000    0.999    0.989    0.997    0.992    0.997
#> Prevalence              0.100    0.107    0.103    0.088    0.111    0.098
#> Detection Rate          0.100    0.106    0.093    0.085    0.104    0.095
#> Detection Prevalence    0.103    0.113    0.096    0.089    0.107    0.099
#> Balanced Accuracy       0.998    0.991    0.950    0.981    0.967    0.982
#>                      Class: 6 Class: 7 Class: 8 Class: 9
#> Sensitivity             0.990    0.979    0.918    0.942
#> Specificity             0.998    0.990    1.000    0.993
#> Pos Pred Value          0.979    0.913    1.000    0.942
#> Neg Pred Value          0.999    0.998    0.991    0.993
#> Prevalence              0.096    0.096    0.097    0.104
#> Detection Rate          0.095    0.094    0.089    0.098
#> Detection Prevalence    0.097    0.103    0.089    0.104
#> Balanced Accuracy       0.994    0.985    0.959    0.968


And we can see that once we do this, when we form the prediction,
we actually improve the accuracy over both k-nearest neighbors and random forest.
Now, notice, in this very simple example, we ensemble just two methods.
In practice, we might ensemble dozens or even hundreds of different methods.
And this really provides substantial improvements.

----------------------------------------------------------------------------------------------------

Q1:
Use the training set to build a model with several of the models available from the caret package. We will test out all of the following models in this exercise:

models <- c("glm", "lda",  "naive_bayes",  "svmLinear", 
                "gamboost",  "gamLoess", "qda", 
                "knn", "kknn", "loclda", "gam",
                "rf", "ranger",  "wsrf", "Rborist", 
                "avNNet", "mlp", "monmlp",
                "adaboost", "gbm",
                "svmRadial", "svmRadialCost", "svmRadialSigma")
We have not explained many of these, but apply them anyway using train with all the default parameters. You will likely need to install some packages. Keep in mind that you will probably get some warnings. Also, it will probably take a while to train all of the models - be patient!

Run the following code to train the various models:

library(caret)
library(dslabs)
set.seed(1)
data("mnist_27")

fits <- lapply(models, function(model){ 
	print(model)
	train(y ~ ., method = model, data = mnist_27$train)
}) 
    
names(fits) <- models
Did you train all of the models?
Yes


Q2:
Now that you have all the trained models in a list, use sapply or map to create a matrix of predictions for the test set. You should end up with a matrix with length(mnist_27$test$y) rows and length(models).

What are the dimensions of the matrix of predictions?

pred <- sapply(fits, function(object) 
	predict(object, newdata = mnist_27$test))
dim(pred)

Number of rows:
200

Number of columns:
23


Q3:
Now compute accuracy for each model on the test set. Report the mean accuracy across all models.

mlist <- sapply(fits, function(object) 
  confusionMatrix(predict(object, mnist_27$test, type = "raw"),
                  mnist_27$test$y)$overall["Accuracy"]
)

mean(mlist)
0.8058696


Q4:
Next, build an ensemble prediction by majority vote and compute the accuracy of the ensemble.

What is the accuracy of the ensemble?


votes <- rowMeans(pred == "7")
y_hat <- ifelse(votes > 0.5, "7", "2")
mean(y_hat == mnist_27$test$y)
0.84

Q5:
In Q3, we computed the accuracy of each method on the training set and noticed that the individual accuracies varied.

How many of the individual methods do better than the ensemble?

ind <- acc > mean(y_hat == mnist_27$test$y)
sum(ind)
models[ind]

3

"gamboost" "qda"      "kknn"


Q6:
It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach 
is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from 
cross validation with the training data. Obtain these estimates and save them in an object. Report the mean accuracy 
of the new estimates.

What is the mean accuracy of the new estimates?

acc_hat <- sapply(fits, function(fit) min(fit$results$Accuracy))
mean(acc_hat)


Q7:
Now let's only consider the methods with an estimated accuracy of greater than or equal to 0.8 when constructing the 
ensemble.

What is the accuracy of the ensemble now?

ind <- acc_hat >= 0.8
votes <- rowMeans(pred[,ind] == "7")
y_hat <- ifelse(votes>=0.5, 7, 2)
mean(y_hat == mnist_27$test$y)


-----------------------------------------------------------------------------------------------------------------

Q1:

We want to explore the tissue_gene_expression predictors by plotting them.

data("tissue_gene_expression")
dim(tissue_gene_expression$x)
We want to get an idea of which observations are close to each other, but, as you can see from the dimensions, the predictors are 500-dimensional, making plotting difficult. Plot the first two principal components with color representing tissue type.

Which tissue is in a cluster by itself?

  pc <- prcomp(tissue_gene_expression$x)
  
  data.frame(PC1 = pc$x[,1], PC2 = pc$x[,2],
             label=factor(tissue_gene_expression$y)) %>%
    ggplot(aes(PC1, PC2, fill=label))+
  geom_point(cex=3, pch=21)
  
  liver
  
Q2:
The predictors for each observation are measured using the same device and experimental procedure. This introduces 
biases that can affect all the predictors from one observation. For each observation, compute the average across all 
predictors, and then plot this against the first PC with color representing tissue. Report the correlation.

What is the correlation?
cor(pc$x[,1], rowMeans(tissue_gene_expression$x))
0.5969088


Q3:
We see an association with the first PC and the observation averages. Redo the PCA but only after removing the center. Part of the code is provided for you.

#BLANK
pc <- prcomp(x)
data.frame(pc_1 = pc$x[,1], pc_2 = pc$x[,2], 
			tissue = tissue_gene_expression$y) %>%
	ggplot(aes(pc_1, pc_2, color = tissue)) +
	geom_point()
Which line of code should be used to replace #BLANK in the code block above?

x <- with(tissue_gene_expression, sweep(x, 1, rowMeans(x)))


Q4:
For the first 10 PCs, make a boxplot showing the values for each tissue.

For the 7th PC, which two tissues have the greatest median difference?

for(i in 1:10){
	boxplot(pc$x[,i] ~ tissue_gene_expression$y, main = paste("PC", i))
}

liver
placenta

Q5:
Plot the percent variance explained by PC number. Hint: use the summary function.

How many PCs are required to reach a cumulative percent variance explained greater than 50%?
plot(summary(pc)$importance[3,])
3