K-fold Cross-validation

In previous videos, we've described how the goal of machine learning is often to find an algorithm that produces
predictors, y hat for an outcome y, that minimizes mean squared error.
When all we have to our disposal is one data set, we can estimate the mean squared error with the observed mean squared
error like this.
These two quantities are often referred to as a "true error" and the "apparent error" respectively.
There are two important characteristics of the apparent error we should always keep in mind.
First, it is a random variable since our data is random.
For example, the data set we have may be a random sample from a larger population.
So an algorithm having lower apparent error than another may be due to luck.
Second, if we train an algorithm on the same data
set that we used to compute the apparent error, we might be overtraining.
In general when we do this, the apparent error will be an underestimate of the true error.
We saw an extreme example of this with the k-nearest neighbors when we said k equals to 1.
Cross-validation is a technique that permits us to alleviate both these problems.
There are several approaches.
I will go over some of them here.

To understand cross-validation, it helps to think of "the true error, a theoretical quantity,
as the average of many, many apparent errors obtained
by applying the algorithm to, let's call it, B, new random samples of the data,
none of them used to train the algorithm."

When we think this way, we can think of the true error
as the average of the apparent errors obtained in each of the random samples.
The formula would be like this.
Here B a large number that can be thought of as practically infinite.
Now, this is a theoretical quantity because we only get to see one set of outcomes.
We don't get to see them over and over again.
The idea of cross-validation is to imitate this theoretical setup as best we can with the data that we have.
To do this, we have to generate a series of different random samples.
There are several approaches to doing this.
But the general idea for all of them is to randomly generate smaller data sets that are not used for training
and instead are used to estimate the true error.

The first one we describe, and the one we focus on in this video, is k-fold cross-validation.
Let's describe it.
Remember, that generally speaking, a machine learning challenge starts with a data set.
And we need to build an algorithm using this data set that will eventually be used in a completely independent data set.
So here we have the data set we have in blue and the independent data set that we'll never see in yellow.
So we don't get to see the yellow, so all we see is the blue.
So as we have already described, to imitate the situation,
we carve out a piece of our data set and pretend it is an independent data set.
We divide the data set into training set, blue, and a test set, red.
We'll train our algorithm exclusively on the training set and use the test set only for evaluation purposes.
We usually try to select a small piece of the data set so we have as much data as possible to train.
However, we also want a test set to be large so that we can obtain stable estimates of the loss.

"Typical choices to use for the size of the test that are 10% to 20% of the original data set."

Let's reiterate that it is indispensable that we do not use the test
set at all when training our algorithm, not for filtering out rows, not for selecting features, nothing.

Now, this presents a new problem.
Because for most machine learning algorithms, we need to select parameters, for example, the number of neighbors
k in the k-nearest neighbors algorithm.
Here we'll refer to the set of parameters as lambda.
So we need to optimize the algorithm parameters lambda without using our test set.
And we know that if we optimize and evaluated on the same data set, we will overtrain.
So here is where we use cross-validation.
This is where cross-validation is most useful.
So let's describe k-fold cross-validation.

For each set of algorithm parameters being considered, we want to estimate the MSE.
And then we will choose the parameters with the smallest MSE.
Cross-validation will provide this estimate.
First, it is important that before we start the cross-validation procedure we fix all the algorithm parameters.
We're computing the MSE for a given parameter.
So as we describe later, we will trained the algorithm on a set of training sets.
The parameter lambda will be the same across all these training sets.
We'll use the notation y hat i parentheses lambda to denote the prediction obtained when we use a parameter
lambda for observation i.

So if we're going to imitate the definition of the expected loss, we could write it like this.
It is the average over B samples that we've taken of the MSE that we obtain on the data separated out as a test set.
For this formula, we want to consider data sets that can be thought of as independent random samples.
And you'll want to do this several times.
With k-fold cross-validation, we do it k times.
In the cartoons we're showing, we use as an example k equals 5.
We will eventually end up with k samples.
But let's start by describing how to construct the first one.
We simply pick N divided K rounded to the nearest integer.

M=N/K

Let's call that M. So we have M observations that we pick at random and think of these as a random sample.
We could denote them using this equation.
And here b equals 1.
It's the first, what we call, fold.
So here's what it looks like graphically.
We have our training set.
We separate out the test set.
And then we take our training set, and we take a small sample of it, which we're going to call the validation
set, the first one.
And that is where we're going to test.
Now we can fit the model in the training set, with the validation set separated out, and compute the apparent error
on the independent set like this.
Note that this is just one sample and will therefore return a noisy estimate of the true error.
This is why we take k sample not just one.
So graphically it would look like this.
In k-fold cross-validation, we randomly split the observations into k non-overlapping sets.

So now we repeat this calculation for each of these sets, b going from 1 all the way up to k.
So we obtain k estimates of the MSE.
In our final estimate, we compute the average like this.
And this gives us an estimate of our loss.
A final step would be to select the lambda, the parameters, that minimize the MSE.
So this is how we use cross-validation to optimize parameters.
However, now we have to take into account the fact that the optimization occurred on the training data.
So we need to compute an estimate of our final algorithm based on data that was not used to optimize this choice.
And this is why we separated out the test set.
That is where we'll compute our final estimate of the MSE.
So note that we can do cross-validation again.
Note that this is not for optimization purpose.
This is simply to know what the MSE of our final algorithm is.
So doing this would give us a better estimate.
However, note that to do this, we have to go through the entire optimization process k times.
You will soon learn that performing machine learning tasks
can take time because we're performing many complex computations.
And therefore we're always looking for ways to reduce this.
So for the final evaluation, we often just use one test set.
We use cross-validation to optimize our algorithm.
But once we've optimized it, we're done, and we
want to have an idea of what our MSE is, we just use this one last test set.
Once we're satisfied with this model, and we want to make it available to others, we could refit the model
on the entire data set, but without changing the parameters.
Now, how do we pick the cross-validation k?
We used five in these examples.
We could use other numbers.
Large values of k are preferable because the training data better imitate the original data.
However, larger values of k will have much lower computation time.
For example, hundredfold cross-validation will be 10 times slower than tenfold cross-validation.
For this reason, the choices of k equals to 5 and 10 are quite popular.

K=5
K=10

Now, one way we can improve the variance of our final estimate is to take more samples.
To do this, we would no longer require that training set be partitioned into non-overlapping sets.
Instead we would just pick k sets of some size at random.
One popular version of this technique, at each fold, picks observations at random with replacement,
which means that the same observation can appear twice.
This approach has some advantages not discussed here is generally referred to as the "bootstrap" approach.
In fact, this is the default approach in the caret package.
In another video, we'll describe the concept of the bootstrap.


-----------------------------------------------------------------------------------------------------------

Q1:
Generate a set of random predictors and outcomes using the following code:

set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()

x_subset <- x[ ,sample(p, 100)]
Because x and y are completely independent, you should not be able to predict y using x with accuracy greater than 0.5. Confirm this by running cross-validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model.

Which code correctly performs this cross-validation?
fit <- train(x_subset, y, method = "glm")
fit$results


Q2:
Now, instead of using a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the  group to those in the  group, for each predictor, using a t-test. You can do perform this step like this:

library(devtools)
devtools::install_bioc("genefilter")
library(genefilter)
tt <- colttests(x, y)
Which of the following lines of code correctly creates a vector of the p-values called pvals?

pvals <- tt$p.value



Q3:
Create an index ind with the column numbers of the predictors that were "statistically significantly" associated with y. Use a p-value cutoff of 0.01 to define "statistically significantly."

How many predictors survive this cutoff?


pvals <- tt$p.value
pvals

ind <- which(pvals <= 0.01)

length(ind)


Q4:
Now re-run the cross-validation after redefinining x_subset to be the subset of x defined by the columns showing "statistically significant" association with y.

What is the accuracy now?


set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()
x_subset_sig <- x[ ,sample(ind, 100)]
fit_sig <- train(x_subset_sig, y, method = "glm")
fit_sig$results
0.7495955


Q5:
Re-run the cross-validation again, but this time using kNN. Try out the following grid k = seq(101, 301, 25) of tuning parameters. Make a plot of the resulting accuracies.

Which code is correct?

fit <- train(x_subset, y, method = "knn", tuneGrid = data.frame(k = seq(101, 301, 25)))
ggplot(fit)


Q6:
In the previous exercises, we see that despite the fact that x and y are completely independent, we were able to predict y with accuracy higher than 70%. We must be doing something wrong then.

What is it?
We used the entire dataset to select the columns used in the model.
Because we used the entire dataset to select the columns in the model, the accuracy is too high. The selection step needs to be included as part of the cross-validation algorithm, and then the cross-validation itself is performed after the column selection step.

As a follow-up exercise, try to re-do the cross-validation, this time including the selection step in the cross-validation algorithm. The accuracy should now be close to 50%.


Q7:

Use the train function to predict tissue from gene expression in the tissue_gene_expression dataset. Use kNN.

What value of k works best?

data("tissue_gene_expression")
fit <- with(tissue_gene_expression, train(x, y, method = "knn", tuneGrid = data.frame( k = seq(1, 7, 2))))
ggplot(fit)
fit$results


----------------------------------------------------------------------------------------------------------------


Bootstrap


In this video we describe the bootstrap.
We're going to use a very simple example to do it.
Suppose the income distribution of a population is as follows.

hist(log10(income))

m <- median(income)
m
#> [1] 45384

The population median is, in this case, about 45,000.
Suppose we don't have access to the entire population,
but want to estimate the median, let's call it M. We take a sample of 250
and estimate the population median, M, with the sample medium big M, like this.

set.seed(1)
N <- 250
X <- sample(income, N)
M <- median(X)
M
#> [1] 48503

Now, can we construct a confidence interval?
What's the distribution of the sample median?
From an Monte Carlo simulation, we see that the distribution of the sample
median is approximately normal with the following expected value and standard errors.
You can see it here.


B <- 10^5
Ms <- replicate(B, {
  X <- sample(income, N)
  M <- median(X)
})
par(mfrow=c(1,2))
hist(Ms)
qqnorm(Ms)
qqline(Ms)
mean(Ms)
#> [1] 45517
sd(Ms)
#> [1] 3670

The problem here is that, as we have described before, in practice, we do not have access to the distribution.
"In the past, we've used the central limit theorem, but the central limit theorem we studied applies to averages
and here we're interested in the median."

The bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution.
The general idea is relatively simple.
We act as if the sample is the entire population and sample "with replacement" data sets of the same size.
Then we compute the summary statistic, in this case, the median, on what is called the bootstrap sample.
There is theory telling us that the distribution of the statistic obtained
with bootstrap samples approximate the distribution of our actual statistic.
This is how we construct bootstrap samples in an approximate distribution.
This simple code.

B <- 10^5
M_stars <- replicate(B, {
  X_star <- sample(X, N, replace = TRUE)
  M_star <- median(X_star)
})

Now we can check how close it is to the actual distribution.
We can see it's relatively close.

qqplot(Ms, M_stars)
abline(0,1) 


We see it's not perfect, but it provides a decent approximation.
In particular, look at the quantities we need to form a 95% confidence interval.
They are quite close.

quantile(Ms, c(0.05, 0.95))
#>    5%   95% 
#> 39727 51778
quantile(M_stars, c(0.05, 0.95))
#>    5%   95% 
#> 40471 53080


This is much better than what we get if we mindlessly use the central limit
theorem, which would give us this confidence interval, which is entirely wrong.

median(X) + 1.96 * sd(X)/sqrt(N) * c(-1,1)
#> [1] 36801 60205


If we know the distribution is normal, we can use a bootstrap to estimate the mean, the standard error,
and then form a confidence interval that way.

mean(Ms) + 1.96*sd(Ms)*c(-1,1)
#> [1] 38325 52710
mean(M_stars) + 1.96*sd(M_stars)*c(-1,1)
#> [1] 40170 55350


---------------------------------------------------------------------------------------------------------

Q1:


The createResample function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the mnist_27 dataset like this:

set.seed(1995)
indexes <- createResample(mnist_27$train$y, 10)
How many times do 3, 4, and 7 appear in the first resampled index?

Enter the number of times 3 appears: 1
Enter the number of times 4 appears: 4
Enter the number of times 7 appears: 0


Q2:
We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the resampled indexes.

What is the total number of times that 3 appears in all of the resampled indexes?

data("mnist_27")

set.seed(1995)
indexes <- createResample(mnist_27$train$y, 10)
f <- function(x) {x == 3}
f2 <- function(x) {
  sum(sapply(indexes[x], f) == TRUE)
}
c <- seq(1,length(indexes))
sum(sapply(c, f2))


Q3:

Generate a random dataset using the following code:

set.seed(1)
y <- rnorm(100, 0, 1)
Estimate the 75th quantile, which we know is qnorm(0.75), with the sample quantile: quantile(y, 0.75).

Run a Monte Carlo simulation with 10,000 repetitions to learn the expected value and standard error of this random variable. Set the seed to 1.

B <- 10000
set.seed(1)
Ms <- replicate(B, {
  y <- rnorm(100, 0, 1)
  quantile(y, 0.75)
})
mean(Ms)
0.6656107
sd(Ms)
0.1353809



Q4:

In practice, we can't run a Monte Carlo simulation. Use 10 bootstrap samples to estimate the standard error using just the initial sample y. Set the seed to 1.


set.seed(1)
indexes <- createResample(y, 10)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
0.7312648
sd(q_75_star)
0.07419278


Q5:

Repeat the exercise from Q4 but with 10,000 bootstrap samples instead of 10. Set the seed to 1.

set.seed(1)
indexes <- createResample(y, 10000)
q_75_star <- sapply(indexes, function(ind){
  y_star <- y[ind]
  quantile(y_star, 0.75)
})
mean(q_75_star)
0.6737512
sd(q_75_star)
0.0930575


Q6:

Compare the SD values obtained using 10 vs 10,000 bootstrap samples.

What do you observe?

The SD is roughly the same in both cases. 

