Least Squares Estimates (LSE)

For linear models to be useful, we have to "estimate the unknown parameters, the betas."
The standard approach in science is to find the values that minimize the distance of the fitted model to the data.
To quantify, this we use the least squares equation.

For [INAUDIBLE] data, we would write something like this.

RSS = nEi=1 {Yi - (B0 + B1xi)}^2

This quantity is called the Residual Sum of Squares, RSS.

Once we find the values that minimize the RSS, we call the values the Least Squares Estimate, LSE,
and denote them, in this case, with beta 0 hat and beta 1 hat.

B^0 B^1

Let's write the function that computes the RSS for any pair of values, beta 0 and beta 1, for our heights data.
It would look like this.

> rss <- function(beta0, beta1, data){
+	resid <- galton_heights$son - (beta0 + beta1 * galton_heights$father)
+	return(sum(resid^2))
+ }


So for any pair of values, we get an RSS.
So this is a three-dimensional plot with beta 1 and beta 2, and x and y and the RSS as a z.
To find the minimum, you would have to look at this three-dimensional plot.
Here, we're just going to make a two-dimensional version by keeping beta 0 fixed at 25.
So it will be a function of the RSS as a function of beta 1.
We can use this code to produce this plot.

> beta1 = seq(0, 1, len=nrow(galton_heights))
> results <- data.frame(beta1 = beta1,
			rss = sapply(beta1, rss, beta0 = 25))
> results %>% ggplot(aes(beta1, rss)) + geom_line() +
+	geom_line(aes(beta1, rss), col=2)


We can see a clear minimum for beta 1 at around 0.65.
So you could see how we would pick the least squares estimates.
However, this minimum is for beta 1 when beta 0 is fixed at 25.
But we don't know if that's the minimum for beta 0.
We don't know if 25 karma 0.65 minimizes the equation across all pairs.
We could use trial and error, but it's really not going to work here.

Instead we will use calculus.
We'll take the partial derivatives, set them equal to 0,
and solve for beta 1 and beta 0.
Of course, if we have many parameters, these equations can get rather complex.
But there are functions in R that do these calculations for us.
We will learn these soon.
To learn the mathematics behind this, you can consult the book on linear models.


Q1:

The following code was used in the video to plot RSS with .

beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss), col=2)
In a model for sons’ heights vs fathers’ heights, what is the least squares estimate (LSE) for  if we assume  is 36?

0.5


Q2:

The least squares estimates for the parameters B0, B1,..., Bn   "minimize"   the residual sum of squares.


-------------------------------------------------------------------------------------

The lm Function

In r, we can obtain the least squares estimates using the lm function.

lm()

To fit the following model where Yi is the son's height
and Xi is the father height, we would write the following piece of code.

Yi = B0 + B1xi + ei

> fit <- lm(son ~ father, data = galton_heights)
> fit

Coefficients:
(Intercept)	father
35.712		0.503


This gives us the least squares estimates, which we can see in the output of r.
The general way we use lm is by using the tilde character to let lm
know which is the value we're predicting that's on the left side of the tilde,
and which variables we're using to predict-- those will be on the right side of the tilde.
The intercept is added automatically to the model.
So you don't have to include it when you write it.
The object fit that we just computed includes more information about the least squares fit.

We can use the function summary to extract more of this information, like this.

> summary(fit)


To understand some of the information included in this summary, we need to remember that the LSE are random variables.
Mathematical statistics gives us some ideas of the distribution of these random variables.
And we'll learn some of that next.


Q1:

Run a linear model in R predicting the number of runs per game based on the number of bases on balls and the number 
of home runs. Remember to first limit your data to 1961-2001.

What is the coefficient for bases on balls?

dat2 <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(BB_per_game = BB/G,
         HR_per_game = HR/G,
         R_per_game = R/G)

lm(R_per_game ~ BB_per_game + HR_per_game, data = dat2)

0.39

--------------------------------------------------------------------------------------------------------------


LSE are Random Variables

The LSE are derived from the data, Y1 through Yn, which are random.

Y1, Y2,...Yn

This implies that our estimates are random variables.
To see this, we can run a Monte Carlo simulation in which we assume that the son and father height data that we
have defines an entire population.
And we're going to take random samples of size 50 and compute the regression slope coefficient for each one.
We write this code, which gives us several estimates of the regression slope.

> B <- 1000
> N <- 50
> lse <- replicate(B, {
+	sample_n(galton_heights, N, replace = TRUE) %>%
+		lm(son ~ father, data = .) %>% .$coef
+ })
> lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,])

We can see the variability of the estimates by plotting their distribution.

> library(gridExtra)
> p1 <- lse %>% ggplot(aes(beta_0)) + geom_histogram(binwidth = 5, color = "black")
> p2 <- lse %>% ggplot(aes(beta_1)) + geom_histogram(binwidth = 0.1, color = "black")
> grid.arrange(p1, p2, ncol=2)


Here you can see the histograms of the estimated beta 0's and the estimated beta 1's.
The reason these look normal is because the central limit theorem applies here as well.

For large enough N, the least squares estimates will be approximately normal with expected value beta 0 and beta 1
respectively.

The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them,
and they are included in the summary provided by the lm function.
Here are the estimated standard errors for one of our simulated data sets.

> sample_n(galton_heights, N, replace = TRUE) %>%
+	lm(son ~ father, data = .) %>% summary

You could see them at the second column in the coefficients table.
You can see that the standard errors estimates reported by the summary
function are closed, so the standard errors that we obtain from our Monte Carlo simulation.

> lse %>% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))
	se_0	se_1
1	9.07	0.131

The summary function also reports t-statistics-- this is the t value column-- and p-values.

This is the Pr bigger than absolute value of t column.

The t-statistic is not actually based on the central limit theorem,
but rather on the assumption that the epsilons follow a normal distribution.
Under this assumption, mathematical theory tells us that the LSE divided by their standard error, which
we can see here and here, follow a t distribution with N
minus p degrees of freedom, with p the number of parameters in our model, which in this case is 2.

B^0 / SE^(B^0)
B^1 / SE^(B^1)

t-distribution

N-p degrees of freedom

The 2p values are testing the null hypothesis that beta 0 is 0 and beta 1 is 0 respectively.

Note that as we described previously, for large enough N,
the central limit works, and the t distribution becomes almost the same as a normal distribution.

So if either you assume the errors are normal and use the t distribution
or if you assume that N is large enough to use the central limit theorem,
you can construct confidence intervals for your parameters.

We know here that although we will not show examples in this video,
hypothesis testing for regression models is very commonly used in, for example,
epidemiology and economics, to make statements such as the effect of A
and B was statistically significant after adjusting for X, Y, and Z.
But it's very important to note that several assumptions-- we just described some of them--
have to hold for these statements to hold.

Q1:

We run a Monte Carlo simulation where we repeatedly take samples of N = 100 from the Galton heights data and compute 
the regression slope coefficients for each sample:

B <- 1000
N <- 100
lse <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>% 
    lm(son ~ father, data = .) %>% .$coef 
})

lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) 

What does the central limit theorem tell us about the variables beta_0 and beta_1?

1. They are approximately normally distributed.
2. The expected value of each is the true value of  and (assuming the Galton heights data is a complete population).


Q2:

In an earlier video, we ran the following linear model and looked at a summary of the results.

> mod <- lm(son ~ father, data = galton_heights)
> summary(mod)

Call:
lm(formula = son ~ father, data = galton_heights)

Residuals:
   Min     1Q  Median     3Q    Max 
-5.902  -1.405  0.092    1.342  8.092 

Coefficients:
                 Estimate  Std. Error  t value     Pr(>|t|)  
(Intercept)     35.7125     4.5174       7.91    2.8e-13 ***
father           0.5028     0.0653       7.70    9.5e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

What null hypothesis is the second p-value (the one in the father row) testing?

B1 = 0, where B1 is the coefficient for the variable "father".

------------------------------------------------------------------------------------------------------

Advanced Note on LSE


Although interpretation is not straight-forward, it is also useful to know that the LSE can be strongly correlated, which can be seen using this code:

lse %>% summarize(cor(beta_0, beta_1))

However, the correlation depends on how the predictors are defined or transformed.

Here we standardize the father heights, which changes xi to xi-est(x).

B <- 1000
N <- 50
lse <- replicate(B, {
      sample_n(galton_heights, N, replace = TRUE) %>%
      mutate(father = father - mean(father)) %>%
      lm(son ~ father, data = .) %>% .$coef 
})

Observe what happens to the correlation in this case:

cor(lse[1,], lse[2,]) 

-----------------------------------------------------------------------------------------------------------

Predicted Variables are Random Variables

Once we fit our model, we can obtain predictions of y by plugging the estimates into the regression model.
For example, if the father's height is x, then our prediction for y--
which we'll denote with a hat on top of the y-- for the son's height will be the following.

Y^ = B^0 + B^1x

We're just plugging in beta-- the estimated betas into the equation.

If we plot y hat versus x, we'll see the regression line.
Note that the prediction y hat is also a random variable, 
and mathematical theory tells us what the standard errors are.

If we assume the errors are normal or have a large enough sample
size to use the Central Limit Theorem, we can construct confidence intervals for our predictions, as well.
In fact, the ggplot layer geom underscore smooth, when we set method equals to lm--
we've previously shown this for several plots-- plots confidence intervals around the predicted y hat.

ggplot2 layer:
geom_smooth(method = "lm")

Let's look at an example with this code.

> galton_heights %>% ggplot(aes(son, father)) +
+	geom_point() +
+	geom_smooth(method = "lm")

You can see the regression line.
Those are the predictions, and you see a band around them.
Those are the confidence intervals.
The R function predict takes an lm object as input and returns these predictions.
We can see it here in this code which produces this plot,


> galton_heights %>%
+	mutate(Y_hat = predict(lm(son ~ father, data = .))) %>%
+	ggplot(aes(father, Y_hat)) +
+	geom_line()


and if requested the standard errors and other information from which we
can construct confidence intervals can be obtained from the predict function.
You can see it by running this code.

> fit <- galton_heights %>% lm(son ~father, data = .)
> Y_hat <- predict(fit, se.fit = TRUE)
> names(Y_hat)

Q1:
Which R code(s) below would properly plot the predictions and confidence intervals for our linear model of sons’ heights?
Select ALL that apply.

2.
galton_heights %>% ggplot(aes(father, son)) +
  geom_point() +
  geom_smooth(method = "lm")

3.
model <- lm(son ~ father, data = galton_heights)
predictions <- predict(model, interval = c("confidence"), level = 0.95)
data <- as.tibble(predictions) %>% bind_cols(father = galton_heights$father)

ggplot(data, aes(x = father, y = fit)) +
  geom_line(color = "blue", size = 1) + 
  geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.2) + 
  geom_point(data = galton_heights, aes(x = father, y = son))