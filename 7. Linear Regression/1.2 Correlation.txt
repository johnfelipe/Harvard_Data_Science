Correlation

Up to now in this series, we have focused mainly on univariate variables.
However, in data science application it is very common 
to be interested in the relationship between two or more variables.

We saw this in our baseball example in which 
we were interested in the relationship, for example, between bases on balls and runs.

we'll come back to this example, but we introduce the concepts of correlation and regression using a simpler example.
It is actually the dataset from which regression was born.
We examine an example from genetics.

Francis Galton studied the variation and heredity of human traits.
Among many other traits, Galton collected and studied height data from families to try to understand heredity.
While doing this, he developed the concepts of correlation and regression,
and a connection to pairs of data that follow a normal distribution.
Note that, at the time this data was collected, what we know today about genetics was not yet understood.

A very specific question Galton tried to answer was, how much of a son's height can I predict with the parents height.
Note that this is similar to predicting runs with bases on balls.
We have access to Galton's family data through the HistData package.
HistData stands for historical data.
We'll create a data set with the heights of fathers and the first sons.

> library(Histdata)
> data("GaltonFamilies")
> galton_heights <- GaltonFamilies %>%
+	filter(childNum ==1 * gender == "male") %>%
+	select(father, childHeight) %>%
+	rename(son = childHeight)

The actual data Galton used to discover and define regression.
So we have the father and son height data.
Suppose we were to summarize these data.
Since both distributions are well approximated by normal distributions,
we can use the two averages and two standard deviations as summaries.
Here they are.

> galton_heights %>%
+	summarize(mean(father), sd(father), mean(son), sd(son))
	mean(father)	sd(father)	mean(son)	sd(son)
1	69.1		2.55		70.5		2.56

You can see the average heights for fathers is 69 inches.
The standard deviation is 2.54.
For sons, they're a little taller, because it's the next generation.
The average height is 70.45 inches, and the standard deviation is 2.55 inches.
However, this summary fails to describe a very important characteristic of the data that you can see in this figure.

> galton_heights %>% ggplot(aes(father, son)) +
+	geom_point(alpha = 0.5)

The trend that the taller the father, the taller the son, is not described by the summary statistics of the average
and the standard deviation.
We will learn that the correlation coefficient is a summary of this trend.

---------------------------------------------------------------------------------------------------------------------


Correlation Coefficient

The correlation coefficient is defined for a list of pairs-- x1, y1 through xn, yn-- 
(X1, Y1),...., (Xn,Yn)
with the following formula:


p = 1/n nEi=1 ( (xi - mux)/sigmax ) ( (yi - muy)/sigmay )


Here, mu x and mu y are the averages of x and y, respectively.
And sigma x and sigma y are the standard deviations.
The Greek letter rho (p) is commonly used in the statistics book to denote this correlation.
The reason is that rho is the Greek letter for r, the first letter of the word regression.
Soon, we will learn about the connection between correlation and regression.

To understand why this equation does, in fact, summarize how two variables move together,
consider the i-th entry of x is xi minus mu x divided by sigma x SDs away from the average.

i-th entry of x is ( (xi - mux)/sigmax) SDs away from the average

Similarly, the yi-- which is paired with the xi-- is yi minus mu y divided by sigma y SDs away from the average y.

yi is ( (yi - muy) / sigmay ) SDs away from the average y

If x and y are unrelated, then the product of these two quantities will be positive.
That happens when they are both positive or when they are both negative as often as they will be negative.
That happens when one is positive and the other is negative, or the other way around.
One is negative and the other one is positive.
This will average to about 0.

product of 
( (xi - mux)/sigmax)  ( (yi - muy) / sigmay ) 
will be positive as often as negative and will average to about 0


The correlation is this average.

And therefore, unrelated variables will have a correlation of about 0.

If instead the quantities vary together, then we are averaging mostly positive products.
Because they're going to be either positive times positive or negative times negative.
And we get a positive correlation.

If they vary in opposite directions, we get a negative correlation.
Another thing to know is that we can show mathematically that the correlation is always between negative 1 and 1.

To see this, consider that we can have higher correlation than when we compare a list to itself.
That would be perfect correlation.
In this case, the correlation is given by this equation,
which we can show is equal to 1.

p = 1/n nEi=1 ( (xi - mux) / sigmax)^2
  = 1/sigma^2 1/n nEi=1 (xi - mux)^2
  = 1


A similar argument with x and its exact opposite, negative x, 
proves that the correlation has to be greater or equal to negative 1.
So it's between minus 1 and 1.

The correlation between father and sons' height is about 0.5.
You can compute that using this code.

> library(HistData)
> data("GaltonFamilies")
> galton_heights <- GaltonFamilies %>%
+	filter(childNum == 1 & gender == "male") %>%
+	select(father, childHeight) %>%
+	rename(son = childHeight)

> galton_heights %>% summarize(cor(father,son))
	cor(father,son)
1	0.501

We saw what the data looks like when the correlation is 0.5.
To see what data looks like for other values of rho, 
here are six examples of pairs with correlations ranging from negative 0.9 to 0.99.
When the correlation is negative, we see that they go in opposite direction.
As x increases, y decreases.
When the correlation gets either closer to 1 or negative 1, we see the clot of points getting thinner and thinner.
When the correlation is 0, we just see a big circle of points.

-------------------------------------------------------------------------------------------------------

Sample Correlation is a Random Variable

Before we continue describing regression,
let's go over a reminder about random variability.
In most data science applications, we do not observe the population, but rathera sample.
As with the average and standard deviation,
the sample correlation is the most commonly used estimate of the population correlation.
This implies that the correlation we compute and use as a summary is a random variable.

As an illustration, let's assume that the 179 pairs of fathers and sons is our entire population.
A less fortunate geneticist can only afford to take a random sample of 25 pairs.
The sample correlation for this random sample can be computed using this code.

> set.seed(0)
> R <- sample_n(galton_heights, 25, replace = TRUE) %>%
+	summarize(cor(father, son))


Here, the variable R is the random variable.

We can run a monte-carlo simulation to see the distribution of this random variable.

> B <- 1000
> N <- 25
> R <- replicate(B, {
+	sample_n(galton_heights, 25, replace = TRUE) %>%
+	summarize(r=cor(father, son)) %>% .$r
+ })
> data.frame(R) %>% ggplot(aes(R)) + geom_histogram(binwidth = 0.05, color = "black")

Here, we recreate R 1000 times, and plot its histogram.
We see that the expected value is the population correlation,
the mean of these Rs is 0.5, and that it has a relatively high standard error relative to its size, SD 0.147.

> mean(R)
[1] 0.501

> sd(R)
[1] 0.147

This is something to keep in mind when interpreting correlations.
It is a random variable, and it can have a pretty large standard error.
Also note that because the sample correlation is
an average of independent draws, the Central Limit Theorem actually applies.

Therefore, for a large enough sample size N, the distribution of these Rs is approximately normal.
The expected value we know is the population correlation.
The standard deviation is somewhat more complex to derive, but this is the actual formula here.

R ~ N(p, sqrt( (1-r^2)/(N-2) ))

In our example, N equals to 25, does not appear
to be large enough to make the approximation
a good one, as we see in this [INAUDIBLE]..


Instead of running a Monte Carlo simulation with a sample size of 25 from our 179 father-son pairs, we now run our simulation with a sample size of 50.
Would you expect the mean of our sample correlation to increase, decrease, or stay approximately the same?
Stay approximately the same correct

Instead of running a Monte Carlo simulation with a sample size of 25 from our 179 father-son pairs, we now run our simulation with a sample size of 50.
Would you expect the standard deviation of our sample correlation to increase, decrease, or stay approximately the same?
Decrease 





