
The Central Limit Theorem in Practice

The central limit theorem tells us that the distribution function for a sum of draws is approximately normal.

We also learned that when dividing a normally distributed random variable by a nonrandom constant,
the resulting random variable is also normally distributed.
(distribution of X¯ is approximately normal)

expected value of X¯ = p

standard error of X¯ = sqrt(p*(1-p)/N)

Suppose we want to know what is the probability that we are within one percentage point from p, that we
made a very, very good estimate?
We are asking Pr(|X¯ - p| <= 0.1)
This is the same as Pr(X¯ <= p + 0.01) - Pr(X¯ <= p - 0.01)


We can us the trick: We subtract the expected value and divide by the standard error on both sides of the equation.

Pr( (X¯ - E(X¯))/ SE(X¯) <=  ((p+0.01) - E(X¯))/ SE(X¯) )  -  Pr( (X¯ - E(X¯))/ SE(X¯) <=  ((p-0.01) - E(X¯))/ SE(X¯) )

What this does is it gives us a standard normal variable, which we have been calling capital Z, on the left side.

Pr( Z <=  ((p+0.01) - E(X¯))/ SE(X¯) )  -  Pr( Z <=  ((p-0.01) - E(X¯))/ SE(X¯) )

Since p is the expected value, and the standard error of X-bar is the square root of p times 1 minus p divided by N,
we get that the probability that we were just calculating is equivalent to probability of Z, our standard normal variable,
being less than 0.01 divided by the standard error of X-bar minus the probability of Z being less than negative 0.01
divided by that standard error of X-bar.

E(X¯) = p

SE(X¯) = sqrt( p*(1-p)/N)

So that means:

Pr( Z <= 0.01/sqrt(p*(1-p)/N) )  -  Pr( Z <= -0.01/sqrt(p*(1-p)/N) )

But we don't know p, so still can't compute the standard error of X-bar.

But it turns out-- and this is something new we're showing you--
that the CLT still works if we use an estimate of the standard error that,
instead of p, uses X-bar in its place. We call this a plug-in estimate.

S^E(X¯) = sqrt( X¯*(1-X¯)/N)

In the mathematical formula we're showing you, you can see a hat on top of the SE.
In statistics textbooks, we use a little hat like this to denote estimates.
This is an estimate of the standard error, not the actual standard error.

But now instead of dividing by the standard error,
we're going to divide by this estimate of the standard error.

Let's compute this estimate of the standard error for the first sample
that we took, in which we had 12 blue beads and 13 red beads.
In that case, X-bar was 0.48.

For the standard error, we write this:
> X_hat <- 0.48
> se <- sqrt(X_hat*(1-X_hat)/25)
> se
[1] 0.09991997

now we can compute the probability of being as close to p as we wanted.

The answer is simply pnorm of 0.01
>pnorm(0.01/se) - pnorm(-0.01/se)
[1] 0.07971926

so the probability of this happening is about 8%.

what we're going to be able to do with the central limit theorem
is determine what sample sizes are better.

--------------------------------------------------------------------------------------------


Margin of Error


Now we can define it because it is simply 2 times the standard error, which we can now estimate.

> 2*se
[1] 0.1998399

which is about 0.2

This is because if you ask what is the probability that we're within 2 standard errors from p, using the 
same previous equations, we end up with an equation like this one.
Pr(|X¯ - p| <=  2SE(X¯))
Pr(X¯ <= p + 2SE(X¯)) - Pr(X¯ <= p - 2SE(X¯))
Pr( (X¯ - E(X¯))/SE(X¯)  <= ((p + 2SE(X¯)) - E(X¯))/SE(X¯))  - Pr( (X¯ - E(X¯))/SE(X¯)  <= ((p - 2SE(X¯)) - E(X¯))/SE(X¯)) 
Pr(Z <= 2SE(X¯)/SE(X¯)) - Pr(Z <= -2SE(X¯)/ SE(X¯))
Pr(Z <= 2) - Pr(Z <= -2)


This one simplifies out, and we're simply asking what is the probability of the standard normal distribution that
has the expected value 0 and standard error one is within two values from 0,
and we know that this is about 95%.

> pnorm(2) - pnorm(-2)
[1] 0.95

so it's 95%


So there's a 95% chance that X-bar will be within 2 standard errors. That's the margin of error, in our case, to p.
Now why do we use 95%?

But traditionally, that's what's been used. It's the most common value that's used to define margins of errors.

In summary, the central limit theorem tells us
that our poll based on a sample of just 25 is not very useful.
We don't really learn much when the margin of error is this large.
All we can really say is that the popular vote will not
be won by a large margin.

Note that if we had obtained  X¯ = 0.48, but with a sample size of N = 2000, the estimated standard error would have been
about 0.01.   S^E(X¯) ~ 0.01

So our result in an estimate of 48% blue beads with a margin of error 2%.

In this case, the result is much more informative
and would make us think that there are more red beads than blue beads.


--------------------------------------------------------------------------------------------------------------

A Monte Carlo Simulation for the CLT

To create the simulation:
> B <- 10000
> N <- 1000
> X_hat <- replicate(B, {
+	X <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
+	mean(X)
+	})

But we cna't run this because we don't know p. However, we could construct an urn like the one we showed in a 
previous video and actually run an analog simulation.

take_poll(n=1000)

But it takes time to count the beads and enter the results into R.

So one thing we can do to corroborate theoretical results
is to pick a value of p or several values of p
and then run simulations using those.

As an example, let's set p to 0.45. We can simulate one poll of 1,000 beads or people using this simple code.

> p <- 0.45
> N <- 1000
> X <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
> X_hat <- mean(X)

Now we can take that into a Monte Carlo simulation.
Do it 10,000 times, each time returning the proportion of blue beads
that we get in our sample.

> B <- 10000
> N <- 1000
> X_hat <- replicate(B, {
+	X <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
+	mean(X)
+	})

To review, the theory tells us that X-bar has approximately normal distribution with expected value 0.45
and a standard error of about 1.5%. The simulation confirms this.

>mean(X_hat)
[1] 0.4499507
>sd(X_hat)
[1] 0.015812

history and qq plot of this X-hat data confirms that the normal aproximation is accurate as well.

>libary(gridExtra)
> p1 <- data.frame(X_hat=X_hat) %>% ggplot(aes(X_hat)) +
+	geom_histogram(binwidth = 0.005, color="black")
> p2 <- data.frame(X-hat=X_hat) %>% ggplot(aes(sample=X_hat)) +
+	stat_qq(dparams = list(mean=mean(X_hat), sd=sd(X_hat))) +
+	geom_abline() +
+	ylab("X_hat") +
+	xlab("Theoretical normal")
> grid.arrange(p1,p2,nrow=1)

-------------------------------------------------------------------------------------------------------

The Spread

The competition is to predict the spread, not the proportion p.
However, because we are assuming there are only two parties,
we know that the spread is just:
p-(1-p)
which is equal to:
2p-1

Now we can plug in X-bar where you should have a p:
2X¯ - 1

And, since we're multiplying a random variable by 2, we know that the standard error goes up by 2:
2S^E(X¯)

Note that subtracting the 1 does not add any variability, so it does not affect the standard error.

So, for our first example, with just the 25 beads, our estimate of p was 0.48 with a margin of error of 0.2.
This means that our estimate of the spread is 4 percentage points, 0.04, with a margin of error of 40%, 0.4.

But the point is that once we have an estimate and standard error for p,
we have it for the spread 2p minus 1.

--------------------------------------------------------------------------------------------------------

Bias: Why Not Run a Very Large Poll?


Note that for realistic values of p, say between 0.35 and 0.65
for the popular vote, if we run a very large poll with say 100,000 people,
theory would tell us that we would predict the election almost perfectly,
since the largest possible margin of error is about 0.3%.

> N <- 100000
> p <- seq(0.35, 0.65, length=100)
> SE <- sapply(p, function(x) 2*sqrt(x*(1-x)/N))
> data.frame(p=p, SE=SE) %>% ggplot(aes(p, SE)) +
+	geom_line()

So why not run a large poll?
it's expensive and, a more important reason, the theory has its limitations.


For example, while the beads are either red or blue,
and you can see it with your eyes, people, when you ask them,
might lie to you.
Also, because you're conducting these polls usually by phone,
you might miss people that don't have phones.
And they might vote differently than those that do.

So, even if our margin of error is very small,
it may not be exactly right that our expected value is p.
We call this bias.
The typical bias appears to be between 1% and 2%.

----------------------------------------------------------------------------------------------------------

Assessment

1.
Write function called take_sample that takes the proportion of Democrats p and the sample size N as arguments and returns the sample average of Democrats (1) and Republicans (0).
Calculate the sample average if the proportion of Democrats equals 0.45 and the sample size is 100.
Define a function called take_sample that takes p and N as arguments.
Use the sample function as the first statement in your function to sample N elements from a vector of options where Democrats are assigned the value '1' and Republicans are assigned the value '0'.
Use the mean function as the second statement in your function to find the average value of the random sample.
# Write a function called `take_sample` that takes `p` and `N` as arguements and returns the average value of a randomly sampled population.
take_sample <- function(p, N){
  x <- sample(1:0, N, replace=TRUE, prob=c(p, 1-p))
  mean(x)
}

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Call the `take_sample` function to determine the sample average of `N` randomly selected people from a population containing a proportion of Democrats equal to `p`. Print this value to the console.
take_sample(p, N)



2.
Assume the proportion of Democrats in the population p equals 0.45 and that your sample size N is 100 polled voters. The take_sample function you defined previously generates our estimate, X¯.
Replicate the random sampling 10,000 times and calculate p-X¯ for each random sample. Save these differences as a vector called errors. Find the average of errors and plot a histogram of the distribution.
The function take_sample that you defined in the previous exercise has already been run for you.
Use the replicate function to replicate subtracting the result of take_sample from the value of p 10,000 times.
Use the mean function to calculate the average of the differences between the sample average and actual value of p.
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Create an objected called `errors` that replicates subtracting the result of the `take_sample` function from `p` for `B` replications
errors <- replicate(B, {
  x <- p- take_sample(p, N)
})

# Calculate the mean of the errors. Print this value to the console.
mean(errors)


3.
In the last exercise, you made a vector of differences between the actual value for p and an estimate, X¯. We called these differences between the actual and estimated values errors.
The errors object has already been loaded for you. Use the hist function to plot a histogram of the values contained in the vector errors. Which statement best describes the distribution of the errors?
hist(errors)
The errors are symmetrically distributed around 0.

4.
The error p-X¯ is a random variable. In practice, the error is not observed because we do not know the actual proportion of Democratic voters, p. However, we can describe the size of the error by constructing a simulation.
What is the average size of the error if we define the size by taking the absolute value |p-X¯| ?
Use the sample code to generate errors, a vector of |p-X¯|.
Calculate the absolute value of errors using the abs function.
Calculate the average of these values using the mean function.
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the mean of the absolute value of each simulated error. Print this value to the console.
mean(abs(errors))



5.
The standard error is related to the typical size of the error we make when predicting. We say size because, as we just saw, the errors are centered around 0. In that sense, the typical error is 0. For mathematical reasons related to the central limit theorem, we actually use the standard deviation of errors rather than the average of the absolute values.
As we have discussed, the standard error is the square root of the average squared distance (X¯-p)2. The standard deviation is defined as the square root of the distance squared.
Calculate the standard deviation of the spread.
Use the sample code to generate errors, a vector of |p-X¯|.
Use ^2 to square the distances.
Calculate the average squared distance using the mean function.
Calculate the square root of these values using the sqrt function.
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the standard deviation of `errors`
sqrt(mean(errors^2))



6.
The theory we just learned tells us what this standard deviation is going to be because it is the standard error of X¯.
Estimate the standard error given an expected value of 0.45 and a sample size of 100.
Calculate the standard error using the sqrt function
# Define `p` as the expected value equal to 0.45
p <- 0.45

# Define `N` as the sample size
N <- 100

# Calculate the standard error

sqrt(p*(1-p)/N)


7.
In practice, we don't know p, so we construct an estimate of the theoretical prediction based by plugging in X¯ for p. Calculate the standard error of the estimate:
SE^(X¯)
Simulate a poll X using the sample function.
When using the sample function, create a vector using c() that contains all possible polling options where '1' indicates a Democratic voter and '0' indicates a Republican voter.
When using the sample function, use replace = TRUE within the sample function to indicate that sampling from the vector should occur with replacement.
When using the sample function, use prob = within the sample function to indicate the probabilities of selecting either element (0 or 1) within the vector of possibilities.
Use the mean function to calculate the average of the simulated poll, X_bar.
Calculate the standard error of the X_bar using the sqrt function and print the result.
# Define `p` as a proportion of Democratic voters to simulate
p <- 0.45

# Define `N` as the sample size
N <- 100

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `X` as a random sample of `N` voters with a probability of picking a Democrat ('1') equal to `p`
X <- sample(1:0, N, replace=TRUE, prob=c(p, 1-p))


# Define `X_bar` as the average sampled proportion
X_bar <- mean(X)

# Calculate the standard error of the estimate. Print the result to the console.
sqrt(X_bar * (1-X_bar)/N)


8.
The standard error estimates obtained from the Monte Carlo simulation, the theoretical prediction, and the estimate of the theoretical prediction are all very close, which tells us that the theory is working. This gives us a practical approach to knowing the typical error we will make if we predict p with X^. The theoretical result gives us an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for p=0.5.
Create a plot of the largest standard error for N ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?
N <- seq(100, 5000, len = 100)
p <- 0.5
se <- sqrt(p*(1-p)/N)
(c) 2500

9.
For N=100, the central limit theorem tells us that the distribution of X^ is...
approximately normal with expected value p and standard error p(1-p)/N---------v.

10.
We calculated a vector errors that contained, for each simulated sample, the difference between the actual value p and our estimate X^.
The errors X¯-p are:
approximately normal with expected value 0 and standard error p(1-p)/N---------v.


11.
Make a qq-plot of the errors you generated previously to see if they follow a normal distribution.
Run the supplied code
Use the qqnorm function to produce a qq-plot of the errors.
Use the qqline function to plot a line showing a normal distribution.
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Generate `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Generate a qq-plot of `errors` with a qq-line showing a normal distribution
qqnorm(errors) 
qqline(errors)


12.
If p=0.45 and N=100, use the central limit theorem to estimate the probability that X¯>0.5.
Use pnorm to define the probability that a value will be greater than 0.5.
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Calculate the probability that the estimated proportion of Democrats in the population is greater than 0.5. Print this value to the console.
se <- sqrt(p*(1-p)/N)
1- pnorm(0.5, p, se)


13.
Assume you are in a practical situation and you don't know p. Take a sample of size N=100 and obtain a sample average of X¯=0.51.
What is the CLT approximation for the probability that your error is equal or larger than 0.01?
Calculate the standard error of the sample average using the sqrt function.
Use pnorm twice to define the probabilities that a value will be less than 0.01 or -0.01.
Calculate the probability that the error will be 0.01 or larger.
# Define `N` as the number of people polled
N <-100

# Define `X_hat` as the sample average
X_hat <- 0.51

# Define `se_hat` as the standard error of the sample average
se_hat <- sqrt(X_hat*(1-X_hat)/N)

# Calculate the probability that the error is 0.01 or larger
1-(pnorm(0.01, mean=0, sd=se_hat)-pnorm(-0.01,mean=0, sd=se_hat))



