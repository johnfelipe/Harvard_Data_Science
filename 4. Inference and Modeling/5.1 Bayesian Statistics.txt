
Bayesian Statistics

What does it mean when an election forecaster tells us that a given candidate has a 90% chance of winning?

In the context of the urn model this would be equivalent to stating that the probability that the proportion p
of people voting for this candidate being bigger than 0.5, than 50%, is 90%.

But as we discussed, in the urn model, p is a fixed parameter, and it does not make sense to talk about the 
probability of p being this or that.

With Bayesian statistics, we assume it is in fact random. And then it makes sense to talk about probability.

Forecasters also use models to describe variability at different levels.
For example: sampling variability, pollster to pollster variability,
day to day variability, and election to election variability.

One of the most successful approaches used to describe these different levels of variability
are called hierarchical models. And hierarchical models are best explained in the context of Bayesian statistics.

------------------------------------------------------------------------------------------------------------

Bayes' Theorem

We start by reviewing Bayes' theorem. We do this using a hypothetical cystic-fibrosis test as an example.

Suppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation to represent this.
We're going to write that the probability of a positive test given that you have the disease, D equals 1, is 0.99.

Prob(+ | D=1) = 0.99, Prob(- | D=0) = 0.99

Also, the probability of a negative test given that you don't have the disease, D equals 0, is 0.99.
Here in this formula plus means a positive test, and D represents if you actually have the disease, 1 or 0.


Suppose we select a random person and they test positive. What is the probability that they have the disease?
Prob(D=1 | +)?


The cystic fibrosis rate is 1 in 3,900, which implies that the probability that D equals 1 is 0.00025.
Prob(D=1) = 0.00025


To answer this question, we'll use Bayes' theorem, which in general tells us that the probability of event A 
happening given that event B happening is equal to the probability of them both happening
divided by the probability of B happening.

Pr(A | B) = Pr(A and B)/Pr(B)

The numerator is split using the multiplication rule into the probability of B happening given A happening
times the probability of A happening.
		
	  = (Pr(B|A)Pr(A)) /Pr(B)
	  
This is going to be useful because sometimes we know the probability of A given B and not the probability of B 
given A, as is the case in the cystic fibrosis example.

Here is the Bayes' theorem equation applied to our cystic-fibrosis example.

Pr(D=1|+) 	=	(Pr(+|D=1) * Pr(D=1)) / Pr(+)
		=	(Pr(+|D=1) * Pr(D=1)) / (Pr(+|D=1) * Pr(D=1) + Pr(+|D=0)*Pr(D=0))
		
we know the probability of a positive test given D equals 1: Pr(+|D=1)
and we know the probability of a positive test given D equals 0: Pr(+|D=0)

And we end up with a larger fraction that includes quantities that we know.

		= 	(0.99 * 0.00025) / (0.99 * 0.00025 + Pr(+ | D=0) * Pr(D=0)
		= 	(0.99 * 0.00025) / (0.99 * 0.00025 + 0.01 * Pr(D=0)
		= 	(0.99 * 0.00025) / (0.99 * 0.00025 + 0.01 * 0.99975
		=	0.02 = 2% chance
		
This says that despite the test having 99% accuracy, 
the probability of having the disease given a positive test is only 2%.


This may appear counterintuitive to some. But we're going to see how it makes sense.
The reason this is the case is because we have to factor in the very rare possibility
that a person chosen at random has the disease. This is the Bayesian way of thinking.

Monte Carlo Simulation
We start by randomly selecting 100,000 people from a population in which the disease in question has a 1
in 3,900 prevalence.

So we set the prevalence to be 0.00025.
We set N to be 100,000.

> prev <- 0.00025
> N <- 100000
> outcome <- sample(c("Disease", "Healthy" ), N, replace =TRUE, prob=c(prev, 1-prev))

> N_D <- sum(outcome == "Disease")
> N_D
[1] 23

The number of people with the disease is low, only 23.

> N_H <- sum(outcome == "Healthy")
> N_H
[1] 99977

And of course there's a lot of healthy people, 99,977.
This makes the probability that we see some false positives quite high.
There are so many people without the disease that are getting the test that, although it's rare,
we were going to get a few people getting a positive test despite them being healthy.
Here's the code that shows this.
Each person has a 99% chance of getting the test giving them the right answer.

> accuracy <- 0.99
> test <- vector("character", N)
> test[outcome=="Disease"] <- sample(c("+","-"), N_D, replace=TRUE, prob = c(accuracy, 1- accuracy))
> test[outcome=="Healthy"] <- sample(c("-","+"), N_H, replace=TRUE, prob = c(accuracy, 1- accuracy))



So we have two variables here, the outcome, which is disease or healthy, and test, which is positive or negative.
We can make a table that shows us the number of people in each one of these four combinations.
We do that using the table command.

> table(outcome, test)
	test
outcome		-	+
Disease		0	23
Healthy		99012	965

We can see that there are a lot of people they are healthy that got a positive outcome.
That's because there are so many more healthy people.

From this table, we can also see that the proportion of positive tests that have the disease is 23.
And this is out of a total of 23 plus 965, which is 988.
23/988 ~ 0.02
2%, just like what Bayes' theorem told us it should be.


--------------------------------------------------------------------------------------------------

Bayes' in Practice

In sports, we use Bayesian thinking all the time, even if we don't realize it.
Let's go to the example.
Jose Iglesias is a professional baseball player. 
In April 2013, when he was starting his career, he was performing rather well.
He had been to bat 20 times and he had nine hits, which is an average of 0.450.

Date	At Bats		H	Avg
April	20		9	0.450

This average of 0.450 means Jose had been successful 45% of the times he had batted, which is rather high 
historically speaking.

Note, for example, that no one has finished a season with an average
of 0.400 or more since Ted Williams did it in 1941.

To illustrate the way hierarchical models are powerful, we will try to predict Jose's batting average at the 
end of the season. In a typical season, players have about 500 at bats.
With the techniques we have learned up to now, referred to as frequentist statistics, the best we can do
is provide a confidence interval.

We can think of outcomes for hitting as a binomial with a success rate of p.
So if the success rate is indeed 0.450, the standard error of just 20 at bats can be computed like this.
And it's 0.111.

Sqrt((0.450(1-0.450))/20) = 0.111

We can use this to construct a 95% confidence interval, which will be from 0.228 to 0.672.

This prediction has two problems.
First, it's very large, so it's not very useful.
Second, it's centered at 0.450, which implies that our best guess is that
this relatively unknown player will break Ted Williams' longstanding record.

If you follow baseball, this last statement will seem wrong.
And this is because you're implicitly using the hierarchical model that factors in information
from years of following baseball.

Here we show how we can quantify this intuition. 
First, let's explore the distribution of batting averages
for all players with more than 500 at bats during the seasons 2010, 2011, and 2012.

The average player had an average 0.275 and the standard deviation of the population of all these players was 0.027.

So we can see already that 0.450 would be quite an anomaly,
since it is over six standard deviations away from the average.
So is Jose lucky or the best batter seen in the last 50 years?

If we become convinced that this is just luck, we should trade him to a team that trusts the 0.450 observation 
and is maybe overestimating his potential.

-----------------------------------------------------------------------------------------------

The Hierarchical Model

The hierarchical model provides a mathematical description of how we come to see the observation of 0.450.
First, we pick a player at random with an intrinsic ability summarized by, for example, p--
the proportion of times they will actually be successful. 
Then we see 20 random outcomes with success probability p.
We use a model to represent two levels of variability in our data.
First, each player is assigned a natural ability to hit at birth.
You can think of it that way.

We will use a symbol, p, to represent this ability. You can think of p as a batting average you
would have converged to if this particular player batted over and over and over and over again.

Based on the plots, we assume that p has a normal distribution.
If we just pick a player at random, the random variable p will have a normal distribution.

We also know that the expected value is about 0.270 and a standard error of 0.027.

p has normal distribution
expected value = 0.270
standard error = 0.027

Now, the second level of variability has to do with luck.
Regardless of how good or bad a player is, sometimes 
you have bad luck, and sometimes you have good luck when you're batting.

At each at bat, this player has a probability of success, p.
If we add up these successes and failures as 0's and 1's, then the CLT
tells us that the observed average, let's call it Y, has a normal distribution with expected value p
and standard error square root of p times 1 minus p, divided by N. N is the number of at bats.

observed average Y
expected value p
standard error sqrt(p(1-p)/ N)

N is number of at bats

Statistical textbooks will write the model like this.
We are going to use a tilde to denote the distribution of something.
So p tilde N(mu, tau) is telling us that p, which is now a random variable,
has a distribution that is normal with respected value mu and standard error tau.

p ~ N(mu, tau) describes randomness in picking a player

Now we describe the distribution at the next level.
So the distribution of the observed batting average Y, given that this player has a talent, p, is also
normally distributed with expected value p and a standard error sigma.

Y | p~N(p,sigma) describe randomness in the performance of this particular player


In our case:
mu = 0.270
tau = 0.027
sigma^2 = p(1-p)/N


Because there are two levels, we call these hierarchical models.
The first one is the player to player variability.
The second is the variability due to luck when batting.

In a Bayesian framework,
first level is called prior distribution
second level is called sampling distribution


Now, let's use this model for Jose's data.
Suppose we want to predict his innate ability in the form of his true batting average, p.
This would be the hierarchical model for our data. p is normal with expected value 0.275, standard error 0.027.
And Y, given p, is normal with expected value p-- we don't know what p is; we're trying to estimate it--
and standard error 0.111.

p ~ N(0.275, 0.027)
Y | p~N(p, 0.111)


We now are ready to compute what is called a posterior distribution to summarize our prediction of p.

What Bayesian statistics lets us do is compute the probability distribution of p given that we have observed data.
This is called a posterior distribution.

Again, the probability distribution of p conditioned that we have observed data Y. 
There is a continuous version of Bayes' rule that lets us compute the posterior distribution in cases like this,
where the distributions are continuous. The normal distribution is a continuous distribution.


We can use this continuous version of Bayes' rule to derive a "posterior probability function" for p
assuming that we have observed Y equals, for example, little y.

In our case, we can show that this posterior distribution
follows a normal distribution with expected value given by this formula.
Now, let's study this formula closely, because it is very informative, and it actually explains our intuition.

E(p|y)	= Bmu + (1-B)Y
	= mu + (1-B)(Y - mu)
B	= sigma^2/ (sigma^2 + tau^2)

Note that this is a weighted average between mu-- mu is the average for all baseball players--
and Y, what we have observed for Jose. 

So if B were to be 1, this would mean that we're just saying Jose is just an average player, so we're going 
to predict mu.

If B is 0, we would be saying forget the past, we're going to predict that Jose is what he is, what we've observed.
His average is 0.450.

Now, look at how B is constructed. B is the standard error sigma squared divided
by the sum of the standard error sigma squared, plus the standard error tau squared.

So B, the weight, is going to be closer to 1 when sigma is large.

When is sigma large?
Sigma is large when the variance, when the standard error, of our observed data is large.

When we don't trust our observed data too much, sigma is large.
So we make B 1.

In this case, we would predict that Jose Iglesias is an average player.
We would predict mu.

On the other hand, if the sigma is very, very small, this means that we really do trust our data Y,
and we're actually going to say, no, we trust our data, and we are going to actually ignore the past and predict Y.

Of course, B is somewhere in the middle, so we get something in the middle.
This weighted average is sometimes referred to as shrinking, because it shrinks the observed Y towards a prior 
mean, which in this case is mu.


We shrink the observed data towards what the average player is, mu.
In the case of Jose Iglesias, we can fill in those numbers
and get that the expected value for the posterior distribution is 0.285.

E(p|Y=0.450)	= B*.275 + (1-B)0.450
	= 0.275 + (1-B)(0.450 - 0.275)
B	= 0.111^2/ (.0111^2 + 0.027^2) = 0.944

E(p|Y=0.450) ~ 0.275

It's a number between the 0.450 that we saw and the 0.270 that we've seen historically for the average player.

The standard error can also be computed. We use mathematics to do this.
SE(p|y)^2 	= 	1/(1/sigma^2 + 1/tau^2)
		=	1/(1/0.111^2 + 1/0.027^2) = 0.00069
		
This is the formula for the standard error of the posterior distribution.
And in this case, we get that the standard deviation is 0.026.

So we started with a frequentist 95% confidence interval that ignored data
from other players from the past and simply summarized Jose's data as 0.450
plus or minus 0.220.

0.450 +- 0.220

Then we used a Bayesian approach that incorporated data from the past,
from other players, and obtained a posterior probability.
We should point out that this is actually referred to as an empirical Bayesian approach.

In a traditional Bayesian approach, we simply state the prior.
In an empirical Bayesian approach, we use data to construct the prior, and that's what we did here.

Using the posterior distribution, we can report what is called a 95% credible interval.
This is a region centered at the expected value with a 95% chance of occurring.
Remember that p is now random, so we can talk about the chances of p happening, falling here or falling there.

E(p|Y)

In our case, we can construct this by adding twice the standard error
to the expected value of the posterior distribution.

E(p|Y) +- 2SE(p|y)
0.285 +- 0.052

Note that the Bayesian approach is giving us a prediction that is much lower than the 0.450.
It's also giving us a much more precise interval.

The Bayesian credible interval suggests that if another team that is ignoring
past data is impressed by the 450, the 0.450 observation, we should consider trading Jose as they probably overvalue--
if we trust our new prediction that is predicting that he will be just slightly above average.

Interestingly, the Red Sox traded Jose Iglesias to the Detroit Tigers in July 2013.
Let's look at his batting average for the next five months.

Month		At Bat		Hits	AVG
April		20		9	0.450
May		26		11	0.423
June		86		34	0.395
July		83		17	0.205
August		85		25	0.294
September	50		10	0.200
Total w/0 April	330		97	0.293


Notice that if we take April out, his batting average for the rest of the season was 0.293.

Although both intervals, the frequentist confidence interval and the Bayesian credible intervals,
included the final batting average of 0.293, the Bayesian credible interval provided a much more precise prediction.

In particular, it predicted that he would not be as good for the remainder of the season.
So trading him was perhaps the right decision.

-----------------------------------------------------------------------------------------------------------

Assessment

1.
In 1999 in England Sally Clark was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998, and she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500 × 8,500 ˜ 73 million.
Based on what we've learned throughout this course, which statement best describes a potential flaw in Sir Meadow's reasoning?
He assummed the second death was independent of the first son. 

2.
Let's assume that there is in fact a genetic component to SIDS and the the probability of Pr(second case of SIDS|first case of SIDS)=1/100, is much higher than 1 in 8,500.
What is the probability of both of Sally Clark's sons dying of SIDS?
Calculate the probability of both sons dying to SIDS.
# Define `Pr_1` as the probability of the first son dying of SIDS
Pr_1 <- 1/8500

# Define `Pr_2` as the probability of the second son dying of SIDS
Pr_2 <- 1/100

# Calculate the probability of both sons dying of SIDS. Print this value to the console.
Pr_12 <- (1/100) * Pr_1
print(Pr_12)


3.
Many press reports stated that the expert claimed the probability of Sally Clark being innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written like this:

Pr(mother is a murderer|two children found dead with no evidence of harm)
Bayes' rule tells us this probability is equal to:

Bayes' rule tells us this probability is equal to:
Pr(two children found dead with no evidence of harm|mother is a murderer)Pr(mother is a murderer)Pr(two children found dead with no evidence of harm)


4.
Assume that the probability of a murderer finding a way to kill her two children without leaving evidence of physical harm is:

Pr(two children found dead with no evidence of harm|mother is a murderer)=0.50
Assume that the murder rate among mothers is 1 in 1,000,000.

Pr(mother is a murderer)=1/1,000,000
According to Bayes' rule, what is the probability of:

Pr(mother is a murderer|two children found dead with no evidence of harm)

Use Bayes' rule to calculate the probability that the mother is a murderer, considering the rates of murdering mothers in the population, the probability that two siblings die of SIDS, and the probability that a murderer kills children without leaving evidence of physical harm.

# Define `Pr_1` as the probability of the first son dying of SIDS
Pr_1 <- 1/8500

# Define `Pr_2` as the probability of the second son dying of SIDS
Pr_2 <- 1/100

# Define `Pr_B` as the probability of both sons dying of SIDS
Pr_B <- Pr_1*Pr_2

# Define Pr_A as the rate of mothers that are murderers
Pr_A <- 1/1000000

# Define Pr_BA as the probability that two children die without evidence of harm, given that their mother is a murderer
Pr_BA <- 0.50

# Define Pr_AB as the probability that a mother is a murderer, given that her two children died with no evidence of physical harm. Print this value to the console.
Pr_BA * Pr_A / Pr_B



5.
After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was "no statistical basis" for the expert's claim. They expressed concern at the "misuse of statistics in the courts". Eventually, Sally Clark was acquitted in June 2003.

In addition to misusing the multiplicative rule as we saw earlier, what else did Sir Meadow miss?
He did not take into account how rare it is for a mother to murder her children.



6.
Florida is one of the most closely watched states in the U.S. election because it has many electoral votes and the election is generally close. Create a table with the poll spread results from Florida taken during the last days before the election using the sample code.

The CLT tells us that the average of these spreads is approximately normal. Calculate a spread average and provide an estimate of the standard error.
Calculate the average of the spreads. Call this average avg in the final table.
Calculate an estimate of the standard error of the spreads. Call this standard error se in the final table.
Use the mean and sd functions nested within summarize to find the average and standard deviation of the grouped spread data.
Save your results in an object called results.
# Load the libraries and poll data
library(dplyr)
library(dslabs)
data(polls_us_election_2016)

# Create an object `polls` that contains the spread of predictions for each candidate in Florida during the last polling days
polls <- polls_us_election_2016 %>% 
  filter(state == "Florida" & enddate >= "2016-11-04" ) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# Examine the `polls` object using the `head` function
head(polls)

# Create an object called `results` that has two columns containing the average spread (`avg`) and the standard error (`se`). Print the results to the console.
results <- polls %>%  summarize(avg=mean(spread), se= sd(spread)/sqrt(n()))
results




7.
Assume a Bayesian model sets the prior distribution for Florida's election night spread d to be normal with expected value µ and standard deviation t.

What are the interpretations of µ and t?
µ and t summarize what we would predict for Florida before seeing any polls.
Based on past elections, we would set mu close to 0, because both Republicans and Democrats have won, and tau at about 0.02, because these elections tend to be close.


8.
The CLT tells us that our estimate of the spread d^ has a normal distribution with expected value d and standard deviation s, which we calculated in a previous exercise.

Use the formulas for the posterior distribution to calculate the expected value of the posterior distribution if we set µ=0 and t=0.01.
Define µ and t
Identify which elements stored in the object results represent s and Y
Estimate B using s and t
Estimate the posterior distribution using B, µ, and Y
# The results` object has already been loaded. Examine the values stored: `avg` and `se` of the spread
results

# Define `mu` and `tau`
mu <- 0
tau <- 0.01

# Define a variable called `sigma` that contains the standard error in the object `results
sigma <- results$se[1]
sigma

# Define a variable called `Y` that contains the average in the object `results`
Y <- results$avg[1]
Y


# Define a variable `B` using `sigma` and `tau`. Print this value to the console.
B <- sigma^2 / (sigma^2 + tau^2)
B

# Calculate the expected value of the posterior distribution
B * mu + (1-B)*Y


9.
Compute the standard error of the posterior distribution.
Using the variables we have defined so far, calculate the standard error of the posterior distribution.
Print this value to the console.
# Here are the variables we have defined
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

# Compute the standard error of the posterior distribution. Print this value to the console.
sqrt(1/(1/sigma^2 + 1/tau^2))


10.
Using the fact that the posterior distribution is normal, create an interval that has a 95% of occurring centered at the posterior expected value. Note that we call these credible intervals.
Calculate the 95% credible intervals using the qnorm function.
Save the lower and upper confidence intervals as an object called ci. Save the lower confidence interval first.
# Here are the variables we have defined in previous exercises
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

# Construct the 95% credible interval. Save the lower and then the upper confidence interval to a variable called `ci`.
exp_value <- B * mu + (1-B)*Y
ci <- c(exp_value - qnorm(0.975)*se, ev + qnorm(0.975)*se)
ci


11.
According to this analysis, what was the probability that Trump wins Florida?
Using the pnorm function, calculate the probability that the spread in Florida was less than 0.
# Assign the expected value of the posterior distribution to the variable `exp_value`
exp_value <- B*mu + (1-B)*Y 

# Assign the standard error of the posterior distribution to the variable `se`
se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

# Using the `pnorm` function, calculate the probability that the actual spread was less than 0 (in Trump's favor). Print this value to the console.
pnorm(0, exp_value, se)


12.
We had set the prior variance t to 0.01, reflecting that these races are often close.

Change the prior variance to include values ranging from 0.005 to 0.05 and observe how the probability of Trump winning Florida changes by making a plot.

Create a vector of values of taus by executing the sample code.
Create a function using function(){} called p_calc that first calculates B given tau and sigma and then calculates the probability of Trump winning, as we did in the previous exercise.
Apply your p_calc function across all the new values of taus.
Use the plot function to plot t on the x-axis and the new probabilities on the y-axis.


# Define the variables from previous exercises
mu <- 0
sigma <- results$se
Y <- results$avg

# Define a variable `taus` as different values of tau
taus <- seq(0.005, 0.05, len = 100)

# Create a function called `p_calc` that generates `B` and calculates the probability of the spread being less than 0
p_calc <- function(tau){
  B <- sigma^2 / (sigma^2 + tau^2)
  exp_value <- B*mu + (1-B)*Y 
  se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))
  pnorm(0, exp_value, se)
}

# Create a vector called `ps` by applying the function `p_calc` across values in `taus`
ps<-p_calc(taus)

# Plot `taus` on the x-axis and `ps` on the y-axis
plot(taus, ps)
