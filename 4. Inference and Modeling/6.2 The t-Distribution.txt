
The t-Distribution

Previously, we made use of the Central Limit Theorem with sample sizes as small as 15.
Because we're also estimating a second parameter, sigma, further variability is introduced into our confidence interval.
And this results in a confidence interval that is overconfident because it doesn't account for that variability.

For very large sample sizes, this extra variability is negligible.
But in general, for values smaller than 30, we need to be cautious about using the Central Limit Theorem.

However, if the data in the urn is known to follow a normal distribution,
in other words, if the population data is known to follow a normal distribution, then we actually
have a mathematical theory that tells us how much bigger
we need to make the intervals to account for the estimation of sigma.


Using this theory, we can construct confidence intervals for any urn but again, only if the data in the urn is
known to follow a normal distribution.
So for the 0, 1 data of previous urn models, this theory definitely does not apply.


The statistic on which confidence intervals for d are based is this one.
We've seen it earlier.
We call it Z. The CLT tells us that Z is approximately normally distributed
with expected value 0 and standard error 1.

Z = (X¯ - d)/(sigma/sqrt(N))

But in practice, we don't know sigma, so we use s instead. We substitute s where we have the sigma.
But by doing this, we introduce some variability. The s, as variability, is estimated from data.

Z = (X¯ - d)/(s/sqrt(N))


This theory that we mentioned tells us that Z follows what is called a "t-distribution" with what
is called N minus 1 degrees of freedom.

degrees of freedom
N-1 

The degrees of freedom is a parameter that controls the variability via what are called fatter tails.


You can see that in this figure, where we have t-distributions with degrees of freedom 3, 5, and 15.
And you can see how the tails, the ends, go higher and higher,
meaning that large values have larger probabilities for smaller values of the degrees of freedom.


In our case of pollster data, if we are willing to assume that the pollster effect data is normally distributed,
then we can use this theory.
Based on the sample, we can corroborate if, in fact, the data is normally distributed.


Here's a q-q plot showing us our sample data versus a normal distribution.
It's not a perfect match, but is relatively close.
And this particular theory is quite robust to deviations from normality.

So once we make that decision, then, perhaps a better confidence interval
for d is constructed using the t-distribution instead of the normal distribution.
So all we change is the 1.96.

> z <- qt(0.975, nrow(one_poll_per_pollster)-1)
> one_poll_per_pollster %>%
+	summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) %>%
+	mutate(start = avg-moe, end = avg + moe)
	avg	moe	start	end
[1]	0.0290	0.0134	0.0156	0.0424


We now change to the quantile coming from a t-distribution with 14 degrees of freedom.
The new confidence interval goes from 1.5% to 4.2%.

So it is a little bit bigger than the one we made using the normal distribution.
This is, of course, expected because the quantile from the t-distribution
is larger than the quantile from the normal distribution, as we can see here.

> qt(0.975, 14)
[1] 2.144787
> qnorm(0.975)
[1] 1.959964


FiveThirtyEight uses the t-distribution to generate errors that better model the deviation we see in election data.
Again, because they have fatter tails.
So, for example, the deviation we saw in Wisconsin between the polls and the actual result, the actual result
was that Trump won by 0.7%, is more in line with t-distributed data than normal distributed data.


-------------------------------------------------------------------------------------------------------------


Assessment


1.
We know that, with a normal distribution, only 5% of values are more than 2 standard deviations away from the mean.

Calculate the probability of seeing t-distributed random variables being more than 2 in absolute value when the degrees of freedom are 3.
Use the pt function to calculate the probability of seeing a value less than or equal to the argument.

# Calculate the probability of seeing t-distributed random variables being more than 2 in absolute value when 'df = 3'.
2*(1-pt(abs(2), df=3))



2.

Now use sapply to compute the same probability for degrees of freedom from 3 to 50.

Make a plot and notice when this probability converges to the normal distribution's 5%.


Make a vector called df that contains a sequence of numbers from 3 to 50.
Using function, make a function called pt_func that recreates the calculation for the probability that a value is greater than 2 as an absolute value for any given degrees of freedom.
Use sapply to apply the pt_func function across all values contained in df. Call these probabilities probs.
Use the plot function to plot probs on the x-axis and df on the y-axis.


# Generate a vector 'df' that contains a sequence of numbers from 3 to 50
df <- seq(3,50)

# Make a function called 'pt_func' that calculates the probability that a value is more than |2| for any degrees of freedom 
pt_func <- function(df){
  2*(1-pt(abs(2), df))
}


# Generate a vector 'probs' that uses the `pt_func` function to calculate the probabilities
probs <- pt_func(df)

# Plot 'df' on the x-axis and 'probs' on the y-axis
plot(df, probs)



3.
In a previous section, we repeatedly took random samples of 50 heights from a distribution of heights. We noticed that about 95% of the samples had confidence intervals spanning the true population mean.

Re-do this Monte Carlo simulation, but now instead of N=50, use N=15. Notice what happens to the proportion of hits.

Use the replicate function to carry out the simulation. Specify the number of times you want the code to run and, within brackets, the three lines of code that should run.
First use the sample function to randomly sample N values from x.
Second, create a vector called interval that calculates the 95% confidence interval for the sample. You will use the qnorm function.
Third, use the between function to determine if the population mean mu is contained between the confidence intervals.
Save the results of the Monte Carlo function to a vector called res.
Use the mean function to determine the proportion of hits in res.
# Load the neccessary libraries and data
library(dslabs)
library(dplyr)
data(heights)

# Use the sample code to generate 'x', a vector of male heights
x <- heights %>% filter(sex == "Male") %>%
  .$height

# Create variables for the mean height 'mu', the sample size 'N', and the number of times the simulation should run 'B'
mu <- mean(x)
N <- 15
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Generate a logical vector 'res' that contains the results of the simulations
res <- replicate(B, {
    X <- sample(x, N, replace = TRUE)
    interval <- c(qnorm(1-.975,mean=mean(X),sd=(sd(X)/sqrt(N))),qnorm(.975,mean=mean(X),sd(X)/sqrt(N)))
    between(mu, interval[1], interval[2])
})

# Calculate the proportion of times the simulation produced values within the 95% confidence interval. Print this value to the console.
mean(res)




4.
N=15 is not that big. We know that heights are normally distributed, so the t-distribution should apply. Repeat the previous Monte Carlo simulation using the t-distribution instead of using the normal distribution to construct the confidence intervals.

What are the proportion of 95% confidence intervals that span the actual mean height now?

Use the replicate function to carry out the simulation. Specify the number of times you want the code to run and, within brackets, the three lines of code that should run.
First use the sample function to randomly sample N values from x.
Second, create a vector called interval that calculates the 95% confidence interval for the sample. Remember to use the qt function this time to generate the confidence interval.
Third, use the between function to determine if the population mean mu is contained between the confidence intervals.
Save the results of the Monte Carlo function to a vector called res.
Use the mean function to determine the proportion of hits in res.
# The vector of filtered heights 'x' has already been loaded for you. Calculate the mean.
mu <- mean(x)

# Use the same sampling parameters as in the previous exercise.
set.seed(1)
N <- 15
B <- 10000

# Generate a logical vector 'res' that contains the results of the simulations using the t-distribution
res <- replicate(B, {
    X <- sample(x, N, replace = TRUE)
    z <- qt(0.975, N-1)
    moe <- z*sd(X)/sqrt(N)
    interval <- c(mean(X)-moe, mean(X)+moe)
    between(mu, interval[1], interval[2])
})

# Calculate the proportion of times the simulation produced values within the 95% confidence interval. Print this value to the console.
mean(res)


5.
Why did the t-distribution confidence intervals work so much better?

The t-distribution takes the variability into account and generates larger confidence intervals.

