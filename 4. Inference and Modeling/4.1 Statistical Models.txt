
Poll Aggregators

In the 2012 presidential election, Barack Obama won the electoral college and he won the popular vote
by a margin of 3.9%. Let's go back to the week before the election before we knew this outcome.
Nate Silver was giving Obama a 90% chance of winning. Yet, none of the individual polls were nearly that sure.

How is Nate Silver so sure?

Let's do a Monte Carlo Simulation.

We're going to generate results for 12 polls taken the week before the election.
We're going to mimic the sample sizes from actual polls.
We're going to construct and report 95% confidence intervals for each of these 12 polls.

> d <- 0.039
> Ns <- c(1298,533,1342,897,774,254,812,324,1291,1056,2172,516)
> p <- (d+1)/2
>
> confidence_intervals <- sapply(Ns, function(N) {
+	X <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p,p))
+	X_hat <- mean(X)
+	SE_hat <- sqrt(X_hat*(1-X_hat)/N)
+	2*c(X_hat, X_hat - 2*SE_hat, X_hat + 2*SE_hat)-1
+ })


(explaination)
d_hat = X_hat-(1-X_hat) = 2*X-hat-1 
E(d_hat) = E(2*X_hat-1) = 2E(X_hat)-1
Var(d_hat) = Var(2*X_hat-1) = 2*Var(X_hat) or SE(d_hat) = sqrt(2)*SE(X_hat)
since d_hat - qnorm(0.975)*SE(d_hat), we can rewrite this in terms of X_hat as:
(2*X_hat - 1) - 2*sqrt(2)*SE(X_hat)
The other 2 you see is because qnorm(0.975) ~ 2 
This will resolve into 2*X_hat - 2*sqrt(2)*SE(X_hat) -1 which is the second element of the final vector.


We're going to generate the data using the actual outcome, 3.9%. So d, the difference, the spread, is 0.039.
The sample sizes were selected to mimic regular polls.
So we see that the first one is 1,298, the second one is 533, et cetera.
We're also going to define p, the proportion of Democrats--or actually, the proportion of people voting for Obama,
as the spread plus 1 divided by 2.

> polls <- data.frame(poll=1:ncol(confidence_intervals),
+	t(confidence_intervals),
+	sample_size=Ns)
> names(polls)<-c("poll", "estimate", "low", "high", "sample_size")
> polls

Confidence level at 95% on the dashed line, however all 12 polls intervals include 0 (solid line).
So graph shows a toss up if you ask each individual pollster.


Now we're going to describe how pundits are missing a key insight.
Poll aggregators, such as Nate Silver, realize that by combining the results of different polls,
you could greatly improve precision.

By doing this, effectively we're conducting a poll with a huge sample size.
As a result, we can report a smaller 95% confidence interval, and therefore a more precise prediction.

Although as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what
we would have obtained had we made one large poll with, in this case, 11,269 people, participants.
> sum(polls$sample_size)
[1] 11269


Basically we construct an estimate of the spread-- let's call it d-- with a weighted average in the following way.
> d_hat <- polls %>%
+	summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %>%
+	.$avg

We basically multiply each individual spread by the sample size.
That's going to give us a total spread. And then we're going to divide by the total number
of participants in our aggregated poll.

This gives us d_hat, which is an estimate of d.

Once we have an estimate of d, we can construct an estimate for the proportion voting for Obama, which we can then
use to estimate the standard error.

> p_hat <- (1+d_hat)/2
> moe <- 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$sample_size))
> moe
[1] 0.0185

Once we do this, we see that our margin of error of the aggregated poll is 0.018.

Thus, using the weighted average, we can predict that the spread will be 3.1% plus or minus 1.8%,
which not only includes the actual result but is quite far from including 0.

> round(d_hat*100,1)
[1] 3.1
> round(moe*100,1)
[1] 1.8

The actual data science exercise of forecasting elections is much more complicated and it involves statistical 
modeling.

------------------------------------------------------------------------------------------------------------

Pollsters and Multilevel Models


Although the polls didn't correctly predicted Trump to have a higher probability, 
note that 29% is a higher probability than the probability of tossing
two coins and getting two heads. It's also much, much bigger than what the other pollsters had predicted.

We will start by looking at the predictions for the popular vote.

FiveThirtyEight predicted a 3.6% advantage for Clinton.

Their interval, their prediction interval, included the actual result of 2.1%, 48.2% for Clinton
compared to 46.1% for Trump.

They were much more confident about Clinton winning this, the popular vote, giving her a 81.4% chance of winning.

Next, we're going to look at actual public polling data from the 2016 US
presidential election to show how models are motivated and built to produce these predictions.

-------------------------------------------------------------------------------------------------------------

Poll Data and Pollster Bias

Let's look at the 2016 polling data:

> data(polls_us_election_2016)
> names(polls_us_election_2016)

The table includes results for national polls, as well as state polls, taken in the year before the election.

For this first illustrative example, we will filter the data to include national polls that happened during the week
before the election. We also remove polls that FiveThirtyEight 
has determined not to be reliable, and they have graded them with a B or less. Some polls have not been graded.
And we're going to leave these in.

> polls <- polls_us+election_2016 %>%
+	filter(state == "U.S." & enddate >= "2016-10-31" &
+		(grade %in% c("A+","A","A-","B+") | is.na(grade)))

Add a spread estimate:
> polls <- polls %>%
+	mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)


Proportion voting for Clinton = p
Proportion voting for Trump = 1-p
Spread = d = 2p-1

Note that we have several estimates of this spread comingfrom the different polls.
The theory we learned tells us that these estimates are a random variable with probability distribution
that is approximately normal.

The expected value is the election night spread, d.

And the standard error is 2 times the square root of p times 1 minus p divided by the sample size N. 
2* sqrt( p(1-p)/N )


Assuming the urn model we described earlier are useful models, we can use this information
to construct a confidence interval based on the aggregated data.
The estimated spread is now computed like this because now the sample size is the sum of all the sample sizes.

> d_hat <- polls %>%
+	summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %>%
+	.$d_hat


And if we use this, we get a standard error, typing this code, that then leads us to a margin of error of .0066,
a very small margin of error:
> p_hat <- (d_hat+1)/2
> moe <- 1.96 * 2 * sqrt(p_hat*1-p_hat)/sum(polls$samplesize))
> moe
[1] 0.006623178

So, if we were going to use this data, we would report a spread of 1.43% with a margin of error of 0.66%.

On election night, we find out that the actual percentage is 2.1%, which is outside of the 95% confidence interval.
So, what happened? Was this just bad luck?

Histogram shows another problem:
> polls %>%
+	ggplot(aes(spread)) +
+	geom_histogram(color="black", binwidth = 0.1)


Does not look normally distributed and standard error appears to be larger than 0.0066.

The theory is not quite working here. Why?

To see why, notice that various pollsters are involved and some are taking several polls a week.

> polls %>% group_by(pollster) %>% summarize(n())

Let's visualize the data for the pollsters that are regularly polling.
We write this piece of code that first filters for only pollsters that polled more than 6 times.

> polls %>% group_by(pollster) %>%
+	filter(n() >= 6) %>%
+	ggplot(aes(pollster, spread)) +
+	geom_point() +
+	theme(axis.text.x = element_text(angle=90, hjust =1))

And then we simply plot the spreads estimated by each pollster. Each one has between five and six.

This plot reveals an unexpected result. First note that the standard error, predicted by theory for each poll--
now, we're going to do this poll by poll-- gives us values between 0.018 and 0.033.

> polls %>% group_by(pollster) %>%
+	filter(n() >= 6) %>%
+	summarize(se = 2*sqrt(p_hat * (1-p_hat)/median(samplesize)))

This appears to be right. This appears to be consistent with what we see in the plot.
However, there appears to be differences across the polls. This is not explained by the theory.

Note for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump
while Ipsos is predicting a win larger than 5% for Clinton.

The theory of learned says nothing about different pollsters producing polls with different expected values.
All the polls should have the same expected value, the actual spread, the spread we will see on election night.

FiveThirtyEight refers to these differences as "house effects." We can also call them pollster bias.
Rather than use urn model theory, we're instead 
going to develop a data-driven model to produce a better estimate and a better confidence interval.

----------------------------------------------------------------------------------------------------------

Data-Driven Models

For each pollster, let's collect their last-reported result before the election using this simple piece of code.

> one_poll_per_pollster <- polls %>% group_by(pollster) %>%
+	filter(enddate == max(enddate)) %>%
+	ungroup()

Here's a histogram of the data for these 15 pollsters.
> one_poll_per_pollster %>%
+	ggplot(aes(spread)) + geom_histogram(binwidth = 0.01)


In the previous video, we saw that using the urn model theory to combine these results might not be
appropriate due to the pollster effect. Instead, we will model this spread data directly.

The new model can also be thought of as an urn model, although the connection to the urn idea is not as direct.
Rather than having beads with zeros and ones inside the urn, 
now the urn contains poll results from all possible pollsters.

We assume that the expected value of our urn is the actual spread, which we have been calling
d, which is equal to 2p minus 1.

d=2p-1

Now, because rather than zeros and ones our urn contains continuous numbers between minus 1 and 1,
the standard deviation of the urn is no longer the square root of p times 1 minus p.

The standard error for our average now includes pollster to polster variability.

Our new urn also includes the sample variability from the polling.
Regardless, this standard deviation is now an unknown parameter.

In statistics textbooks, the Greek symbol sigma is used to represent this parameter.

So now we have two unknown parameter:
1. expected value d (what we want to estimate)
2. standard deviation, sigma


Our task is to estimate d and provide inference for it.
Because we model the observed values, let's call them X1 through XN,as a random sample from the urn, 
the central limit theorem
still works for the average of these values because it's an average of independent random variables.

For a large enough sample size N, the probability distribution of the sample average, which we'll call X-bar,
is approximately normal with expected value d and standard deviation sigma divided by the square root of N. 

If we are willing to consider N equals to 15 large enough, we can use this to construct a confidence interval.

But we don't know sigma.  But the theory tells us that we can estimate the urn model
sigma, the unobserved sigma, with the "sample standard 
deviation", which is defined like this with this mathematical formula.

s = (1/(n-1)) E(i=1:N)( (Xi - X¯ )^2)

Now note in the mathematical formula that unlike the population standard deviation, we now divide by N minus 1.
This makes s a better estimate of sigma than if we just divided by N.

Now the sd function in R computes the sample standard deviation:
> sd(one_poll_per_pollster$spread)
[1] 0.024

We are now ready to form a confidence interval based on our new data-driven model.

We simply use the central limit theorem and create a confidence interval using this simple code:
> results <- one_poll_per_pollster %>%
+	summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
+	mutate(start = avg - 1.96*se, end = avg + 1.96*se)
> round(results*100,1)
	avg	se	start	end
[1]	2.9	0.6	1.7	4.1


Note that our new confidence interval is wider, and it now incorporates the pollster variability.
It does include the election-night result of 2.1%, and also it's small enough not to include 0.
Which means that in this particular case, we would have been quite confident that Clinton would win the popular vote.

Now, are we now ready to declare a probability of Clinton winning as the pollsters do?
Not yet.
In our model, d is a fixed parameter, so we can't talk about probabilities.
To provide probabilities, we'll need to learn something new.
We're going to have to learn about Bayesian statistics.
And we do that next.

-----------------------------------------------------------------------------------------------------------

Assessment

1.
We have been using urn models to motivate the use of probability models. However, most data science applications are not related to data obtained from urns. More common are data that come from individuals. Probability plays a role because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population.
Let's revisit the heights dataset. For now, consider x to be the heights of all males in the data set. Mathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it.
What are the population average and standard deviation of our population?
Execute the lines of code that create a vector x that contains heights for all males in the population.
Calculate the average of x.
Calculate the standard deviation of x.
# Load the 'dslabs' package and data contained in 'heights'
library(dslabs)
data(heights)

# Make a vector of heights from all males in the population
x <- heights %>% filter(sex == "Male") %>%
  .$height

# Calculate the population average. Print this value to the console.
mean(x)

# Calculate the population standard deviation. Print this value to the console.
sd(x)


2.
Call the population average computed above µ and the standard deviation s. Now take a sample of size 50, with replacement, and construct an estimate for µ and s.
Use the sample function to sample N values from x.
Calculate the mean of the sampled heights.
Calculate the standard deviation of the sampled heights.
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `X` as a random sample from our population `x`
X <- sample(x,N, replace=TRUE)
X

# Calculate the sample average. Print this value to the console.
mean(X)

# Calculate the sample standard deviation. Print this value to the console.
sd(X)


3.
What does the central limit theory tell us about the sample average and how it is related to µ, the population average?
It is a random variable with expected value µ and standard error s/sqrt(N).


4.
We will use X¯ as our estimate of the heights in the population from our sample size N. We know from previous exercises that the standard estimate of our error X¯-µ is s/N--v.
Construct a 95% confidence interval for µ.
Use the sd and sqrt functions to define the standard error se
Calculate the 95% confidence intervals using the qnorm function. Save the lower then the upper confidence interval to a variable called ci.
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)

# Define `se` as the standard error of the estimate. Print this value to the console.
se <-sd(X)/ sqrt(N)
se

# Construct a 95% confidence interval for the population average based on our sample. Save the lower and then the upper confidence interval to a variable called `ci`.
avg = mean(X)
ci <- c(qnorm(1-.975,mean=avg,sd=se),qnorm(.975,mean=avg,se))



5.
Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include µ?
Use the replicate function to replicate the sample code for B <- 10000 simulations. Save the results of the replicated code to a variable called res. The replicated code should complete the following steps: -1. Use the sample function to sample N values from x. Save the sampled heights as a vector called X. -2. Create an object called interval that contains the 95% confidence interval for each of the samples. Use the same formula you used in the previous exercise to calculate this interval. -3. Use the between function to determine if µ is contained within the confidence interval of that simulation.
Finally, use the mean function to determine the proportion of results in res that contain mu.
# Define `mu` as the population average
mu <- mean(x)

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `B` as the number of times to run the model
B <- 10000


# Define an object `res` that contains a logical vector for simulated intervals that contain mu

res <- replicate(B, {
    X <- sample(x, N, replace = TRUE)
    interval <- c(qnorm(1-.975,mean=mean(X),sd=(sd(X)/sqrt(N))),qnorm(.975,mean=mean(X),sd(X)/sqrt(N)))
    between(mu, interval[1], interval[2])
})

# Calculate the proportion of results in `res` that include mu. Print this value to the console.
mean(res)

6.
In this section, we used visualization to motivate the presence of pollster bias in election polls. Here we will examine that bias more rigorously. Lets consider two pollsters that conducted daily polls and look at national polls for the month before the election.

Is there a poll bias? Make a plot of the spreads for each poll.
Use ggplot to plot the spread for each of the two pollsters.
Define the x- and y-axes usingusing aes() within the ggplot function.
Use geom_boxplot to make a boxplot of the data.
Use geom_point to add data points to the plot.
# Load the libraries and data you need for the following exercises
library(dslabs)
library(dplyr)
library(ggplot2)
data("polls_us_election_2016")

# These lines of code filter for the polls we want and calculate the spreads
polls <- polls_us_election_2016 %>% 
  filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid") &
           enddate >= "2016-10-15" &
           state == "U.S.") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) 

head(polls)
# Make a boxplot with points of the spread for each pollster
polls %>% group_by(pollster) %>% ggplot(aes(pollster, spread)) + geom_boxplot() + geom_point()



7.
The data do seem to suggest there is a difference between the pollsters. However, these data are subject to variability. Perhaps the differences we observe are due to chance. Under the urn model, both pollsters should have the same expected value: the election day difference, d.

We will model the observed data Yij in the following way:

Yij=d+bi+eij
with i=1,2 indexing the two pollsters, bi the bias for pollster i, and eij poll to poll chance variability. We assume the e are independent from each other, have expected value 0 and standard deviation si regardless of j.

Which of the following statements best reflects what we need to know to determine if our data fit the urn model?
Is b1?b2?


8.
We modelled the observed data Yij as:

Yij=d+bi+eij
On the right side of this model, only eij is a random variable. The other two values are constants.

What is the expected value of Y1j?
d+b1



9.
Suppose we define Y¯1 as the average of poll results from the first poll and s1 as 
the standard deviation of the first poll.

What is the expected value and standard error of Y¯1?
The expected value is d+b1 and the standard error is s1/sqrt(N2)



10.
Now we define Y¯2 as the average of poll results from the second poll.

What is the expected value and standard error of Y¯2?
The expected value is d+b2 and the standard error is s2/sqrt(N2)


11.
Using what we learned by answering the previous questions, what is the expected value of Y¯2-Y¯1?
b2-b1


12.
Using what we learned by answering the questions above, what is the standard error of Y¯2-Y¯1?

sqrt(s2^2/N2 + s1^2/N1)


13.
The answer to the previous question depends on s1 and s2, which we don't know. We learned that we can estimate these values using the sample standard deviation.

Compute the estimates of s1 and s2.
Group the data by pollster.
Summarize the standard deviation of the spreads for each of the two pollsters.
Store the pollster names and standard deviations of the spreads (s) in an object called sigma
# The `polls` data have already been loaded for you. Use the `head` function to examine them.
head(polls)

# Create an object called `sigma` that contains a column for `pollster` and a column for `s`, the standard deviation of the spread
sigma <- polls %>% group_by(pollster) %>% mutate(spread= rawpoll_clinton/100 - rawpoll_trump/100) %>% summarize(s = sd(spread))


# Print the contents of sigma to the console
sigma

14.
What does the central limit theorem tell us about the distribution of the differences between the pollster averages, Y¯2-Y¯1?
If we assume N2 and N1 are large enough, Y¯2 and Y¯1, and their difference, are approximately normal.


15.
# The `polls` data have already been loaded for you. Use the `head` function to examine them.
head(polls)

# Create an object called `res` that summarizes the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>% summarize(avg=mean(spread), s=sd(spread), N=n())

# Store the difference between the larger average and the smaller in a variable called `estimate`. Print this value to the console.
estimate <- res$avg[2] - res$avg[1]
estimate

# Store the standard error of the estimates as a variable called `se_hat`. Print this value to the console.
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])
se_hat


# Calculate the 95% confidence interval of the spreads. Save the lower and then the upper confidence interval to a variable called `ci`.

ci <- c(lower = estimate - qnorm(0.975)*se_hat, upper = estimate + qnorm(0.975)*se_hat)


16.
The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. Random variability does not seem to explain it.

Compute a p-value to relay the fact that chance does not explain the observed pollster effect.
Use the pnorm function to calculate the probability that a random value is larger than the observed ratio of the estimate to the standard error.
Multiply the probability by 2, because this is the two-tailed test.
# We made an object `res` to summarize the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>% 
  summarize(avg = mean(spread), s = sd(spread), N = n()) 

# The variables `estimate` and `se_hat` contain the spread estimates and standard error, respectively.
estimate <- res$avg[2] - res$avg[1]
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])

# Calculate the p-value
z<- estimate/se_hat
1-(pnorm(z)-pnorm(-z))

17.
We compute statistic called the t-statistic by dividing our estimate of b2-b1 by its estimated standard error:

Y¯2-Y¯1 / sqrt(s2^2/N2 + s1^2/N1)
Later we learn will learn of another approximation for the distribution of this statistic for values of N2 and N1 that aren't large enough for the CLT.

Note that our data has more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?

Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.

Group the polls data by pollster.
Summarize the average and standard deviation of the spreads for each pollster.
Create an object called var that contains three columns: pollster, mean spread, and standard deviation.
Be sure to name the column for mean avg and the column for standard deviation s.

# Execute the following lines of code to filter the polling data and calculate the spread
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-15" &
           state == "U.S.") %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  ungroup()

# Create an object called `var` that contains columns for the pollster, mean spread, and standard deviation. Print the contents of this object to the console.
var <- polls %>% group_by(pollster)%>% summarize(avg=mean(spread), s=sd(spread))
var



